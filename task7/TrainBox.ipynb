{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import os \n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path = 'public_tests/splitted/train/images'\n",
    "test_imgs_path = 'public_tests/splitted/test/images'\n",
    "train_gt_path = 'public_tests/splitted/train/gt.csv'\n",
    "test_gt_path = 'public_tests/splitted/test/gt.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = detection.FacesPointsDataset(test_imgs_path, test_gt_path, 'val')\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\CV\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 1.7 M \n",
      "----------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.773     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\CV\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "d:\\venvs\\CV\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 149/150 [00:43<00:00,  3.45it/s, v_num=1, loss_step=1.42e+3]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 0: 100%|██████████| 150/150 [01:17<00:00,  1.93it/s, v_num=1, loss_step=1.39e+3, val_loss_step=1.32e+3, val_loss_epoch=1.37e+3, loss_epoch=1.84e+3]Epoch 0, Train_loss: 1838.56\n",
      "Epoch 1:  99%|█████████▉| 149/150 [00:58<00:00,  2.54it/s, v_num=1, loss_step=699.0, val_loss_step=1.32e+3, val_loss_epoch=1.37e+3, loss_epoch=1.84e+3]  Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 1: 100%|██████████| 150/150 [01:20<00:00,  1.87it/s, v_num=1, loss_step=692.0, val_loss_step=843.0, val_loss_epoch=805.0, loss_epoch=1.03e+3]    Epoch 1, Train_loss: 1029.777\n",
      "Epoch 2:  99%|█████████▉| 149/150 [00:46<00:00,  3.19it/s, v_num=1, loss_step=294.0, val_loss_step=843.0, val_loss_epoch=805.0, loss_epoch=1.03e+3]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 2: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=1, loss_step=277.0, val_loss_step=281.0, val_loss_epoch=274.0, loss_epoch=467.0]  Epoch 2, Train_loss: 467.444\n",
      "Epoch 3:  99%|█████████▉| 149/150 [00:45<00:00,  3.26it/s, v_num=1, loss_step=95.30, val_loss_step=281.0, val_loss_epoch=274.0, loss_epoch=467.0]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 3: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=1, loss_step=99.30, val_loss_step=134.0, val_loss_epoch=126.0, loss_epoch=183.0]Epoch 3, Train_loss: 182.986\n",
      "Epoch 4:  99%|█████████▉| 149/150 [00:43<00:00,  3.43it/s, v_num=1, loss_step=44.00, val_loss_step=134.0, val_loss_epoch=126.0, loss_epoch=183.0]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 4: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=1, loss_step=48.30, val_loss_step=37.70, val_loss_epoch=47.20, loss_epoch=80.60]Epoch 4, Train_loss: 80.592\n",
      "Epoch 5:  99%|█████████▉| 149/150 [00:43<00:00,  3.43it/s, v_num=1, loss_step=27.30, val_loss_step=37.70, val_loss_epoch=47.20, loss_epoch=80.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 5: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=1, loss_step=38.40, val_loss_step=34.80, val_loss_epoch=38.30, loss_epoch=48.50]Epoch 5, Train_loss: 48.496\n",
      "Epoch 6:  99%|█████████▉| 149/150 [00:43<00:00,  3.40it/s, v_num=1, loss_step=24.80, val_loss_step=34.80, val_loss_epoch=38.30, loss_epoch=48.50]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=1, loss_step=34.50, val_loss_step=28.30, val_loss_epoch=34.20, loss_epoch=40.00]Epoch 6, Train_loss: 40.041\n",
      "Epoch 7:  99%|█████████▉| 149/150 [00:44<00:00,  3.37it/s, v_num=1, loss_step=23.30, val_loss_step=28.30, val_loss_epoch=34.20, loss_epoch=40.00]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 7: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=1, loss_step=29.60, val_loss_step=25.40, val_loss_epoch=32.40, loss_epoch=36.50]Epoch 7, Train_loss: 36.478\n",
      "Epoch 8:  99%|█████████▉| 149/150 [00:43<00:00,  3.41it/s, v_num=1, loss_step=23.40, val_loss_step=25.40, val_loss_epoch=32.40, loss_epoch=36.50]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 8: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=1, loss_step=26.50, val_loss_step=28.20, val_loss_epoch=31.90, loss_epoch=34.70]Epoch 8, Train_loss: 34.739\n",
      "Epoch 9:  99%|█████████▉| 149/150 [00:44<00:00,  3.34it/s, v_num=1, loss_step=23.50, val_loss_step=28.20, val_loss_epoch=31.90, loss_epoch=34.70]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 9: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s, v_num=1, loss_step=25.50, val_loss_step=25.60, val_loss_epoch=30.40, loss_epoch=34.00]Epoch 9, Train_loss: 34.004\n",
      "Epoch 10:  99%|█████████▉| 149/150 [00:43<00:00,  3.45it/s, v_num=1, loss_step=21.20, val_loss_step=25.60, val_loss_epoch=30.40, loss_epoch=34.00]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 10: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=1, loss_step=22.20, val_loss_step=24.90, val_loss_epoch=30.00, loss_epoch=33.20]Epoch 10, Train_loss: 33.218\n",
      "Epoch 11:  99%|█████████▉| 149/150 [00:43<00:00,  3.40it/s, v_num=1, loss_step=21.40, val_loss_step=24.90, val_loss_epoch=30.00, loss_epoch=33.20]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 11: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=1, loss_step=22.90, val_loss_step=24.70, val_loss_epoch=29.70, loss_epoch=32.80]Epoch 11, Train_loss: 32.752\n",
      "Epoch 12:  99%|█████████▉| 149/150 [00:43<00:00,  3.39it/s, v_num=1, loss_step=21.00, val_loss_step=24.70, val_loss_epoch=29.70, loss_epoch=32.80]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 12: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=1, loss_step=21.30, val_loss_step=24.00, val_loss_epoch=28.60, loss_epoch=31.90]Epoch 12, Train_loss: 31.852\n",
      "Epoch 13:  99%|█████████▉| 149/150 [00:44<00:00,  3.35it/s, v_num=1, loss_step=18.10, val_loss_step=24.00, val_loss_epoch=28.60, loss_epoch=31.90]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 13: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=1, loss_step=19.00, val_loss_step=23.60, val_loss_epoch=27.00, loss_epoch=30.60]Epoch 13, Train_loss: 30.581\n",
      "Epoch 14:  99%|█████████▉| 149/150 [00:44<00:00,  3.35it/s, v_num=1, loss_step=18.50, val_loss_step=23.60, val_loss_epoch=27.00, loss_epoch=30.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 14: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=1, loss_step=18.60, val_loss_step=23.60, val_loss_epoch=26.00, loss_epoch=28.90]Epoch 14, Train_loss: 28.856\n",
      "Epoch 15:  99%|█████████▉| 149/150 [00:41<00:00,  3.60it/s, v_num=1, loss_step=17.50, val_loss_step=23.60, val_loss_epoch=26.00, loss_epoch=28.90]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 15: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=1, loss_step=16.90, val_loss_step=23.80, val_loss_epoch=23.90, loss_epoch=27.10]Epoch 15, Train_loss: 27.139\n",
      "Epoch 16:  99%|█████████▉| 149/150 [00:43<00:00,  3.45it/s, v_num=1, loss_step=16.70, val_loss_step=23.80, val_loss_epoch=23.90, loss_epoch=27.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 16: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=1, loss_step=15.90, val_loss_step=23.80, val_loss_epoch=23.00, loss_epoch=25.10]Epoch 16, Train_loss: 25.094\n",
      "Epoch 17:  99%|█████████▉| 149/150 [00:44<00:00,  3.35it/s, v_num=1, loss_step=12.80, val_loss_step=23.80, val_loss_epoch=23.00, loss_epoch=25.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 17: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=1, loss_step=15.00, val_loss_step=24.00, val_loss_epoch=21.60, loss_epoch=23.20]Epoch 17, Train_loss: 23.182\n",
      "Epoch 18:  99%|█████████▉| 149/150 [00:44<00:00,  3.34it/s, v_num=1, loss_step=13.80, val_loss_step=24.00, val_loss_epoch=21.60, loss_epoch=23.20]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=1, loss_step=14.20, val_loss_step=25.60, val_loss_epoch=20.30, loss_epoch=21.90]Epoch 18, Train_loss: 21.942\n",
      "Epoch 19:  99%|█████████▉| 149/150 [00:45<00:00,  3.25it/s, v_num=1, loss_step=12.80, val_loss_step=25.60, val_loss_epoch=20.30, loss_epoch=21.90]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 19: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=1, loss_step=13.60, val_loss_step=25.10, val_loss_epoch=18.90, loss_epoch=21.10]Epoch 19, Train_loss: 21.101\n",
      "Epoch 20:  99%|█████████▉| 149/150 [00:47<00:00,  3.14it/s, v_num=1, loss_step=11.80, val_loss_step=25.10, val_loss_epoch=18.90, loss_epoch=21.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 20: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s, v_num=1, loss_step=14.80, val_loss_step=24.30, val_loss_epoch=19.20, loss_epoch=20.00]Epoch 20, Train_loss: 20.035\n",
      "Epoch 21:  99%|█████████▉| 149/150 [00:46<00:00,  3.18it/s, v_num=1, loss_step=10.40, val_loss_step=24.30, val_loss_epoch=19.20, loss_epoch=20.00]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 21: 100%|██████████| 150/150 [01:14<00:00,  2.00it/s, v_num=1, loss_step=13.80, val_loss_step=23.90, val_loss_epoch=18.60, loss_epoch=19.40]Epoch 21, Train_loss: 19.438\n",
      "Epoch 22:  99%|█████████▉| 149/150 [00:47<00:00,  3.13it/s, v_num=1, loss_step=11.80, val_loss_step=23.90, val_loss_epoch=18.60, loss_epoch=19.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 22: 100%|██████████| 150/150 [01:14<00:00,  2.00it/s, v_num=1, loss_step=13.90, val_loss_step=24.00, val_loss_epoch=18.20, loss_epoch=18.90]Epoch 22, Train_loss: 18.908\n",
      "Epoch 23:  99%|█████████▉| 149/150 [00:45<00:00,  3.24it/s, v_num=1, loss_step=12.20, val_loss_step=24.00, val_loss_epoch=18.20, loss_epoch=18.90]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 23: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=1, loss_step=12.60, val_loss_step=24.70, val_loss_epoch=17.40, loss_epoch=18.30]Epoch 23, Train_loss: 18.31\n",
      "Epoch 24:  99%|█████████▉| 149/150 [00:47<00:00,  3.12it/s, v_num=1, loss_step=10.20, val_loss_step=24.70, val_loss_epoch=17.40, loss_epoch=18.30]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 24: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s, v_num=1, loss_step=11.70, val_loss_step=23.90, val_loss_epoch=17.40, loss_epoch=18.10]Epoch 24, Train_loss: 18.15\n",
      "Epoch 25:  99%|█████████▉| 149/150 [00:49<00:00,  2.98it/s, v_num=1, loss_step=10.80, val_loss_step=23.90, val_loss_epoch=17.40, loss_epoch=18.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 25: 100%|██████████| 150/150 [01:17<00:00,  1.93it/s, v_num=1, loss_step=13.50, val_loss_step=27.40, val_loss_epoch=16.70, loss_epoch=17.40]Epoch 25, Train_loss: 17.443\n",
      "Epoch 26:  99%|█████████▉| 149/150 [00:45<00:00,  3.27it/s, v_num=1, loss_step=11.80, val_loss_step=27.40, val_loss_epoch=16.70, loss_epoch=17.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 26: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=1, loss_step=13.90, val_loss_step=27.30, val_loss_epoch=17.60, loss_epoch=17.10]Epoch 26, Train_loss: 17.087\n",
      "Epoch 27:  99%|█████████▉| 149/150 [00:44<00:00,  3.33it/s, v_num=1, loss_step=10.10, val_loss_step=27.30, val_loss_epoch=17.60, loss_epoch=17.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 27: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=1, loss_step=13.30, val_loss_step=26.70, val_loss_epoch=16.40, loss_epoch=16.70]Epoch 27, Train_loss: 16.75\n",
      "Epoch 28:  99%|█████████▉| 149/150 [00:41<00:00,  3.57it/s, v_num=1, loss_step=10.50, val_loss_step=26.70, val_loss_epoch=16.40, loss_epoch=16.70]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 28: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=1, loss_step=13.10, val_loss_step=24.10, val_loss_epoch=16.30, loss_epoch=16.60]Epoch 28, Train_loss: 16.608\n",
      "Epoch 29:  99%|█████████▉| 149/150 [00:49<00:00,  3.03it/s, v_num=1, loss_step=10.10, val_loss_step=24.10, val_loss_epoch=16.30, loss_epoch=16.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 29: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=1, loss_step=13.30, val_loss_step=25.60, val_loss_epoch=16.10, loss_epoch=16.30]Epoch 29, Train_loss: 16.34\n",
      "Epoch 30:  99%|█████████▉| 149/150 [00:44<00:00,  3.36it/s, v_num=1, loss_step=10.20, val_loss_step=25.60, val_loss_epoch=16.10, loss_epoch=16.30]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 30: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=1, loss_step=15.00, val_loss_step=24.90, val_loss_epoch=15.50, loss_epoch=16.00]Epoch 30, Train_loss: 16.035\n",
      "Epoch 31:  99%|█████████▉| 149/150 [00:50<00:00,  2.97it/s, v_num=1, loss_step=9.330, val_loss_step=24.90, val_loss_epoch=15.50, loss_epoch=16.00]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 31: 100%|██████████| 150/150 [01:14<00:00,  2.01it/s, v_num=1, loss_step=13.30, val_loss_step=24.50, val_loss_epoch=15.40, loss_epoch=15.80]Epoch 31, Train_loss: 15.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=32` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s, v_num=1, loss_step=13.30, val_loss_step=24.50, val_loss_epoch=15.40, loss_epoch=15.80]\n"
     ]
    }
   ],
   "source": [
    "model = detection.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt_path ='lightning_logs/version_18/checkpoints/epoch=15-step=4800.ckpt' #lightning_logs/version_17/checkpoints/epoch=15-step=4800.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(current_shape, labels):\n",
    "    x_scale = 100/current_shape[2]\n",
    "    y_scale = 100/current_shape[1]\n",
    "    transformed = labels.clone()\n",
    "    for i in range(len(labels)):\n",
    "        if i % 2 == 0:\n",
    "            scale = x_scale\n",
    "        else:\n",
    "            scale = y_scale\n",
    "        transformed[i] = int(transformed[i]*scale)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_err(test_imgs_dir, test_gt_path, model):\n",
    "    err = 0.0\n",
    "    dataset = detection.FacesPointsDataset(test_imgs_dir, test_gt_path, mode='test', transform=None)\n",
    "    for i in range(len(dataset)):\n",
    "        img, label = dataset[i]\n",
    "        pred = model(img[None,:]).detach()[0]\n",
    "        # result[dataset.items[i][0]] = pred\n",
    "        diff = ((pred - label)**2).mean()\n",
    "        err += diff\n",
    "    err /= len(dataset)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.5695)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_err(test_imgs_path, test_gt_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
