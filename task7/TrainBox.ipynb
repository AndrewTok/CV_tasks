{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import os \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_loader(path):\n",
    "    result = {}\n",
    "    data = pd.read_csv(path)\n",
    "    for i in range(len(data)):\n",
    "        row = data.loc[i]\n",
    "        name = row['filename']\n",
    "        values = row[data.columns[1:]].values\n",
    "        result[name] = values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path = 'public_tests/splitted/train/images'\n",
    "test_imgs_path = train_imgs_path #'public_tests/splitted/test/images'\n",
    "train_gt_path = 'public_tests/splitted/train/gt.csv'\n",
    "test_gt_path = train_gt_path #'public_tests/splitted/test/gt.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = detection_32.FacesPointsDataset(test_imgs_path, test_gt_path, fraction = 0.8, mode='val')\n",
    "valid_loader = DataLoader(valid_dataset, 32, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:452: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.283    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 120/120 [00:53<00:00,  2.23it/s, v_num=86, loss_step=112.0, val_loss_step=134.0, val_loss_epoch=91.00, loss_epoch=346.0]Epoch 0, Train_loss: 345.811\n",
      "Epoch 1: 100%|██████████| 120/120 [00:50<00:00,  2.39it/s, v_num=86, loss_step=86.40, val_loss_step=47.40, val_loss_epoch=44.10, loss_epoch=161.0]Epoch 1, Train_loss: 161.387\n",
      "Epoch 2: 100%|██████████| 120/120 [00:55<00:00,  2.18it/s, v_num=86, loss_step=143.0, val_loss_step=50.80, val_loss_epoch=51.70, loss_epoch=140.0]Epoch 2, Train_loss: 139.605\n",
      "Epoch 3: 100%|██████████| 120/120 [00:51<00:00,  2.33it/s, v_num=86, loss_step=101.0, val_loss_step=50.90, val_loss_epoch=52.90, loss_epoch=128.0]Epoch 3, Train_loss: 128.468\n",
      "Epoch 4: 100%|██████████| 120/120 [00:50<00:00,  2.35it/s, v_num=86, loss_step=99.10, val_loss_step=48.10, val_loss_epoch=46.50, loss_epoch=120.0]Epoch 4, Train_loss: 119.878\n",
      "Epoch 5: 100%|██████████| 120/120 [00:50<00:00,  2.36it/s, v_num=86, loss_step=68.90, val_loss_step=27.40, val_loss_epoch=27.60, loss_epoch=111.0]Epoch 5, Train_loss: 110.844\n",
      "Epoch 6: 100%|██████████| 120/120 [00:51<00:00,  2.34it/s, v_num=86, loss_step=80.30, val_loss_step=28.70, val_loss_epoch=26.60, loss_epoch=105.0]Epoch 6, Train_loss: 105.062\n",
      "Epoch 7: 100%|██████████| 120/120 [00:52<00:00,  2.30it/s, v_num=86, loss_step=95.30, val_loss_step=25.60, val_loss_epoch=20.90, loss_epoch=101.0]Epoch 7, Train_loss: 100.773\n",
      "Epoch 8: 100%|██████████| 120/120 [00:51<00:00,  2.33it/s, v_num=86, loss_step=74.60, val_loss_step=23.20, val_loss_epoch=23.10, loss_epoch=98.80]Epoch 8, Train_loss: 98.838\n",
      "Epoch 9: 100%|██████████| 120/120 [00:53<00:00,  2.26it/s, v_num=86, loss_step=66.60, val_loss_step=24.50, val_loss_epoch=28.20, loss_epoch=93.80]Epoch 9, Train_loss: 93.828\n",
      "Epoch 10: 100%|██████████| 120/120 [00:53<00:00,  2.23it/s, v_num=86, loss_step=89.80, val_loss_step=17.10, val_loss_epoch=17.80, loss_epoch=97.40]Epoch 10, Train_loss: 97.386\n",
      "Epoch 11: 100%|██████████| 120/120 [00:54<00:00,  2.22it/s, v_num=86, loss_step=104.0, val_loss_step=19.80, val_loss_epoch=18.70, loss_epoch=95.30]Epoch 11, Train_loss: 95.341\n",
      "Epoch 12: 100%|██████████| 120/120 [00:54<00:00,  2.22it/s, v_num=86, loss_step=116.0, val_loss_step=18.40, val_loss_epoch=20.10, loss_epoch=90.10]Epoch 12, Train_loss: 90.116\n",
      "Epoch 13: 100%|██████████| 120/120 [00:52<00:00,  2.27it/s, v_num=86, loss_step=49.90, val_loss_step=19.90, val_loss_epoch=17.10, loss_epoch=94.20]Epoch 13, Train_loss: 94.165\n",
      "Epoch 14: 100%|██████████| 120/120 [00:53<00:00,  2.23it/s, v_num=86, loss_step=62.70, val_loss_step=16.60, val_loss_epoch=18.50, loss_epoch=90.90]Epoch 14, Train_loss: 90.866\n",
      "Epoch 15: 100%|██████████| 120/120 [00:52<00:00,  2.27it/s, v_num=86, loss_step=86.00, val_loss_step=21.40, val_loss_epoch=16.60, loss_epoch=95.50]Epoch 15, Train_loss: 95.464\n",
      "Epoch 16: 100%|██████████| 120/120 [00:54<00:00,  2.22it/s, v_num=86, loss_step=60.90, val_loss_step=20.00, val_loss_epoch=17.10, loss_epoch=88.90]Epoch 16, Train_loss: 88.95\n",
      "Epoch 17: 100%|██████████| 120/120 [00:54<00:00,  2.22it/s, v_num=86, loss_step=87.80, val_loss_step=19.50, val_loss_epoch=16.20, loss_epoch=88.70]Epoch 17, Train_loss: 88.748\n",
      "Epoch 18: 100%|██████████| 120/120 [00:54<00:00,  2.20it/s, v_num=86, loss_step=101.0, val_loss_step=15.20, val_loss_epoch=15.90, loss_epoch=88.90]Epoch 18, Train_loss: 88.932\n",
      "Epoch 19: 100%|██████████| 120/120 [00:53<00:00,  2.25it/s, v_num=86, loss_step=91.50, val_loss_step=15.00, val_loss_epoch=13.50, loss_epoch=89.80]Epoch 19, Train_loss: 89.769\n",
      "Epoch 20: 100%|██████████| 120/120 [00:53<00:00,  2.23it/s, v_num=86, loss_step=76.20, val_loss_step=13.40, val_loss_epoch=11.90, loss_epoch=87.10]Epoch 20, Train_loss: 87.063\n",
      "Epoch 21: 100%|██████████| 120/120 [00:51<00:00,  2.33it/s, v_num=86, loss_step=54.30, val_loss_step=14.90, val_loss_epoch=16.80, loss_epoch=86.50]Epoch 21, Train_loss: 86.543\n",
      "Epoch 22: 100%|██████████| 120/120 [00:51<00:00,  2.32it/s, v_num=86, loss_step=71.90, val_loss_step=15.30, val_loss_epoch=16.70, loss_epoch=85.90]Epoch 22, Train_loss: 85.943\n",
      "Epoch 23: 100%|██████████| 120/120 [00:50<00:00,  2.40it/s, v_num=86, loss_step=73.00, val_loss_step=13.00, val_loss_epoch=13.10, loss_epoch=82.00]Epoch 23, Train_loss: 82.026\n",
      "Epoch 24: 100%|██████████| 120/120 [00:51<00:00,  2.33it/s, v_num=86, loss_step=74.30, val_loss_step=14.30, val_loss_epoch=13.10, loss_epoch=77.60]Epoch 24, Train_loss: 77.605\n",
      "Epoch 25: 100%|██████████| 120/120 [00:56<00:00,  2.13it/s, v_num=86, loss_step=82.30, val_loss_step=11.90, val_loss_epoch=12.90, loss_epoch=82.70]Epoch 25, Train_loss: 82.741\n",
      "Epoch 26: 100%|██████████| 120/120 [00:53<00:00,  2.23it/s, v_num=86, loss_step=36.00, val_loss_step=12.50, val_loss_epoch=12.50, loss_epoch=75.80]Epoch 26, Train_loss: 75.762\n",
      "Epoch 27: 100%|██████████| 120/120 [00:54<00:00,  2.19it/s, v_num=86, loss_step=85.40, val_loss_step=14.60, val_loss_epoch=13.40, loss_epoch=81.00]Epoch 27, Train_loss: 80.969\n",
      "Epoch 28: 100%|██████████| 120/120 [00:53<00:00,  2.24it/s, v_num=86, loss_step=70.80, val_loss_step=11.80, val_loss_epoch=10.80, loss_epoch=80.20]Epoch 28, Train_loss: 80.153\n",
      "Epoch 29: 100%|██████████| 120/120 [00:55<00:00,  2.17it/s, v_num=86, loss_step=77.50, val_loss_step=13.70, val_loss_epoch=13.60, loss_epoch=78.60]Epoch 29, Train_loss: 78.573\n",
      "Epoch 30: 100%|██████████| 120/120 [00:57<00:00,  2.10it/s, v_num=86, loss_step=77.80, val_loss_step=13.80, val_loss_epoch=15.10, loss_epoch=76.30]Epoch 30, Train_loss: 76.276\n",
      "Epoch 31: 100%|██████████| 120/120 [00:56<00:00,  2.14it/s, v_num=86, loss_step=90.10, val_loss_step=10.20, val_loss_epoch=10.60, loss_epoch=75.40]Epoch 31, Train_loss: 75.422\n",
      "Epoch 32: 100%|██████████| 120/120 [00:56<00:00,  2.11it/s, v_num=86, loss_step=68.60, val_loss_step=13.70, val_loss_epoch=11.10, loss_epoch=73.50]Epoch 32, Train_loss: 73.452\n",
      "Epoch 33:  43%|████▎     | 52/120 [00:21<00:28,  2.37it/s, v_num=86, loss_step=92.40, val_loss_step=13.70, val_loss_epoch=11.10, loss_epoch=73.50] "
     ]
    }
   ],
   "source": [
    "import detection_32\n",
    "model = detection_32.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(current_shape, labels):\n",
    "    x_scale = 100/current_shape[2]\n",
    "    y_scale = 100/current_shape[1]\n",
    "    transformed = labels.clone()\n",
    "    for i in range(len(labels)):\n",
    "        if i % 2 == 0:\n",
    "            scale = x_scale\n",
    "        else:\n",
    "            scale = y_scale\n",
    "        transformed[i] = int(transformed[i]*scale)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_err(test_imgs_dir, test_gt_path, model):\n",
    "    model.eval()\n",
    "    err = 0.0\n",
    "    dataset = detection.FacesPointsDataset(test_imgs_dir, test_gt_path, mode='test', transform=None)\n",
    "    for i in range(len(dataset)):\n",
    "        img, label = dataset[i]\n",
    "        pred = model(img[None,:]).detach()[0]\n",
    "        # result[dataset.items[i][0]] = pred\n",
    "        diff = ((pred - label)**2).mean()\n",
    "        err += diff\n",
    "    err /= len(dataset)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection_16\n",
    "import detection_17 \n",
    "import detection_18\n",
    "import detection_19\n",
    "import detection_20\n",
    "import detection_21\n",
    "import detection_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:452: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 7.7 M \n",
      "----------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.612    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 149/150 [00:55<00:00,  2.66it/s, v_num=70, loss_step=147.0] Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 0: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=70, loss_step=165.0, val_loss_step=81.70, val_loss_epoch=105.0, loss_epoch=325.0]Epoch 0, Train_loss: 324.627\n",
      "Epoch 1:  99%|█████████▉| 149/150 [00:56<00:00,  2.62it/s, v_num=70, loss_step=154.0, val_loss_step=81.70, val_loss_epoch=105.0, loss_epoch=325.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 1: 100%|██████████| 150/150 [01:05<00:00,  2.31it/s, v_num=70, loss_step=132.0, val_loss_step=25.80, val_loss_epoch=39.70, loss_epoch=162.0]Epoch 1, Train_loss: 162.38\n",
      "Epoch 2:  99%|█████████▉| 149/150 [00:55<00:00,  2.67it/s, v_num=70, loss_step=169.0, val_loss_step=25.80, val_loss_epoch=39.70, loss_epoch=162.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 2: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=70, loss_step=119.0, val_loss_step=25.50, val_loss_epoch=40.90, loss_epoch=155.0]Epoch 2, Train_loss: 154.723\n",
      "Epoch 3:  99%|█████████▉| 149/150 [00:55<00:00,  2.70it/s, v_num=70, loss_step=192.0, val_loss_step=25.50, val_loss_epoch=40.90, loss_epoch=155.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 3: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=70, loss_step=137.0, val_loss_step=33.70, val_loss_epoch=46.10, loss_epoch=157.0]Epoch 3, Train_loss: 157.15\n",
      "Epoch 4:  99%|█████████▉| 149/150 [00:56<00:00,  2.66it/s, v_num=70, loss_step=128.0, val_loss_step=33.70, val_loss_epoch=46.10, loss_epoch=157.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 4: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=70, loss_step=175.0, val_loss_step=33.70, val_loss_epoch=45.60, loss_epoch=156.0]Epoch 4, Train_loss: 156.037\n",
      "Epoch 5:  99%|█████████▉| 149/150 [00:54<00:00,  2.72it/s, v_num=70, loss_step=97.70, val_loss_step=33.70, val_loss_epoch=45.60, loss_epoch=156.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 5: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=70, loss_step=137.0, val_loss_step=24.80, val_loss_epoch=37.70, loss_epoch=148.0]Epoch 5, Train_loss: 148.218\n",
      "Epoch 6:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=104.0, val_loss_step=24.80, val_loss_epoch=37.70, loss_epoch=148.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 6: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=125.0, val_loss_step=101.0, val_loss_epoch=108.0, loss_epoch=141.0]Epoch 6, Train_loss: 140.518\n",
      "Epoch 7:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=108.0, val_loss_step=101.0, val_loss_epoch=108.0, loss_epoch=141.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 7: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=107.0, val_loss_step=25.90, val_loss_epoch=32.80, loss_epoch=132.0]Epoch 7, Train_loss: 131.55\n",
      "Epoch 8:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=120.0, val_loss_step=25.90, val_loss_epoch=32.80, loss_epoch=132.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 8: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=98.00, val_loss_step=18.30, val_loss_epoch=29.40, loss_epoch=119.0]Epoch 8, Train_loss: 118.686\n",
      "Epoch 9:  99%|█████████▉| 149/150 [00:55<00:00,  2.67it/s, v_num=70, loss_step=105.0, val_loss_step=18.30, val_loss_epoch=29.40, loss_epoch=119.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 9: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=70, loss_step=108.0, val_loss_step=13.70, val_loss_epoch=23.60, loss_epoch=115.0]Epoch 9, Train_loss: 114.597\n",
      "Epoch 10:  99%|█████████▉| 149/150 [00:54<00:00,  2.73it/s, v_num=70, loss_step=97.70, val_loss_step=13.70, val_loss_epoch=23.60, loss_epoch=115.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 10: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=70, loss_step=96.40, val_loss_step=12.50, val_loss_epoch=18.70, loss_epoch=111.0]Epoch 10, Train_loss: 110.829\n",
      "Epoch 11:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=117.0, val_loss_step=12.50, val_loss_epoch=18.70, loss_epoch=111.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 11: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=109.0, val_loss_step=22.20, val_loss_epoch=28.10, loss_epoch=105.0]Epoch 11, Train_loss: 104.832\n",
      "Epoch 12:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=133.0, val_loss_step=22.20, val_loss_epoch=28.10, loss_epoch=105.0]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 12: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=77.20, val_loss_step=11.30, val_loss_epoch=19.50, loss_epoch=98.00]Epoch 12, Train_loss: 97.981\n",
      "Epoch 13:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=88.40, val_loss_step=11.30, val_loss_epoch=19.50, loss_epoch=98.00]Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 13: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=40.90, val_loss_step=13.20, val_loss_epoch=19.90, loss_epoch=103.0]Epoch 13, Train_loss: 102.986\n",
      "Epoch 14:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=55.00, val_loss_step=13.20, val_loss_epoch=19.90, loss_epoch=103.0]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=85.90, val_loss_step=14.80, val_loss_epoch=22.50, loss_epoch=98.30]Epoch 14, Train_loss: 98.297\n",
      "Epoch 15:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=58.10, val_loss_step=14.80, val_loss_epoch=22.50, loss_epoch=98.30]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 15: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=122.0, val_loss_step=10.40, val_loss_epoch=15.60, loss_epoch=93.10]Epoch 15, Train_loss: 93.118\n",
      "Epoch 16:  99%|█████████▉| 149/150 [00:52<00:00,  2.83it/s, v_num=70, loss_step=101.0, val_loss_step=10.40, val_loss_epoch=15.60, loss_epoch=93.10]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=70, loss_step=79.10, val_loss_step=9.800, val_loss_epoch=15.00, loss_epoch=93.30]Epoch 16, Train_loss: 93.259\n",
      "Epoch 17:  99%|█████████▉| 149/150 [00:54<00:00,  2.76it/s, v_num=70, loss_step=79.70, val_loss_step=9.800, val_loss_epoch=15.00, loss_epoch=93.30]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 17: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=121.0, val_loss_step=14.50, val_loss_epoch=18.50, loss_epoch=94.20]Epoch 17, Train_loss: 94.225\n",
      "Epoch 18:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=116.0, val_loss_step=14.50, val_loss_epoch=18.50, loss_epoch=94.20]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 18: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=97.50, val_loss_step=12.50, val_loss_epoch=16.70, loss_epoch=89.30]Epoch 18, Train_loss: 89.304\n",
      "Epoch 19:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=76.10, val_loss_step=12.50, val_loss_epoch=16.70, loss_epoch=89.30]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 19: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=96.00, val_loss_step=13.20, val_loss_epoch=17.70, loss_epoch=94.70]Epoch 19, Train_loss: 94.729\n",
      "Epoch 20:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=87.90, val_loss_step=13.20, val_loss_epoch=17.70, loss_epoch=94.70]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 20: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=143.0, val_loss_step=11.00, val_loss_epoch=16.10, loss_epoch=89.20]Epoch 20, Train_loss: 89.218\n",
      "Epoch 21:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=41.60, val_loss_step=11.00, val_loss_epoch=16.10, loss_epoch=89.20]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 21: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=110.0, val_loss_step=8.890, val_loss_epoch=13.80, loss_epoch=92.00]Epoch 21, Train_loss: 91.985\n",
      "Epoch 22:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=78.20, val_loss_step=8.890, val_loss_epoch=13.80, loss_epoch=92.00]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 22: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=125.0, val_loss_step=11.30, val_loss_epoch=14.30, loss_epoch=92.70]Epoch 22, Train_loss: 92.651\n",
      "Epoch 23:  99%|█████████▉| 149/150 [00:53<00:00,  2.80it/s, v_num=70, loss_step=88.70, val_loss_step=11.30, val_loss_epoch=14.30, loss_epoch=92.70]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 23: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=104.0, val_loss_step=11.70, val_loss_epoch=15.60, loss_epoch=90.90]Epoch 23, Train_loss: 90.872\n",
      "Epoch 24:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=90.10, val_loss_step=11.70, val_loss_epoch=15.60, loss_epoch=90.90]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 24: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=66.40, val_loss_step=11.30, val_loss_epoch=16.50, loss_epoch=91.10]Epoch 24, Train_loss: 91.125\n",
      "Epoch 25:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=88.90, val_loss_step=11.30, val_loss_epoch=16.50, loss_epoch=91.10]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 25: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=94.40, val_loss_step=10.90, val_loss_epoch=15.50, loss_epoch=88.90]Epoch 25, Train_loss: 88.857\n",
      "Epoch 26:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=80.80, val_loss_step=10.90, val_loss_epoch=15.50, loss_epoch=88.90]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 26: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=147.0, val_loss_step=9.280, val_loss_epoch=14.10, loss_epoch=90.60]Epoch 26, Train_loss: 90.628\n",
      "Epoch 27:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=89.10, val_loss_step=9.280, val_loss_epoch=14.10, loss_epoch=90.60]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 27: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=113.0, val_loss_step=9.160, val_loss_epoch=12.70, loss_epoch=91.60]Epoch 27, Train_loss: 91.629\n",
      "Epoch 28:  99%|█████████▉| 149/150 [00:54<00:00,  2.74it/s, v_num=70, loss_step=91.60, val_loss_step=9.160, val_loss_epoch=12.70, loss_epoch=91.60]Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 28: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=109.0, val_loss_step=9.140, val_loss_epoch=12.60, loss_epoch=88.40]Epoch 28, Train_loss: 88.353\n",
      "Epoch 29:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=35.20, val_loss_step=9.140, val_loss_epoch=12.60, loss_epoch=88.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 29: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=92.70, val_loss_step=14.50, val_loss_epoch=16.70, loss_epoch=89.30]Epoch 29, Train_loss: 89.313\n",
      "Epoch 30:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=41.20, val_loss_step=14.50, val_loss_epoch=16.70, loss_epoch=89.30]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 30: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=97.20, val_loss_step=10.10, val_loss_epoch=12.20, loss_epoch=90.20]Epoch 30, Train_loss: 90.246\n",
      "Epoch 31:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=88.30, val_loss_step=10.10, val_loss_epoch=12.20, loss_epoch=90.20]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 31: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=124.0, val_loss_step=11.70, val_loss_epoch=14.00, loss_epoch=84.40]Epoch 31, Train_loss: 84.396\n",
      "Epoch 32:  99%|█████████▉| 149/150 [00:54<00:00,  2.76it/s, v_num=70, loss_step=55.40, val_loss_step=11.70, val_loss_epoch=14.00, loss_epoch=84.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 32: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=114.0, val_loss_step=13.20, val_loss_epoch=15.00, loss_epoch=84.10]Epoch 32, Train_loss: 84.072\n",
      "Epoch 33:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=47.60, val_loss_step=13.20, val_loss_epoch=15.00, loss_epoch=84.10]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 33: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=63.70, val_loss_step=8.550, val_loss_epoch=11.60, loss_epoch=81.60]Epoch 33, Train_loss: 81.558\n",
      "Epoch 34:  99%|█████████▉| 149/150 [00:52<00:00,  2.84it/s, v_num=70, loss_step=49.30, val_loss_step=8.550, val_loss_epoch=11.60, loss_epoch=81.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 34: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=70, loss_step=107.0, val_loss_step=9.410, val_loss_epoch=11.90, loss_epoch=78.60]Epoch 34, Train_loss: 78.558\n",
      "Epoch 35:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=66.50, val_loss_step=9.410, val_loss_epoch=11.90, loss_epoch=78.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 35: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=75.30, val_loss_step=9.370, val_loss_epoch=12.20, loss_epoch=79.60]Epoch 35, Train_loss: 79.588\n",
      "Epoch 36:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=56.00, val_loss_step=9.370, val_loss_epoch=12.20, loss_epoch=79.60]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 36: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=94.10, val_loss_step=9.140, val_loss_epoch=11.40, loss_epoch=79.50]Epoch 36, Train_loss: 79.495\n",
      "Epoch 37:  99%|█████████▉| 149/150 [00:52<00:00,  2.82it/s, v_num=70, loss_step=57.80, val_loss_step=9.140, val_loss_epoch=11.40, loss_epoch=79.50]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 37: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=70, loss_step=79.10, val_loss_step=9.620, val_loss_epoch=12.20, loss_epoch=79.30]Epoch 37, Train_loss: 79.275\n",
      "Epoch 38:  99%|█████████▉| 149/150 [00:54<00:00,  2.73it/s, v_num=70, loss_step=60.40, val_loss_step=9.620, val_loss_epoch=12.20, loss_epoch=79.30]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 38: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=62.20, val_loss_step=11.10, val_loss_epoch=13.70, loss_epoch=79.40]Epoch 38, Train_loss: 79.372\n",
      "Epoch 39:  99%|█████████▉| 149/150 [00:54<00:00,  2.74it/s, v_num=70, loss_step=86.20, val_loss_step=11.10, val_loss_epoch=13.70, loss_epoch=79.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 39: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=69.00, val_loss_step=12.70, val_loss_epoch=14.30, loss_epoch=73.90]Epoch 39, Train_loss: 73.866\n",
      "Epoch 40:  99%|█████████▉| 149/150 [00:53<00:00,  2.80it/s, v_num=70, loss_step=62.10, val_loss_step=12.70, val_loss_epoch=14.30, loss_epoch=73.90]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 40: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=60.20, val_loss_step=13.80, val_loss_epoch=17.10, loss_epoch=76.20]Epoch 40, Train_loss: 76.211\n",
      "Epoch 41:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=75.50, val_loss_step=13.80, val_loss_epoch=17.10, loss_epoch=76.20]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 41: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=87.50, val_loss_step=11.90, val_loss_epoch=14.40, loss_epoch=76.40]Epoch 41, Train_loss: 76.352\n",
      "Epoch 42:  99%|█████████▉| 149/150 [00:54<00:00,  2.76it/s, v_num=70, loss_step=62.70, val_loss_step=11.90, val_loss_epoch=14.40, loss_epoch=76.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 42: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=73.50, val_loss_step=8.940, val_loss_epoch=12.50, loss_epoch=74.40]Epoch 42, Train_loss: 74.44\n",
      "Epoch 43:  99%|█████████▉| 149/150 [00:54<00:00,  2.73it/s, v_num=70, loss_step=85.70, val_loss_step=8.940, val_loss_epoch=12.50, loss_epoch=74.40]Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 43: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=70, loss_step=68.60, val_loss_step=11.50, val_loss_epoch=14.00, loss_epoch=76.00]Epoch 43, Train_loss: 76.007\n",
      "Epoch 44:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=47.70, val_loss_step=11.50, val_loss_epoch=14.00, loss_epoch=76.00]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 44: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=104.0, val_loss_step=14.90, val_loss_epoch=15.90, loss_epoch=75.00]Epoch 44, Train_loss: 74.97\n",
      "Epoch 45:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=38.70, val_loss_step=14.90, val_loss_epoch=15.90, loss_epoch=75.00]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 45: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=94.90, val_loss_step=8.590, val_loss_epoch=10.50, loss_epoch=71.40]Epoch 45, Train_loss: 71.435\n",
      "Epoch 46:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=33.60, val_loss_step=8.590, val_loss_epoch=10.50, loss_epoch=71.40]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 46: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=77.90, val_loss_step=7.850, val_loss_epoch=9.890, loss_epoch=69.20]Epoch 46, Train_loss: 69.23\n",
      "Epoch 47:  99%|█████████▉| 149/150 [00:52<00:00,  2.82it/s, v_num=70, loss_step=66.50, val_loss_step=7.850, val_loss_epoch=9.890, loss_epoch=69.20]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 47: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=70, loss_step=97.50, val_loss_step=8.740, val_loss_epoch=11.80, loss_epoch=67.70]Epoch 47, Train_loss: 67.655\n",
      "Epoch 48:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=47.70, val_loss_step=8.740, val_loss_epoch=11.80, loss_epoch=67.70]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 48: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=47.60, val_loss_step=9.280, val_loss_epoch=11.00, loss_epoch=70.80]Epoch 48, Train_loss: 70.755\n",
      "Epoch 49:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=56.80, val_loss_step=9.280, val_loss_epoch=11.00, loss_epoch=70.80]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 49: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=36.50, val_loss_step=10.10, val_loss_epoch=12.10, loss_epoch=69.70]Epoch 49, Train_loss: 69.71\n",
      "Epoch 50:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=48.20, val_loss_step=10.10, val_loss_epoch=12.10, loss_epoch=69.70]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 50: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=112.0, val_loss_step=8.030, val_loss_epoch=9.970, loss_epoch=67.70]Epoch 50, Train_loss: 67.684\n",
      "Epoch 51:  99%|█████████▉| 149/150 [00:58<00:00,  2.55it/s, v_num=70, loss_step=31.90, val_loss_step=8.030, val_loss_epoch=9.970, loss_epoch=67.70]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 51: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s, v_num=70, loss_step=90.80, val_loss_step=8.420, val_loss_epoch=10.50, loss_epoch=64.30]Epoch 51, Train_loss: 64.271\n",
      "Epoch 52:  99%|█████████▉| 149/150 [00:52<00:00,  2.85it/s, v_num=70, loss_step=28.10, val_loss_step=8.420, val_loss_epoch=10.50, loss_epoch=64.30]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 52: 100%|██████████| 150/150 [00:59<00:00,  2.50it/s, v_num=70, loss_step=101.0, val_loss_step=8.570, val_loss_epoch=10.10, loss_epoch=65.90]Epoch 52, Train_loss: 65.907\n",
      "Epoch 53:  99%|█████████▉| 149/150 [00:52<00:00,  2.86it/s, v_num=70, loss_step=31.80, val_loss_step=8.570, val_loss_epoch=10.10, loss_epoch=65.90]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 53: 100%|██████████| 150/150 [00:59<00:00,  2.51it/s, v_num=70, loss_step=53.50, val_loss_step=9.950, val_loss_epoch=10.60, loss_epoch=66.40]Epoch 53, Train_loss: 66.425\n",
      "Epoch 54:  99%|█████████▉| 149/150 [00:51<00:00,  2.90it/s, v_num=70, loss_step=53.30, val_loss_step=9.950, val_loss_epoch=10.60, loss_epoch=66.40]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 54: 100%|██████████| 150/150 [00:58<00:00,  2.55it/s, v_num=70, loss_step=132.0, val_loss_step=8.250, val_loss_epoch=10.20, loss_epoch=65.80]Epoch 54, Train_loss: 65.803\n",
      "Epoch 55:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=65.80, val_loss_step=8.250, val_loss_epoch=10.20, loss_epoch=65.80]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 55: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=44.00, val_loss_step=7.860, val_loss_epoch=10.60, loss_epoch=67.60]Epoch 55, Train_loss: 67.599\n",
      "Epoch 56:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=47.00, val_loss_step=7.860, val_loss_epoch=10.60, loss_epoch=67.60]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 56: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=72.40, val_loss_step=8.250, val_loss_epoch=9.990, loss_epoch=64.20]Epoch 56, Train_loss: 64.193\n",
      "Epoch 57:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=72.10, val_loss_step=8.250, val_loss_epoch=9.990, loss_epoch=64.20]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 57: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=31.60, val_loss_step=9.410, val_loss_epoch=10.80, loss_epoch=67.10]Epoch 57, Train_loss: 67.102\n",
      "Epoch 58:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=72.50, val_loss_step=9.410, val_loss_epoch=10.80, loss_epoch=67.10]Adjusting learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 58: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=73.10, val_loss_step=8.110, val_loss_epoch=9.830, loss_epoch=62.50]Epoch 58, Train_loss: 62.535\n",
      "Epoch 59:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=31.00, val_loss_step=8.110, val_loss_epoch=9.830, loss_epoch=62.50]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 59: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=70, loss_step=63.80, val_loss_step=8.490, val_loss_epoch=10.40, loss_epoch=65.00]Epoch 59, Train_loss: 64.984\n",
      "Epoch 60:  99%|█████████▉| 149/150 [00:54<00:00,  2.76it/s, v_num=70, loss_step=73.90, val_loss_step=8.490, val_loss_epoch=10.40, loss_epoch=65.00]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 60: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=70.50, val_loss_step=8.000, val_loss_epoch=9.520, loss_epoch=63.60]Epoch 60, Train_loss: 63.594\n",
      "Epoch 61:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=65.50, val_loss_step=8.000, val_loss_epoch=9.520, loss_epoch=63.60]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 61: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=48.40, val_loss_step=7.850, val_loss_epoch=9.650, loss_epoch=61.40]Epoch 61, Train_loss: 61.402\n",
      "Epoch 62:  99%|█████████▉| 149/150 [00:53<00:00,  2.80it/s, v_num=70, loss_step=39.40, val_loss_step=7.850, val_loss_epoch=9.650, loss_epoch=61.40]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 62: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=72.40, val_loss_step=7.470, val_loss_epoch=9.970, loss_epoch=60.60]Epoch 62, Train_loss: 60.587\n",
      "Epoch 63:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=57.60, val_loss_step=7.470, val_loss_epoch=9.970, loss_epoch=60.60]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 63: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=107.0, val_loss_step=7.760, val_loss_epoch=9.320, loss_epoch=58.90]Epoch 63, Train_loss: 58.893\n",
      "Epoch 64:  99%|█████████▉| 149/150 [00:54<00:00,  2.74it/s, v_num=70, loss_step=69.70, val_loss_step=7.760, val_loss_epoch=9.320, loss_epoch=58.90]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 64: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=70, loss_step=77.20, val_loss_step=7.760, val_loss_epoch=8.840, loss_epoch=65.00]Epoch 64, Train_loss: 65.037\n",
      "Epoch 65:  99%|█████████▉| 149/150 [00:53<00:00,  2.81it/s, v_num=70, loss_step=47.60, val_loss_step=7.760, val_loss_epoch=8.840, loss_epoch=65.00]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 65: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=55.20, val_loss_step=7.870, val_loss_epoch=9.160, loss_epoch=60.40]Epoch 65, Train_loss: 60.351\n",
      "Epoch 66:  99%|█████████▉| 149/150 [00:54<00:00,  2.71it/s, v_num=70, loss_step=37.30, val_loss_step=7.870, val_loss_epoch=9.160, loss_epoch=60.40]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 66: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=70, loss_step=46.50, val_loss_step=9.310, val_loss_epoch=9.930, loss_epoch=59.60]Epoch 66, Train_loss: 59.609\n",
      "Epoch 67:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=54.30, val_loss_step=9.310, val_loss_epoch=9.930, loss_epoch=59.60]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 67: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=47.60, val_loss_step=8.020, val_loss_epoch=8.600, loss_epoch=62.80]Epoch 67, Train_loss: 62.783\n",
      "Epoch 68:  99%|█████████▉| 149/150 [00:54<00:00,  2.73it/s, v_num=70, loss_step=77.60, val_loss_step=8.020, val_loss_epoch=8.600, loss_epoch=62.80]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 68: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=70, loss_step=31.10, val_loss_step=8.320, val_loss_epoch=9.040, loss_epoch=59.70]Epoch 68, Train_loss: 59.702\n",
      "Epoch 69:  99%|█████████▉| 149/150 [00:53<00:00,  2.81it/s, v_num=70, loss_step=71.20, val_loss_step=8.320, val_loss_epoch=9.040, loss_epoch=59.70]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 69: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=50.40, val_loss_step=7.900, val_loss_epoch=9.500, loss_epoch=58.30]Epoch 69, Train_loss: 58.31\n",
      "Epoch 70:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=38.10, val_loss_step=7.900, val_loss_epoch=9.500, loss_epoch=58.30]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 70: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=48.90, val_loss_step=7.930, val_loss_epoch=8.770, loss_epoch=63.50]Epoch 70, Train_loss: 63.524\n",
      "Epoch 71:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=82.00, val_loss_step=7.930, val_loss_epoch=8.770, loss_epoch=63.50]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 71: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=53.00, val_loss_step=7.740, val_loss_epoch=8.410, loss_epoch=59.40]Epoch 71, Train_loss: 59.356\n",
      "Epoch 72:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=50.60, val_loss_step=7.740, val_loss_epoch=8.410, loss_epoch=59.40]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 72: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=50.30, val_loss_step=7.950, val_loss_epoch=8.820, loss_epoch=58.00]Epoch 72, Train_loss: 57.979\n",
      "Epoch 73:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=53.10, val_loss_step=7.950, val_loss_epoch=8.820, loss_epoch=58.00]Adjusting learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 73: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=56.80, val_loss_step=7.920, val_loss_epoch=8.420, loss_epoch=60.60]Epoch 73, Train_loss: 60.562\n",
      "Epoch 74:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=38.80, val_loss_step=7.920, val_loss_epoch=8.420, loss_epoch=60.60]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 74: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=48.50, val_loss_step=8.150, val_loss_epoch=9.140, loss_epoch=57.80]Epoch 74, Train_loss: 57.794\n",
      "Epoch 75:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=75.20, val_loss_step=8.150, val_loss_epoch=9.140, loss_epoch=57.80]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 75: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=44.40, val_loss_step=8.280, val_loss_epoch=8.990, loss_epoch=55.40]Epoch 75, Train_loss: 55.418\n",
      "Epoch 76:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=18.80, val_loss_step=8.280, val_loss_epoch=8.990, loss_epoch=55.40]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 76: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=55.60, val_loss_step=7.820, val_loss_epoch=8.890, loss_epoch=58.50]Epoch 76, Train_loss: 58.508\n",
      "Epoch 77:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=41.20, val_loss_step=7.820, val_loss_epoch=8.890, loss_epoch=58.50]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 77: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=119.0, val_loss_step=8.190, val_loss_epoch=8.250, loss_epoch=58.00]Epoch 77, Train_loss: 58.008\n",
      "Epoch 78:  99%|█████████▉| 149/150 [00:54<00:00,  2.76it/s, v_num=70, loss_step=81.80, val_loss_step=8.190, val_loss_epoch=8.250, loss_epoch=58.00]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 78: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=74.30, val_loss_step=7.780, val_loss_epoch=8.620, loss_epoch=59.40]Epoch 78, Train_loss: 59.426\n",
      "Epoch 79:  99%|█████████▉| 149/150 [00:53<00:00,  2.80it/s, v_num=70, loss_step=51.60, val_loss_step=7.780, val_loss_epoch=8.620, loss_epoch=59.40]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 79: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=42.40, val_loss_step=7.860, val_loss_epoch=8.400, loss_epoch=55.90]Epoch 79, Train_loss: 55.869\n",
      "Epoch 80:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=39.50, val_loss_step=7.860, val_loss_epoch=8.400, loss_epoch=55.90]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 80: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=70, loss_step=52.30, val_loss_step=8.370, val_loss_epoch=8.440, loss_epoch=58.00]Epoch 80, Train_loss: 57.977\n",
      "Epoch 81:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=38.30, val_loss_step=8.370, val_loss_epoch=8.440, loss_epoch=58.00]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 81: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=26.60, val_loss_step=8.100, val_loss_epoch=8.250, loss_epoch=56.20]Epoch 81, Train_loss: 56.165\n",
      "Epoch 82:  99%|█████████▉| 149/150 [00:52<00:00,  2.82it/s, v_num=70, loss_step=35.80, val_loss_step=8.100, val_loss_epoch=8.250, loss_epoch=56.20]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 82: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=70, loss_step=52.90, val_loss_step=8.160, val_loss_epoch=8.310, loss_epoch=53.60]Epoch 82, Train_loss: 53.599\n",
      "Epoch 83:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=39.00, val_loss_step=8.160, val_loss_epoch=8.310, loss_epoch=53.60]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 83: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=34.00, val_loss_step=8.910, val_loss_epoch=8.590, loss_epoch=57.50]Epoch 83, Train_loss: 57.512\n",
      "Epoch 84:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=23.50, val_loss_step=8.910, val_loss_epoch=8.590, loss_epoch=57.50]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 84: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=70, loss_step=60.80, val_loss_step=7.940, val_loss_epoch=8.110, loss_epoch=58.00]Epoch 84, Train_loss: 58.008\n",
      "Epoch 85:  99%|█████████▉| 149/150 [00:53<00:00,  2.78it/s, v_num=70, loss_step=44.20, val_loss_step=7.940, val_loss_epoch=8.110, loss_epoch=58.00]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 85: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=69.40, val_loss_step=8.110, val_loss_epoch=8.350, loss_epoch=53.50]Epoch 85, Train_loss: 53.468\n",
      "Epoch 86:  99%|█████████▉| 149/150 [00:55<00:00,  2.70it/s, v_num=70, loss_step=86.30, val_loss_step=8.110, val_loss_epoch=8.350, loss_epoch=53.50]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 86: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=70, loss_step=27.30, val_loss_step=8.010, val_loss_epoch=8.430, loss_epoch=57.30]Epoch 86, Train_loss: 57.258\n",
      "Epoch 87:  99%|█████████▉| 149/150 [00:54<00:00,  2.75it/s, v_num=70, loss_step=53.90, val_loss_step=8.010, val_loss_epoch=8.430, loss_epoch=57.30]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 87: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=63.60, val_loss_step=7.900, val_loss_epoch=8.390, loss_epoch=57.50]Epoch 87, Train_loss: 57.492\n",
      "Epoch 88:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=37.90, val_loss_step=7.900, val_loss_epoch=8.390, loss_epoch=57.50]Adjusting learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 88: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=68.70, val_loss_step=7.980, val_loss_epoch=8.190, loss_epoch=55.50]Epoch 88, Train_loss: 55.538\n",
      "Epoch 89:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=42.40, val_loss_step=7.980, val_loss_epoch=8.190, loss_epoch=55.50]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 89: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=71.50, val_loss_step=7.930, val_loss_epoch=7.890, loss_epoch=58.00]Epoch 89, Train_loss: 58.033\n",
      "Epoch 90:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=29.50, val_loss_step=7.930, val_loss_epoch=7.890, loss_epoch=58.00]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 90: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=93.70, val_loss_step=8.010, val_loss_epoch=7.810, loss_epoch=56.10]Epoch 90, Train_loss: 56.12\n",
      "Epoch 91:  99%|█████████▉| 149/150 [00:53<00:00,  2.80it/s, v_num=70, loss_step=55.00, val_loss_step=8.010, val_loss_epoch=7.810, loss_epoch=56.10]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 91: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=60.10, val_loss_step=7.920, val_loss_epoch=7.820, loss_epoch=53.80]Epoch 91, Train_loss: 53.81\n",
      "Epoch 92:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=100.0, val_loss_step=7.920, val_loss_epoch=7.820, loss_epoch=53.80]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 92: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=64.20, val_loss_step=8.540, val_loss_epoch=8.110, loss_epoch=54.70]Epoch 92, Train_loss: 54.676\n",
      "Epoch 93:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=20.10, val_loss_step=8.540, val_loss_epoch=8.110, loss_epoch=54.70]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 93: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=70, loss_step=29.00, val_loss_step=7.940, val_loss_epoch=7.900, loss_epoch=53.10]Epoch 93, Train_loss: 53.115\n",
      "Epoch 94:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=32.60, val_loss_step=7.940, val_loss_epoch=7.900, loss_epoch=53.10]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 94: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=70, loss_step=84.40, val_loss_step=8.000, val_loss_epoch=7.800, loss_epoch=52.60]Epoch 94, Train_loss: 52.601\n",
      "Epoch 95:  99%|█████████▉| 149/150 [00:53<00:00,  2.77it/s, v_num=70, loss_step=47.90, val_loss_step=8.000, val_loss_epoch=7.800, loss_epoch=52.60]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 95: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=70, loss_step=34.40, val_loss_step=8.070, val_loss_epoch=7.710, loss_epoch=56.10]Epoch 95, Train_loss: 56.06\n",
      "Epoch 96:  99%|█████████▉| 149/150 [00:54<00:00,  2.72it/s, v_num=70, loss_step=58.40, val_loss_step=8.070, val_loss_epoch=7.710, loss_epoch=56.10]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 96: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=70, loss_step=71.80, val_loss_step=7.560, val_loss_epoch=7.410, loss_epoch=54.90]Epoch 96, Train_loss: 54.868\n",
      "Epoch 97:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=53.00, val_loss_step=7.560, val_loss_epoch=7.410, loss_epoch=54.90]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 97: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=44.20, val_loss_step=8.000, val_loss_epoch=7.680, loss_epoch=52.70]Epoch 97, Train_loss: 52.693\n",
      "Epoch 98:  99%|█████████▉| 149/150 [00:53<00:00,  2.79it/s, v_num=70, loss_step=56.80, val_loss_step=8.000, val_loss_epoch=7.680, loss_epoch=52.70]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 98: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=70, loss_step=51.50, val_loss_step=8.380, val_loss_epoch=7.810, loss_epoch=57.00]Epoch 98, Train_loss: 56.988\n",
      "Epoch 99:  99%|█████████▉| 149/150 [00:53<00:00,  2.76it/s, v_num=70, loss_step=42.40, val_loss_step=8.380, val_loss_epoch=7.810, loss_epoch=57.00]Adjusting learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 99: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=70, loss_step=54.00, val_loss_step=7.810, val_loss_epoch=7.510, loss_epoch=56.10]Epoch 99, Train_loss: 56.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=70, loss_step=54.00, val_loss_step=7.810, val_loss_epoch=7.510, loss_epoch=56.10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 1.6 M \n",
      "----------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.412     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=120.0, val_loss_step=37.50, val_loss_epoch=52.40, loss_epoch=460.0]Epoch 0, Train_loss: 459.757\n",
      "Epoch 1: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=132.0, val_loss_step=29.10, val_loss_epoch=39.90, loss_epoch=149.0]Epoch 1, Train_loss: 149.073\n",
      "Epoch 2: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=71, loss_step=104.0, val_loss_step=26.70, val_loss_epoch=36.90, loss_epoch=136.0]Epoch 2, Train_loss: 136.218\n",
      "Epoch 3: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=156.0, val_loss_step=23.30, val_loss_epoch=34.30, loss_epoch=129.0]Epoch 3, Train_loss: 129.313\n",
      "Epoch 4: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=116.0, val_loss_step=20.40, val_loss_epoch=31.60, loss_epoch=120.0]Epoch 4, Train_loss: 120.245\n",
      "Epoch 5: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=96.60, val_loss_step=16.70, val_loss_epoch=24.90, loss_epoch=107.0]Epoch 5, Train_loss: 106.965\n",
      "Epoch 6: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=111.0, val_loss_step=10.20, val_loss_epoch=18.40, loss_epoch=101.0]Epoch 6, Train_loss: 100.7\n",
      "Epoch 7: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=71, loss_step=106.0, val_loss_step=11.00, val_loss_epoch=17.30, loss_epoch=97.00]Epoch 7, Train_loss: 96.984\n",
      "Epoch 8: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=140.0, val_loss_step=9.760, val_loss_epoch=17.80, loss_epoch=102.0]Epoch 8, Train_loss: 101.928\n",
      "Epoch 9: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=133.0, val_loss_step=11.50, val_loss_epoch=18.00, loss_epoch=95.50]Epoch 9, Train_loss: 95.521\n",
      "Epoch 10: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=92.40, val_loss_step=10.70, val_loss_epoch=16.20, loss_epoch=96.80]Epoch 10, Train_loss: 96.775\n",
      "Epoch 11: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=119.0, val_loss_step=11.10, val_loss_epoch=17.40, loss_epoch=92.40]Epoch 11, Train_loss: 92.43\n",
      "Epoch 12: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=109.0, val_loss_step=10.60, val_loss_epoch=14.20, loss_epoch=90.20]Epoch 12, Train_loss: 90.201\n",
      "Epoch 13: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=86.60, val_loss_step=12.20, val_loss_epoch=16.10, loss_epoch=91.50]Epoch 13, Train_loss: 91.456\n",
      "Epoch 14: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=57.60, val_loss_step=9.150, val_loss_epoch=13.00, loss_epoch=87.50]Epoch 14, Train_loss: 87.513\n",
      "Epoch 15: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=100.0, val_loss_step=10.10, val_loss_epoch=12.80, loss_epoch=88.10]Epoch 15, Train_loss: 88.119\n",
      "Epoch 16: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=78.80, val_loss_step=10.90, val_loss_epoch=13.00, loss_epoch=88.10]Epoch 16, Train_loss: 88.144\n",
      "Epoch 17: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=57.00, val_loss_step=11.20, val_loss_epoch=12.60, loss_epoch=85.90]Epoch 17, Train_loss: 85.872\n",
      "Epoch 18: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=112.0, val_loss_step=9.370, val_loss_epoch=11.80, loss_epoch=89.80]Epoch 18, Train_loss: 89.759\n",
      "Epoch 19: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=45.90, val_loss_step=9.510, val_loss_epoch=12.70, loss_epoch=84.50]Epoch 19, Train_loss: 84.466\n",
      "Epoch 20: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=92.50, val_loss_step=9.900, val_loss_epoch=13.30, loss_epoch=88.00]Epoch 20, Train_loss: 88.029\n",
      "Epoch 21: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=47.60, val_loss_step=8.720, val_loss_epoch=10.90, loss_epoch=84.70]Epoch 21, Train_loss: 84.689\n",
      "Epoch 22: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=68.60, val_loss_step=9.100, val_loss_epoch=10.80, loss_epoch=86.70]Epoch 22, Train_loss: 86.691\n",
      "Epoch 23: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=42.80, val_loss_step=10.80, val_loss_epoch=11.70, loss_epoch=85.00]Epoch 23, Train_loss: 84.968\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 24: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=66.10, val_loss_step=9.150, val_loss_epoch=10.20, loss_epoch=81.70]Epoch 24, Train_loss: 81.652\n",
      "Epoch 25: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=103.0, val_loss_step=9.360, val_loss_epoch=10.30, loss_epoch=85.40]Epoch 25, Train_loss: 85.375\n",
      "Epoch 26: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=71.40, val_loss_step=9.930, val_loss_epoch=10.30, loss_epoch=82.70]Epoch 26, Train_loss: 82.663\n",
      "Epoch 27: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=71, loss_step=92.20, val_loss_step=9.860, val_loss_epoch=10.50, loss_epoch=81.60]Epoch 27, Train_loss: 81.615\n",
      "Epoch 28: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=58.80, val_loss_step=9.320, val_loss_epoch=9.740, loss_epoch=80.70]Epoch 28, Train_loss: 80.703\n",
      "Epoch 29: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=46.70, val_loss_step=9.150, val_loss_epoch=11.10, loss_epoch=79.60]Epoch 29, Train_loss: 79.617\n",
      "Epoch 30: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=87.90, val_loss_step=8.550, val_loss_epoch=9.520, loss_epoch=78.90]Epoch 30, Train_loss: 78.903\n",
      "Epoch 31: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=68.30, val_loss_step=9.230, val_loss_epoch=9.530, loss_epoch=81.00]Epoch 31, Train_loss: 80.953\n",
      "Epoch 32: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=71, loss_step=89.20, val_loss_step=8.400, val_loss_epoch=10.40, loss_epoch=76.80]Epoch 32, Train_loss: 76.795\n",
      "Epoch 33: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=79.30, val_loss_step=8.230, val_loss_epoch=9.110, loss_epoch=79.40]Epoch 33, Train_loss: 79.376\n",
      "Epoch 34: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=77.00, val_loss_step=8.710, val_loss_epoch=9.450, loss_epoch=83.50]Epoch 34, Train_loss: 83.513\n",
      "Epoch 35: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=56.90, val_loss_step=9.150, val_loss_epoch=9.670, loss_epoch=76.30]Epoch 35, Train_loss: 76.319\n",
      "Epoch 36: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=86.50, val_loss_step=8.790, val_loss_epoch=9.400, loss_epoch=75.60]Epoch 36, Train_loss: 75.589\n",
      "Epoch 37: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=62.10, val_loss_step=9.180, val_loss_epoch=9.820, loss_epoch=75.80]Epoch 37, Train_loss: 75.779\n",
      "Epoch 38: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=111.0, val_loss_step=8.430, val_loss_epoch=8.970, loss_epoch=74.70]Epoch 38, Train_loss: 74.702\n",
      "Epoch 39: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=61.50, val_loss_step=8.830, val_loss_epoch=9.890, loss_epoch=76.30]Epoch 39, Train_loss: 76.295\n",
      "Epoch 40: 100%|██████████| 150/150 [00:59<00:00,  2.53it/s, v_num=71, loss_step=57.00, val_loss_step=9.230, val_loss_epoch=10.20, loss_epoch=74.60]Epoch 40, Train_loss: 74.553\n",
      "Epoch 41: 100%|██████████| 150/150 [00:58<00:00,  2.56it/s, v_num=71, loss_step=68.10, val_loss_step=8.920, val_loss_epoch=9.660, loss_epoch=71.80]Epoch 41, Train_loss: 71.767\n",
      "Epoch 42: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=77.80, val_loss_step=8.550, val_loss_epoch=9.290, loss_epoch=72.00]Epoch 42, Train_loss: 72.033\n",
      "Epoch 43: 100%|██████████| 150/150 [00:59<00:00,  2.51it/s, v_num=71, loss_step=79.80, val_loss_step=8.500, val_loss_epoch=9.170, loss_epoch=73.80]Epoch 43, Train_loss: 73.793\n",
      "Epoch 44: 100%|██████████| 150/150 [00:59<00:00,  2.51it/s, v_num=71, loss_step=76.20, val_loss_step=9.510, val_loss_epoch=9.680, loss_epoch=68.50]Epoch 44, Train_loss: 68.462\n",
      "Epoch 45: 100%|██████████| 150/150 [01:00<00:00,  2.49it/s, v_num=71, loss_step=54.60, val_loss_step=8.960, val_loss_epoch=9.450, loss_epoch=73.80]Epoch 45, Train_loss: 73.764\n",
      "Epoch 46: 100%|██████████| 150/150 [00:59<00:00,  2.51it/s, v_num=71, loss_step=59.90, val_loss_step=8.550, val_loss_epoch=9.250, loss_epoch=74.20]Epoch 46, Train_loss: 74.167\n",
      "Epoch 47: 100%|██████████| 150/150 [01:00<00:00,  2.50it/s, v_num=71, loss_step=96.00, val_loss_step=9.230, val_loss_epoch=9.800, loss_epoch=73.20]Epoch 47, Train_loss: 73.171\n",
      "Epoch 48: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=73.90, val_loss_step=10.20, val_loss_epoch=10.50, loss_epoch=75.30]Epoch 48, Train_loss: 75.325\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 49: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=99.70, val_loss_step=8.750, val_loss_epoch=9.060, loss_epoch=69.50]Epoch 49, Train_loss: 69.53\n",
      "Epoch 50: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=71, loss_step=101.0, val_loss_step=8.890, val_loss_epoch=9.030, loss_epoch=70.20]Epoch 50, Train_loss: 70.238\n",
      "Epoch 51: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=95.40, val_loss_step=9.180, val_loss_epoch=9.070, loss_epoch=69.10]Epoch 51, Train_loss: 69.069\n",
      "Epoch 52: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=79.60, val_loss_step=9.000, val_loss_epoch=9.120, loss_epoch=71.10]Epoch 52, Train_loss: 71.125\n",
      "Epoch 00053: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 53: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=63.60, val_loss_step=9.920, val_loss_epoch=9.720, loss_epoch=71.70]Epoch 53, Train_loss: 71.727\n",
      "Epoch 54: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=42.70, val_loss_step=9.340, val_loss_epoch=9.170, loss_epoch=70.60]Epoch 54, Train_loss: 70.572\n",
      "Epoch 55: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=58.60, val_loss_step=9.070, val_loss_epoch=8.920, loss_epoch=72.50]Epoch 55, Train_loss: 72.486\n",
      "Epoch 56: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=62.40, val_loss_step=9.220, val_loss_epoch=9.040, loss_epoch=66.80]Epoch 56, Train_loss: 66.814\n",
      "Epoch 57: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=80.10, val_loss_step=9.250, val_loss_epoch=9.060, loss_epoch=72.10]Epoch 57, Train_loss: 72.113\n",
      "Epoch 58: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=88.60, val_loss_step=9.010, val_loss_epoch=8.840, loss_epoch=67.30]Epoch 58, Train_loss: 67.3\n",
      "Epoch 59: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=70.30, val_loss_step=8.980, val_loss_epoch=8.880, loss_epoch=71.40]Epoch 59, Train_loss: 71.426\n",
      "Epoch 60: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=69.80, val_loss_step=9.760, val_loss_epoch=9.380, loss_epoch=70.10]Epoch 60, Train_loss: 70.095\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 61: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=71.10, val_loss_step=9.550, val_loss_epoch=9.340, loss_epoch=70.80]Epoch 61, Train_loss: 70.82\n",
      "Epoch 62: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=61.90, val_loss_step=10.30, val_loss_epoch=9.920, loss_epoch=70.90]Epoch 62, Train_loss: 70.928\n",
      "Epoch 63: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=71.20, val_loss_step=9.350, val_loss_epoch=9.240, loss_epoch=69.10]Epoch 63, Train_loss: 69.099\n",
      "Epoch 64: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=45.40, val_loss_step=8.790, val_loss_epoch=8.670, loss_epoch=70.40]Epoch 64, Train_loss: 70.425\n",
      "Epoch 00065: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 65: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=71, loss_step=59.00, val_loss_step=8.750, val_loss_epoch=8.710, loss_epoch=70.40]Epoch 65, Train_loss: 70.36\n",
      "Epoch 66: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=67.60, val_loss_step=9.350, val_loss_epoch=9.210, loss_epoch=73.40]Epoch 66, Train_loss: 73.377\n",
      "Epoch 67: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=62.20, val_loss_step=8.740, val_loss_epoch=8.820, loss_epoch=69.00]Epoch 67, Train_loss: 68.974\n",
      "Epoch 68: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=65.90, val_loss_step=10.20, val_loss_epoch=9.890, loss_epoch=72.80]Epoch 68, Train_loss: 72.808\n",
      "Epoch 00069: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 69: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=90.60, val_loss_step=9.230, val_loss_epoch=8.890, loss_epoch=68.80]Epoch 69, Train_loss: 68.829\n",
      "Epoch 70: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=77.30, val_loss_step=9.390, val_loss_epoch=9.210, loss_epoch=67.90]Epoch 70, Train_loss: 67.93\n",
      "Epoch 71: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=71, loss_step=85.10, val_loss_step=9.110, val_loss_epoch=8.990, loss_epoch=71.30]Epoch 71, Train_loss: 71.275\n",
      "Epoch 72: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=44.70, val_loss_step=9.860, val_loss_epoch=9.550, loss_epoch=67.30]Epoch 72, Train_loss: 67.276\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 73: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=69.40, val_loss_step=9.970, val_loss_epoch=9.630, loss_epoch=72.60]Epoch 73, Train_loss: 72.595\n",
      "Epoch 74: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=62.30, val_loss_step=8.860, val_loss_epoch=8.750, loss_epoch=71.70]Epoch 74, Train_loss: 71.673\n",
      "Epoch 75: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=111.0, val_loss_step=9.250, val_loss_epoch=8.940, loss_epoch=71.70]Epoch 75, Train_loss: 71.728\n",
      "Epoch 76: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=50.30, val_loss_step=9.100, val_loss_epoch=8.910, loss_epoch=65.90]Epoch 76, Train_loss: 65.853\n",
      "Epoch 77: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=45.70, val_loss_step=8.740, val_loss_epoch=8.680, loss_epoch=68.40]Epoch 77, Train_loss: 68.446\n",
      "Epoch 78: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=90.10, val_loss_step=9.000, val_loss_epoch=8.840, loss_epoch=71.20]Epoch 78, Train_loss: 71.201\n",
      "Epoch 79: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=59.00, val_loss_step=9.710, val_loss_epoch=9.410, loss_epoch=70.60]Epoch 79, Train_loss: 70.575\n",
      "Epoch 80: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=49.00, val_loss_step=10.00, val_loss_epoch=9.650, loss_epoch=69.40]Epoch 80, Train_loss: 69.409\n",
      "Epoch 00081: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 81: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=62.20, val_loss_step=9.130, val_loss_epoch=9.000, loss_epoch=71.80]Epoch 81, Train_loss: 71.835\n",
      "Epoch 82: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=48.80, val_loss_step=9.530, val_loss_epoch=9.220, loss_epoch=70.30]Epoch 82, Train_loss: 70.29\n",
      "Epoch 83: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=74.10, val_loss_step=9.020, val_loss_epoch=8.950, loss_epoch=69.30]Epoch 83, Train_loss: 69.296\n",
      "Epoch 84: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=62.60, val_loss_step=9.050, val_loss_epoch=8.970, loss_epoch=73.30]Epoch 84, Train_loss: 73.291\n",
      "Epoch 00085: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 85: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=66.90, val_loss_step=11.50, val_loss_epoch=10.90, loss_epoch=72.50]Epoch 85, Train_loss: 72.479\n",
      "Epoch 86: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=71, loss_step=73.30, val_loss_step=9.440, val_loss_epoch=9.240, loss_epoch=70.20]Epoch 86, Train_loss: 70.205\n",
      "Epoch 87: 100%|██████████| 150/150 [01:00<00:00,  2.49it/s, v_num=71, loss_step=52.40, val_loss_step=8.850, val_loss_epoch=8.710, loss_epoch=68.70]Epoch 87, Train_loss: 68.726\n",
      "Epoch 88: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=52.50, val_loss_step=9.340, val_loss_epoch=9.110, loss_epoch=69.00]Epoch 88, Train_loss: 69.022\n",
      "Epoch 89: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=74.00, val_loss_step=9.000, val_loss_epoch=8.910, loss_epoch=70.10]Epoch 89, Train_loss: 70.061\n",
      "Epoch 90: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=71, loss_step=79.10, val_loss_step=9.150, val_loss_epoch=8.920, loss_epoch=69.80]Epoch 90, Train_loss: 69.795\n",
      "Epoch 91: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=71, loss_step=110.0, val_loss_step=9.910, val_loss_epoch=9.610, loss_epoch=71.00]Epoch 91, Train_loss: 71.021\n",
      "Epoch 92: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=108.0, val_loss_step=9.050, val_loss_epoch=8.910, loss_epoch=69.40]Epoch 92, Train_loss: 69.404\n",
      "Epoch 93: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=78.30, val_loss_step=9.780, val_loss_epoch=9.520, loss_epoch=70.40]Epoch 93, Train_loss: 70.385\n",
      "Epoch 94: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=47.60, val_loss_step=9.050, val_loss_epoch=8.830, loss_epoch=72.50]Epoch 94, Train_loss: 72.468\n",
      "Epoch 95: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=71, loss_step=90.80, val_loss_step=9.850, val_loss_epoch=9.550, loss_epoch=70.30]Epoch 95, Train_loss: 70.304\n",
      "Epoch 96: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=71, loss_step=36.50, val_loss_step=8.770, val_loss_epoch=8.720, loss_epoch=70.00]Epoch 96, Train_loss: 70.034\n",
      "Epoch 97: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=75.60, val_loss_step=9.350, val_loss_epoch=9.150, loss_epoch=72.60]Epoch 97, Train_loss: 72.63\n",
      "Epoch 98: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=71, loss_step=88.10, val_loss_step=9.020, val_loss_epoch=8.880, loss_epoch=70.90]Epoch 98, Train_loss: 70.925\n",
      "Epoch 99: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=71, loss_step=37.50, val_loss_step=9.670, val_loss_epoch=9.450, loss_epoch=70.90]Epoch 99, Train_loss: 70.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=71, loss_step=37.50, val_loss_step=9.670, val_loss_epoch=9.450, loss_epoch=70.90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 1.6 M \n",
      "----------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.412     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=163.0, val_loss_step=25.40, val_loss_epoch=39.10, loss_epoch=472.0]Epoch 0, Train_loss: 472.221\n",
      "Epoch 1: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=158.0, val_loss_step=31.30, val_loss_epoch=45.80, loss_epoch=168.0]Epoch 1, Train_loss: 168.218\n",
      "Epoch 2: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=131.0, val_loss_step=28.80, val_loss_epoch=43.10, loss_epoch=161.0]Epoch 2, Train_loss: 160.599\n",
      "Epoch 3: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=139.0, val_loss_step=55.70, val_loss_epoch=67.40, loss_epoch=144.0]Epoch 3, Train_loss: 143.518\n",
      "Epoch 4: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=131.0, val_loss_step=13.30, val_loss_epoch=23.60, loss_epoch=136.0]Epoch 4, Train_loss: 136.285\n",
      "Epoch 5: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=103.0, val_loss_step=12.20, val_loss_epoch=20.20, loss_epoch=123.0]Epoch 5, Train_loss: 123.091\n",
      "Epoch 6: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=99.00, val_loss_step=14.80, val_loss_epoch=24.30, loss_epoch=120.0]Epoch 6, Train_loss: 120.374\n",
      "Epoch 7: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=72, loss_step=71.30, val_loss_step=11.20, val_loss_epoch=18.70, loss_epoch=118.0]Epoch 7, Train_loss: 117.573\n",
      "Epoch 8: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=100.0, val_loss_step=16.00, val_loss_epoch=23.20, loss_epoch=119.0]Epoch 8, Train_loss: 118.504\n",
      "Epoch 9: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=121.0, val_loss_step=11.20, val_loss_epoch=17.70, loss_epoch=112.0]Epoch 9, Train_loss: 112.0\n",
      "Epoch 10: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=92.60, val_loss_step=10.00, val_loss_epoch=18.00, loss_epoch=103.0]Epoch 10, Train_loss: 102.793\n",
      "Epoch 11: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=92.50, val_loss_step=9.260, val_loss_epoch=15.70, loss_epoch=105.0]Epoch 11, Train_loss: 105.249\n",
      "Epoch 12: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=72, loss_step=123.0, val_loss_step=9.310, val_loss_epoch=14.90, loss_epoch=108.0]Epoch 12, Train_loss: 108.346\n",
      "Epoch 13: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=146.0, val_loss_step=10.40, val_loss_epoch=15.20, loss_epoch=101.0]Epoch 13, Train_loss: 101.118\n",
      "Epoch 14: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=104.0, val_loss_step=8.550, val_loss_epoch=13.60, loss_epoch=105.0]Epoch 14, Train_loss: 105.345\n",
      "Epoch 15: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=69.90, val_loss_step=9.330, val_loss_epoch=15.40, loss_epoch=104.0]Epoch 15, Train_loss: 103.907\n",
      "Epoch 16: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=72.10, val_loss_step=9.680, val_loss_epoch=14.70, loss_epoch=102.0]Epoch 16, Train_loss: 102.453\n",
      "Epoch 17: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=85.30, val_loss_step=8.770, val_loss_epoch=14.10, loss_epoch=105.0]Epoch 17, Train_loss: 104.599\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 18: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=62.20, val_loss_step=8.680, val_loss_epoch=12.40, loss_epoch=94.40]Epoch 18, Train_loss: 94.415\n",
      "Epoch 19: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=97.20, val_loss_step=9.150, val_loss_epoch=12.50, loss_epoch=94.80]Epoch 19, Train_loss: 94.762\n",
      "Epoch 20: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=51.20, val_loss_step=8.170, val_loss_epoch=11.70, loss_epoch=95.90]Epoch 20, Train_loss: 95.93\n",
      "Epoch 21: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=72, loss_step=82.90, val_loss_step=8.320, val_loss_epoch=11.70, loss_epoch=94.60]Epoch 21, Train_loss: 94.606\n",
      "Epoch 22: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=87.20, val_loss_step=8.200, val_loss_epoch=11.70, loss_epoch=97.70]Epoch 22, Train_loss: 97.665\n",
      "Epoch 00023: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 23: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=67.70, val_loss_step=8.440, val_loss_epoch=11.20, loss_epoch=100.0]Epoch 23, Train_loss: 100.309\n",
      "Epoch 24: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=96.80, val_loss_step=8.210, val_loss_epoch=11.00, loss_epoch=93.40]Epoch 24, Train_loss: 93.413\n",
      "Epoch 25: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=71.90, val_loss_step=8.220, val_loss_epoch=10.80, loss_epoch=96.10]Epoch 25, Train_loss: 96.101\n",
      "Epoch 26: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=77.30, val_loss_step=8.360, val_loss_epoch=11.20, loss_epoch=91.00]Epoch 26, Train_loss: 90.998\n",
      "Epoch 27: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=80.20, val_loss_step=8.380, val_loss_epoch=10.80, loss_epoch=91.40]Epoch 27, Train_loss: 91.435\n",
      "Epoch 28: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=72, loss_step=102.0, val_loss_step=8.280, val_loss_epoch=10.60, loss_epoch=97.80]Epoch 28, Train_loss: 97.801\n",
      "Epoch 29: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=90.40, val_loss_step=8.510, val_loss_epoch=10.90, loss_epoch=96.00]Epoch 29, Train_loss: 96.044\n",
      "Epoch 30: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=95.10, val_loss_step=8.240, val_loss_epoch=10.80, loss_epoch=95.30]Epoch 30, Train_loss: 95.252\n",
      "Epoch 00031: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 31: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=122.0, val_loss_step=8.350, val_loss_epoch=10.90, loss_epoch=94.00]Epoch 31, Train_loss: 94.005\n",
      "Epoch 32: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=103.0, val_loss_step=8.380, val_loss_epoch=10.90, loss_epoch=94.70]Epoch 32, Train_loss: 94.661\n",
      "Epoch 33: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=60.70, val_loss_step=8.390, val_loss_epoch=10.90, loss_epoch=95.80]Epoch 33, Train_loss: 95.79\n",
      "Epoch 34: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=63.80, val_loss_step=8.540, val_loss_epoch=10.80, loss_epoch=95.90]Epoch 34, Train_loss: 95.883\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 35: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=76.80, val_loss_step=8.660, val_loss_epoch=11.00, loss_epoch=94.20]Epoch 35, Train_loss: 94.182\n",
      "Epoch 36: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=58.70, val_loss_step=8.410, val_loss_epoch=11.00, loss_epoch=91.30]Epoch 36, Train_loss: 91.328\n",
      "Epoch 37: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=115.0, val_loss_step=8.310, val_loss_epoch=10.80, loss_epoch=94.30]Epoch 37, Train_loss: 94.307\n",
      "Epoch 38: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=107.0, val_loss_step=8.330, val_loss_epoch=10.60, loss_epoch=95.90]Epoch 38, Train_loss: 95.89\n",
      "Epoch 00039: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 39: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=71.30, val_loss_step=8.350, val_loss_epoch=10.90, loss_epoch=93.10]Epoch 39, Train_loss: 93.093\n",
      "Epoch 40: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=96.30, val_loss_step=8.410, val_loss_epoch=10.70, loss_epoch=91.10]Epoch 40, Train_loss: 91.145\n",
      "Epoch 41: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=101.0, val_loss_step=8.500, val_loss_epoch=10.80, loss_epoch=94.10]Epoch 41, Train_loss: 94.054\n",
      "Epoch 42: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=72, loss_step=59.60, val_loss_step=8.530, val_loss_epoch=10.90, loss_epoch=99.00]Epoch 42, Train_loss: 98.976\n",
      "Epoch 00043: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 43: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=101.0, val_loss_step=8.340, val_loss_epoch=10.50, loss_epoch=95.70]Epoch 43, Train_loss: 95.715\n",
      "Epoch 44: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=104.0, val_loss_step=8.390, val_loss_epoch=10.60, loss_epoch=95.30]Epoch 44, Train_loss: 95.272\n",
      "Epoch 45: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=64.10, val_loss_step=8.630, val_loss_epoch=11.30, loss_epoch=96.30]Epoch 45, Train_loss: 96.313\n",
      "Epoch 46: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s, v_num=72, loss_step=116.0, val_loss_step=8.600, val_loss_epoch=11.00, loss_epoch=95.70]Epoch 46, Train_loss: 95.677\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 47: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=106.0, val_loss_step=8.430, val_loss_epoch=10.70, loss_epoch=95.90]Epoch 47, Train_loss: 95.892\n",
      "Epoch 48: 100%|██████████| 150/150 [01:00<00:00,  2.49it/s, v_num=72, loss_step=130.0, val_loss_step=8.350, val_loss_epoch=10.50, loss_epoch=95.10]Epoch 48, Train_loss: 95.086\n",
      "Epoch 49: 100%|██████████| 150/150 [01:00<00:00,  2.49it/s, v_num=72, loss_step=107.0, val_loss_step=8.480, val_loss_epoch=10.70, loss_epoch=93.50]Epoch 49, Train_loss: 93.467\n",
      "Epoch 50: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=70.20, val_loss_step=8.390, val_loss_epoch=10.60, loss_epoch=92.90]Epoch 50, Train_loss: 92.885\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 51: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=115.0, val_loss_step=8.440, val_loss_epoch=10.60, loss_epoch=93.50]Epoch 51, Train_loss: 93.464\n",
      "Epoch 52: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=72, loss_step=78.30, val_loss_step=8.420, val_loss_epoch=11.00, loss_epoch=93.70]Epoch 52, Train_loss: 93.686\n",
      "Epoch 53: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=98.90, val_loss_step=8.320, val_loss_epoch=10.90, loss_epoch=91.90]Epoch 53, Train_loss: 91.927\n",
      "Epoch 54: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=72, loss_step=56.40, val_loss_step=8.410, val_loss_epoch=10.80, loss_epoch=94.10]Epoch 54, Train_loss: 94.059\n",
      "Epoch 00055: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 55: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=137.0, val_loss_step=8.630, val_loss_epoch=10.70, loss_epoch=92.60]Epoch 55, Train_loss: 92.601\n",
      "Epoch 56: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=75.10, val_loss_step=8.560, val_loss_epoch=10.90, loss_epoch=94.00]Epoch 56, Train_loss: 94.0\n",
      "Epoch 57: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=72, loss_step=93.60, val_loss_step=8.700, val_loss_epoch=11.40, loss_epoch=93.90]Epoch 57, Train_loss: 93.851\n",
      "Epoch 58: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=72, loss_step=94.90, val_loss_step=8.390, val_loss_epoch=10.60, loss_epoch=94.20]Epoch 58, Train_loss: 94.221\n",
      "Epoch 59: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=75.70, val_loss_step=8.400, val_loss_epoch=10.90, loss_epoch=95.90]Epoch 59, Train_loss: 95.866\n",
      "Epoch 60: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=130.0, val_loss_step=8.620, val_loss_epoch=11.00, loss_epoch=97.00]Epoch 60, Train_loss: 97.036\n",
      "Epoch 61: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=72, loss_step=88.30, val_loss_step=8.410, val_loss_epoch=10.70, loss_epoch=89.90]Epoch 61, Train_loss: 89.876\n",
      "Epoch 62: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=97.90, val_loss_step=8.360, val_loss_epoch=10.80, loss_epoch=96.50]Epoch 62, Train_loss: 96.491\n",
      "Epoch 63: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=60.70, val_loss_step=8.380, val_loss_epoch=10.90, loss_epoch=93.80]Epoch 63, Train_loss: 93.839\n",
      "Epoch 64: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=72, loss_step=128.0, val_loss_step=8.510, val_loss_epoch=11.10, loss_epoch=94.20]Epoch 64, Train_loss: 94.202\n",
      "Epoch 65: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=89.60, val_loss_step=8.270, val_loss_epoch=10.80, loss_epoch=93.60]Epoch 65, Train_loss: 93.634\n",
      "Epoch 66: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=72, loss_step=64.50, val_loss_step=8.660, val_loss_epoch=10.90, loss_epoch=95.20]Epoch 66, Train_loss: 95.187\n",
      "Epoch 67: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=67.50, val_loss_step=8.370, val_loss_epoch=10.80, loss_epoch=93.60]Epoch 67, Train_loss: 93.551\n",
      "Epoch 68: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=72, loss_step=70.00, val_loss_step=8.420, val_loss_epoch=10.70, loss_epoch=93.70]Epoch 68, Train_loss: 93.673\n",
      "Epoch 69: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=86.70, val_loss_step=8.520, val_loss_epoch=10.90, loss_epoch=95.80]Epoch 69, Train_loss: 95.84\n",
      "Epoch 70: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=72, loss_step=80.60, val_loss_step=8.360, val_loss_epoch=10.70, loss_epoch=95.20]Epoch 70, Train_loss: 95.233\n",
      "Epoch 71: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=59.50, val_loss_step=8.440, val_loss_epoch=10.80, loss_epoch=92.60]Epoch 71, Train_loss: 92.644\n",
      "Epoch 72: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=90.90, val_loss_step=8.380, val_loss_epoch=10.70, loss_epoch=92.30]Epoch 72, Train_loss: 92.269\n",
      "Epoch 73: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=101.0, val_loss_step=8.500, val_loss_epoch=10.90, loss_epoch=96.00]Epoch 73, Train_loss: 95.972\n",
      "Epoch 74: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=72, loss_step=83.80, val_loss_step=8.400, val_loss_epoch=10.70, loss_epoch=95.40]Epoch 74, Train_loss: 95.43\n",
      "Epoch 75: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=72, loss_step=92.50, val_loss_step=8.460, val_loss_epoch=10.70, loss_epoch=96.60]Epoch 75, Train_loss: 96.579\n",
      "Epoch 76: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s, v_num=72, loss_step=88.60, val_loss_step=8.300, val_loss_epoch=10.70, loss_epoch=97.50]Epoch 76, Train_loss: 97.517\n",
      "Epoch 77: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=72.60, val_loss_step=8.400, val_loss_epoch=10.60, loss_epoch=91.90]Epoch 77, Train_loss: 91.866\n",
      "Epoch 78: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=80.60, val_loss_step=8.510, val_loss_epoch=11.00, loss_epoch=93.30]Epoch 78, Train_loss: 93.343\n",
      "Epoch 79: 100%|██████████| 150/150 [01:01<00:00,  2.46it/s, v_num=72, loss_step=61.30, val_loss_step=8.470, val_loss_epoch=10.70, loss_epoch=93.20]Epoch 79, Train_loss: 93.183\n",
      "Epoch 80: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=101.0, val_loss_step=8.400, val_loss_epoch=10.80, loss_epoch=96.80]Epoch 80, Train_loss: 96.775\n",
      "Epoch 81: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=92.60, val_loss_step=8.350, val_loss_epoch=10.60, loss_epoch=93.50]Epoch 81, Train_loss: 93.494\n",
      "Epoch 82: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=99.90, val_loss_step=8.570, val_loss_epoch=11.00, loss_epoch=93.20]Epoch 82, Train_loss: 93.22\n",
      "Epoch 83: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=78.30, val_loss_step=8.390, val_loss_epoch=11.10, loss_epoch=94.30]Epoch 83, Train_loss: 94.298\n",
      "Epoch 84: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=86.60, val_loss_step=8.400, val_loss_epoch=11.00, loss_epoch=95.40]Epoch 84, Train_loss: 95.389\n",
      "Epoch 85: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=96.90, val_loss_step=8.430, val_loss_epoch=10.80, loss_epoch=94.40]Epoch 85, Train_loss: 94.373\n",
      "Epoch 86: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=69.90, val_loss_step=8.590, val_loss_epoch=10.90, loss_epoch=93.90]Epoch 86, Train_loss: 93.882\n",
      "Epoch 87: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=70.00, val_loss_step=8.410, val_loss_epoch=10.80, loss_epoch=97.10]Epoch 87, Train_loss: 97.13\n",
      "Epoch 88: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=91.80, val_loss_step=8.440, val_loss_epoch=10.70, loss_epoch=95.10]Epoch 88, Train_loss: 95.127\n",
      "Epoch 89: 100%|██████████| 150/150 [01:00<00:00,  2.46it/s, v_num=72, loss_step=122.0, val_loss_step=8.390, val_loss_epoch=10.80, loss_epoch=97.20]Epoch 89, Train_loss: 97.15\n",
      "Epoch 90: 100%|██████████| 150/150 [01:00<00:00,  2.48it/s, v_num=72, loss_step=77.70, val_loss_step=8.390, val_loss_epoch=10.70, loss_epoch=94.70]Epoch 90, Train_loss: 94.736\n",
      "Epoch 91: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s, v_num=72, loss_step=58.90, val_loss_step=8.530, val_loss_epoch=10.90, loss_epoch=94.50]Epoch 91, Train_loss: 94.459\n",
      "Epoch 92: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=124.0, val_loss_step=8.330, val_loss_epoch=10.70, loss_epoch=95.00]Epoch 92, Train_loss: 95.028\n",
      "Epoch 93: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=120.0, val_loss_step=8.460, val_loss_epoch=10.90, loss_epoch=94.40]Epoch 93, Train_loss: 94.446\n",
      "Epoch 94: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s, v_num=72, loss_step=137.0, val_loss_step=8.430, val_loss_epoch=10.70, loss_epoch=96.90]Epoch 94, Train_loss: 96.897\n",
      "Epoch 95: 100%|██████████| 150/150 [01:00<00:00,  2.47it/s, v_num=72, loss_step=46.30, val_loss_step=8.550, val_loss_epoch=11.00, loss_epoch=92.60]Epoch 95, Train_loss: 92.633\n",
      "Epoch 96: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=91.00, val_loss_step=8.430, val_loss_epoch=10.80, loss_epoch=94.10]Epoch 96, Train_loss: 94.063\n",
      "Epoch 97: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s, v_num=72, loss_step=94.10, val_loss_step=8.500, val_loss_epoch=10.70, loss_epoch=95.20]Epoch 97, Train_loss: 95.206\n",
      "Epoch 98: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=72, loss_step=67.00, val_loss_step=8.370, val_loss_epoch=10.80, loss_epoch=96.10]Epoch 98, Train_loss: 96.104\n",
      "Epoch 99: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=72, loss_step=99.10, val_loss_step=8.440, val_loss_epoch=10.80, loss_epoch=94.50]Epoch 99, Train_loss: 94.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=72, loss_step=99.10, val_loss_step=8.440, val_loss_epoch=10.80, loss_epoch=94.50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.280    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=73, loss_step=184.0, val_loss_step=54.80, val_loss_epoch=76.80, loss_epoch=629.0]Epoch 0, Train_loss: 628.699\n",
      "Epoch 1: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=107.0, val_loss_step=57.30, val_loss_epoch=70.30, loss_epoch=163.0]Epoch 1, Train_loss: 162.797\n",
      "Epoch 2: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=104.0, val_loss_step=28.90, val_loss_epoch=40.80, loss_epoch=154.0]Epoch 2, Train_loss: 153.609\n",
      "Epoch 3: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=158.0, val_loss_step=31.40, val_loss_epoch=42.90, loss_epoch=147.0]Epoch 3, Train_loss: 146.545\n",
      "Epoch 4: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=121.0, val_loss_step=23.50, val_loss_epoch=33.00, loss_epoch=131.0]Epoch 4, Train_loss: 130.902\n",
      "Epoch 5: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=133.0, val_loss_step=21.10, val_loss_epoch=29.00, loss_epoch=123.0]Epoch 5, Train_loss: 123.238\n",
      "Epoch 6: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=73, loss_step=115.0, val_loss_step=18.40, val_loss_epoch=27.10, loss_epoch=117.0]Epoch 6, Train_loss: 116.786\n",
      "Epoch 7: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=116.0, val_loss_step=12.40, val_loss_epoch=19.70, loss_epoch=117.0]Epoch 7, Train_loss: 116.959\n",
      "Epoch 8: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=63.10, val_loss_step=12.30, val_loss_epoch=18.70, loss_epoch=111.0]Epoch 8, Train_loss: 111.234\n",
      "Epoch 9: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=134.0, val_loss_step=13.70, val_loss_epoch=20.30, loss_epoch=107.0]Epoch 9, Train_loss: 107.22\n",
      "Epoch 10: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=73, loss_step=104.0, val_loss_step=11.70, val_loss_epoch=18.80, loss_epoch=108.0]Epoch 10, Train_loss: 108.244\n",
      "Epoch 11: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=73, loss_step=101.0, val_loss_step=11.00, val_loss_epoch=16.60, loss_epoch=105.0]Epoch 11, Train_loss: 105.253\n",
      "Epoch 12: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=73, loss_step=106.0, val_loss_step=11.80, val_loss_epoch=17.20, loss_epoch=98.30]Epoch 12, Train_loss: 98.305\n",
      "Epoch 13: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=73, loss_step=114.0, val_loss_step=10.10, val_loss_epoch=15.90, loss_epoch=103.0]Epoch 13, Train_loss: 103.153\n",
      "Epoch 14: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=73, loss_step=127.0, val_loss_step=12.60, val_loss_epoch=15.70, loss_epoch=102.0]Epoch 14, Train_loss: 101.725\n",
      "Epoch 15: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s, v_num=73, loss_step=79.50, val_loss_step=14.80, val_loss_epoch=17.90, loss_epoch=95.40]Epoch 15, Train_loss: 95.4\n",
      "Epoch 16: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=73, loss_step=66.10, val_loss_step=9.760, val_loss_epoch=15.00, loss_epoch=100.0]Epoch 16, Train_loss: 100.408\n",
      "Epoch 17: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=73, loss_step=96.10, val_loss_step=9.040, val_loss_epoch=13.20, loss_epoch=97.40]Epoch 17, Train_loss: 97.365\n",
      "Epoch 18: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=73, loss_step=80.80, val_loss_step=9.830, val_loss_epoch=14.40, loss_epoch=99.20]Epoch 18, Train_loss: 99.219\n",
      "Epoch 19: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=73, loss_step=72.60, val_loss_step=8.940, val_loss_epoch=12.60, loss_epoch=96.70]Epoch 19, Train_loss: 96.736\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 20: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=73, loss_step=92.60, val_loss_step=8.590, val_loss_epoch=11.90, loss_epoch=96.40]Epoch 20, Train_loss: 96.395\n",
      "Epoch 21: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=73, loss_step=82.60, val_loss_step=8.550, val_loss_epoch=11.60, loss_epoch=92.20]Epoch 21, Train_loss: 92.205\n",
      "Epoch 22: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=73, loss_step=97.00, val_loss_step=8.440, val_loss_epoch=11.50, loss_epoch=92.80]Epoch 22, Train_loss: 92.793\n",
      "Epoch 23: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s, v_num=73, loss_step=104.0, val_loss_step=9.510, val_loss_epoch=12.10, loss_epoch=94.40]Epoch 23, Train_loss: 94.357\n",
      "Epoch 24: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=73, loss_step=77.70, val_loss_step=9.330, val_loss_epoch=11.70, loss_epoch=92.80]Epoch 24, Train_loss: 92.788\n",
      "Epoch 25: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=73, loss_step=70.90, val_loss_step=8.480, val_loss_epoch=11.70, loss_epoch=94.70]Epoch 25, Train_loss: 94.733\n",
      "Epoch 00026: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 26: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=74.00, val_loss_step=8.370, val_loss_epoch=11.00, loss_epoch=90.90]Epoch 26, Train_loss: 90.867\n",
      "Epoch 27: 100%|██████████| 150/150 [01:03<00:00,  2.34it/s, v_num=73, loss_step=70.30, val_loss_step=8.600, val_loss_epoch=10.90, loss_epoch=89.70]Epoch 27, Train_loss: 89.701\n",
      "Epoch 28: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=71.20, val_loss_step=8.710, val_loss_epoch=11.10, loss_epoch=94.30]Epoch 28, Train_loss: 94.284\n",
      "Epoch 29: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s, v_num=73, loss_step=138.0, val_loss_step=8.810, val_loss_epoch=10.70, loss_epoch=94.90]Epoch 29, Train_loss: 94.944\n",
      "Epoch 30: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=103.0, val_loss_step=9.370, val_loss_epoch=11.20, loss_epoch=93.10]Epoch 30, Train_loss: 93.058\n",
      "Epoch 31: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=89.50, val_loss_step=8.770, val_loss_epoch=10.90, loss_epoch=91.50]Epoch 31, Train_loss: 91.459\n",
      "Epoch 00032: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 32: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=89.70, val_loss_step=8.700, val_loss_epoch=10.90, loss_epoch=99.70]Epoch 32, Train_loss: 99.741\n",
      "Epoch 33: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=148.0, val_loss_step=9.430, val_loss_epoch=11.20, loss_epoch=94.60]Epoch 33, Train_loss: 94.636\n",
      "Epoch 34: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=84.80, val_loss_step=9.000, val_loss_epoch=11.00, loss_epoch=94.30]Epoch 34, Train_loss: 94.334\n",
      "Epoch 35: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=73, loss_step=78.60, val_loss_step=8.840, val_loss_epoch=10.70, loss_epoch=96.00]Epoch 35, Train_loss: 95.985\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 36: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=101.0, val_loss_step=8.940, val_loss_epoch=10.90, loss_epoch=92.90]Epoch 36, Train_loss: 92.882\n",
      "Epoch 37: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=86.80, val_loss_step=9.040, val_loss_epoch=10.90, loss_epoch=98.20]Epoch 37, Train_loss: 98.217\n",
      "Epoch 38: 100%|██████████| 150/150 [01:03<00:00,  2.34it/s, v_num=73, loss_step=106.0, val_loss_step=9.000, val_loss_epoch=11.00, loss_epoch=90.70]Epoch 38, Train_loss: 90.727\n",
      "Epoch 39: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=101.0, val_loss_step=8.780, val_loss_epoch=10.80, loss_epoch=89.30]Epoch 39, Train_loss: 89.346\n",
      "Epoch 00040: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 40: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=117.0, val_loss_step=8.710, val_loss_epoch=10.80, loss_epoch=93.40]Epoch 40, Train_loss: 93.396\n",
      "Epoch 41: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=80.30, val_loss_step=8.990, val_loss_epoch=11.00, loss_epoch=92.40]Epoch 41, Train_loss: 92.405\n",
      "Epoch 42: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=79.70, val_loss_step=8.910, val_loss_epoch=10.90, loss_epoch=93.00]Epoch 42, Train_loss: 92.976\n",
      "Epoch 43: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=80.40, val_loss_step=8.850, val_loss_epoch=10.80, loss_epoch=93.70]Epoch 43, Train_loss: 93.671\n",
      "Epoch 00044: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 44: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=117.0, val_loss_step=8.840, val_loss_epoch=10.80, loss_epoch=93.70]Epoch 44, Train_loss: 93.662\n",
      "Epoch 45: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=99.10, val_loss_step=8.980, val_loss_epoch=10.90, loss_epoch=95.40]Epoch 45, Train_loss: 95.39\n",
      "Epoch 46: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=117.0, val_loss_step=8.870, val_loss_epoch=10.80, loss_epoch=89.70]Epoch 46, Train_loss: 89.664\n",
      "Epoch 47: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=84.40, val_loss_step=8.850, val_loss_epoch=10.90, loss_epoch=94.70]Epoch 47, Train_loss: 94.742\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 48: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=65.30, val_loss_step=8.920, val_loss_epoch=10.80, loss_epoch=95.10]Epoch 48, Train_loss: 95.118\n",
      "Epoch 49: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=96.60, val_loss_step=9.250, val_loss_epoch=11.00, loss_epoch=92.70]Epoch 49, Train_loss: 92.716\n",
      "Epoch 50: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=80.10, val_loss_step=8.860, val_loss_epoch=10.80, loss_epoch=92.00]Epoch 50, Train_loss: 91.964\n",
      "Epoch 51: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=87.80, val_loss_step=9.150, val_loss_epoch=11.10, loss_epoch=96.00]Epoch 51, Train_loss: 95.958\n",
      "Epoch 00052: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 52: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=106.0, val_loss_step=8.900, val_loss_epoch=11.00, loss_epoch=92.70]Epoch 52, Train_loss: 92.65\n",
      "Epoch 53: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=73, loss_step=113.0, val_loss_step=8.660, val_loss_epoch=10.90, loss_epoch=93.10]Epoch 53, Train_loss: 93.111\n",
      "Epoch 54: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=73, loss_step=98.50, val_loss_step=8.750, val_loss_epoch=10.80, loss_epoch=93.70]Epoch 54, Train_loss: 93.73\n",
      "Epoch 55: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=104.0, val_loss_step=8.720, val_loss_epoch=11.10, loss_epoch=91.80]Epoch 55, Train_loss: 91.803\n",
      "Epoch 00056: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 56: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=95.60, val_loss_step=9.100, val_loss_epoch=11.00, loss_epoch=91.50]Epoch 56, Train_loss: 91.478\n",
      "Epoch 57: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=92.60, val_loss_step=8.860, val_loss_epoch=10.80, loss_epoch=91.70]Epoch 57, Train_loss: 91.73\n",
      "Epoch 58: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=72.00, val_loss_step=8.960, val_loss_epoch=10.90, loss_epoch=92.60]Epoch 58, Train_loss: 92.618\n",
      "Epoch 59: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=89.30, val_loss_step=8.990, val_loss_epoch=11.00, loss_epoch=94.70]Epoch 59, Train_loss: 94.671\n",
      "Epoch 60: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=82.30, val_loss_step=8.650, val_loss_epoch=10.80, loss_epoch=92.30]Epoch 60, Train_loss: 92.28\n",
      "Epoch 61: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=59.60, val_loss_step=8.770, val_loss_epoch=10.70, loss_epoch=96.10]Epoch 61, Train_loss: 96.066\n",
      "Epoch 62: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=153.0, val_loss_step=8.940, val_loss_epoch=11.10, loss_epoch=92.00]Epoch 62, Train_loss: 92.014\n",
      "Epoch 63: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=117.0, val_loss_step=9.000, val_loss_epoch=10.90, loss_epoch=92.40]Epoch 63, Train_loss: 92.363\n",
      "Epoch 64: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=116.0, val_loss_step=8.890, val_loss_epoch=10.90, loss_epoch=92.20]Epoch 64, Train_loss: 92.206\n",
      "Epoch 65: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=84.00, val_loss_step=9.330, val_loss_epoch=11.10, loss_epoch=93.50]Epoch 65, Train_loss: 93.511\n",
      "Epoch 66: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=68.30, val_loss_step=8.990, val_loss_epoch=11.00, loss_epoch=90.40]Epoch 66, Train_loss: 90.423\n",
      "Epoch 67: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=92.00, val_loss_step=9.010, val_loss_epoch=11.30, loss_epoch=91.10]Epoch 67, Train_loss: 91.071\n",
      "Epoch 68: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=52.90, val_loss_step=9.030, val_loss_epoch=10.80, loss_epoch=93.40]Epoch 68, Train_loss: 93.353\n",
      "Epoch 69: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=73, loss_step=113.0, val_loss_step=9.330, val_loss_epoch=11.00, loss_epoch=94.60]Epoch 69, Train_loss: 94.598\n",
      "Epoch 70: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=73, loss_step=106.0, val_loss_step=9.060, val_loss_epoch=10.90, loss_epoch=93.80]Epoch 70, Train_loss: 93.81\n",
      "Epoch 71: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s, v_num=73, loss_step=133.0, val_loss_step=8.670, val_loss_epoch=11.00, loss_epoch=90.10]Epoch 71, Train_loss: 90.067\n",
      "Epoch 72: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=73, loss_step=67.20, val_loss_step=8.660, val_loss_epoch=10.90, loss_epoch=93.30]Epoch 72, Train_loss: 93.251\n",
      "Epoch 73: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=73, loss_step=82.50, val_loss_step=9.530, val_loss_epoch=11.30, loss_epoch=91.40]Epoch 73, Train_loss: 91.443\n",
      "Epoch 74: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=104.0, val_loss_step=9.180, val_loss_epoch=11.10, loss_epoch=94.50]Epoch 74, Train_loss: 94.534\n",
      "Epoch 75: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=86.60, val_loss_step=8.730, val_loss_epoch=10.90, loss_epoch=91.40]Epoch 75, Train_loss: 91.426\n",
      "Epoch 76: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=73, loss_step=123.0, val_loss_step=8.880, val_loss_epoch=10.70, loss_epoch=94.40]Epoch 76, Train_loss: 94.364\n",
      "Epoch 77: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=107.0, val_loss_step=8.690, val_loss_epoch=10.60, loss_epoch=94.90]Epoch 77, Train_loss: 94.932\n",
      "Epoch 78: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=147.0, val_loss_step=9.010, val_loss_epoch=10.80, loss_epoch=95.10]Epoch 78, Train_loss: 95.089\n",
      "Epoch 79: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=60.70, val_loss_step=8.670, val_loss_epoch=10.80, loss_epoch=91.90]Epoch 79, Train_loss: 91.862\n",
      "Epoch 80: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=155.0, val_loss_step=8.730, val_loss_epoch=10.70, loss_epoch=92.50]Epoch 80, Train_loss: 92.538\n",
      "Epoch 81: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=80.40, val_loss_step=8.750, val_loss_epoch=10.80, loss_epoch=91.10]Epoch 81, Train_loss: 91.145\n",
      "Epoch 82: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=69.20, val_loss_step=8.920, val_loss_epoch=10.90, loss_epoch=92.20]Epoch 82, Train_loss: 92.188\n",
      "Epoch 83: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=83.20, val_loss_step=8.860, val_loss_epoch=10.90, loss_epoch=88.80]Epoch 83, Train_loss: 88.842\n",
      "Epoch 84: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=73, loss_step=61.40, val_loss_step=9.260, val_loss_epoch=11.10, loss_epoch=93.60]Epoch 84, Train_loss: 93.587\n",
      "Epoch 85: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=73, loss_step=78.10, val_loss_step=8.960, val_loss_epoch=10.90, loss_epoch=92.50]Epoch 85, Train_loss: 92.467\n",
      "Epoch 86: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=45.10, val_loss_step=9.080, val_loss_epoch=11.10, loss_epoch=91.70]Epoch 86, Train_loss: 91.717\n",
      "Epoch 87: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=73, loss_step=104.0, val_loss_step=8.920, val_loss_epoch=10.80, loss_epoch=93.00]Epoch 87, Train_loss: 92.982\n",
      "Epoch 88: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=107.0, val_loss_step=8.950, val_loss_epoch=10.80, loss_epoch=90.20]Epoch 88, Train_loss: 90.198\n",
      "Epoch 89: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=73, loss_step=96.10, val_loss_step=8.920, val_loss_epoch=10.90, loss_epoch=89.20]Epoch 89, Train_loss: 89.165\n",
      "Epoch 90: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=109.0, val_loss_step=9.100, val_loss_epoch=10.90, loss_epoch=96.50]Epoch 90, Train_loss: 96.457\n",
      "Epoch 91: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=76.80, val_loss_step=8.730, val_loss_epoch=10.70, loss_epoch=92.30]Epoch 91, Train_loss: 92.333\n",
      "Epoch 92: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=73.80, val_loss_step=9.030, val_loss_epoch=10.90, loss_epoch=91.40]Epoch 92, Train_loss: 91.39\n",
      "Epoch 93: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=123.0, val_loss_step=8.810, val_loss_epoch=11.00, loss_epoch=91.20]Epoch 93, Train_loss: 91.21\n",
      "Epoch 94: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=77.10, val_loss_step=8.850, val_loss_epoch=10.90, loss_epoch=93.70]Epoch 94, Train_loss: 93.725\n",
      "Epoch 95: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=73, loss_step=53.60, val_loss_step=9.250, val_loss_epoch=11.00, loss_epoch=90.50]Epoch 95, Train_loss: 90.475\n",
      "Epoch 96: 100%|██████████| 150/150 [01:02<00:00,  2.41it/s, v_num=73, loss_step=93.30, val_loss_step=8.760, val_loss_epoch=10.80, loss_epoch=92.10]Epoch 96, Train_loss: 92.057\n",
      "Epoch 97: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=73, loss_step=119.0, val_loss_step=9.360, val_loss_epoch=11.10, loss_epoch=91.10]Epoch 97, Train_loss: 91.076\n",
      "Epoch 98: 100%|██████████| 150/150 [01:03<00:00,  2.34it/s, v_num=73, loss_step=89.50, val_loss_step=8.930, val_loss_epoch=10.90, loss_epoch=92.00]Epoch 98, Train_loss: 92.01\n",
      "Epoch 99: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=73, loss_step=107.0, val_loss_step=8.750, val_loss_epoch=10.70, loss_epoch=95.30]Epoch 99, Train_loss: 95.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=73, loss_step=107.0, val_loss_step=8.750, val_loss_epoch=10.70, loss_epoch=95.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 5.8 M \n",
      "----------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.209    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=118.0, val_loss_step=34.60, val_loss_epoch=49.20, loss_epoch=637.0]Epoch 0, Train_loss: 637.32\n",
      "Epoch 1: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=74, loss_step=158.0, val_loss_step=30.10, val_loss_epoch=42.80, loss_epoch=164.0]Epoch 1, Train_loss: 163.715\n",
      "Epoch 2: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=183.0, val_loss_step=37.40, val_loss_epoch=48.20, loss_epoch=159.0]Epoch 2, Train_loss: 158.77\n",
      "Epoch 3: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=102.0, val_loss_step=24.40, val_loss_epoch=38.30, loss_epoch=149.0]Epoch 3, Train_loss: 148.858\n",
      "Epoch 4: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=126.0, val_loss_step=29.60, val_loss_epoch=38.10, loss_epoch=145.0]Epoch 4, Train_loss: 144.853\n",
      "Epoch 5: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=90.40, val_loss_step=17.90, val_loss_epoch=27.70, loss_epoch=133.0]Epoch 5, Train_loss: 132.581\n",
      "Epoch 6: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=83.90, val_loss_step=16.90, val_loss_epoch=31.10, loss_epoch=121.0]Epoch 6, Train_loss: 120.703\n",
      "Epoch 7: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=205.0, val_loss_step=15.30, val_loss_epoch=25.80, loss_epoch=120.0]Epoch 7, Train_loss: 120.215\n",
      "Epoch 8: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=66.60, val_loss_step=16.90, val_loss_epoch=25.60, loss_epoch=116.0]Epoch 8, Train_loss: 116.336\n",
      "Epoch 9: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=103.0, val_loss_step=23.60, val_loss_epoch=30.80, loss_epoch=113.0]Epoch 9, Train_loss: 112.771\n",
      "Epoch 10: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=92.30, val_loss_step=14.90, val_loss_epoch=22.80, loss_epoch=112.0]Epoch 10, Train_loss: 112.201\n",
      "Epoch 11: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=83.80, val_loss_step=12.10, val_loss_epoch=20.30, loss_epoch=109.0]Epoch 11, Train_loss: 109.059\n",
      "Epoch 12: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=74.80, val_loss_step=16.30, val_loss_epoch=25.80, loss_epoch=106.0]Epoch 12, Train_loss: 106.073\n",
      "Epoch 13: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=108.0, val_loss_step=25.40, val_loss_epoch=29.50, loss_epoch=104.0]Epoch 13, Train_loss: 104.426\n",
      "Epoch 14: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=96.00, val_loss_step=15.30, val_loss_epoch=21.30, loss_epoch=100.0]Epoch 14, Train_loss: 100.159\n",
      "Epoch 15: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=114.0, val_loss_step=10.20, val_loss_epoch=16.50, loss_epoch=105.0]Epoch 15, Train_loss: 105.376\n",
      "Epoch 16: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=72.30, val_loss_step=11.70, val_loss_epoch=17.00, loss_epoch=102.0]Epoch 16, Train_loss: 101.884\n",
      "Epoch 17: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=109.0, val_loss_step=11.10, val_loss_epoch=16.00, loss_epoch=99.90]Epoch 17, Train_loss: 99.873\n",
      "Epoch 18: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=98.40, val_loss_step=12.80, val_loss_epoch=18.80, loss_epoch=98.80]Epoch 18, Train_loss: 98.8\n",
      "Epoch 19: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=66.60, val_loss_step=10.40, val_loss_epoch=14.90, loss_epoch=98.90]Epoch 19, Train_loss: 98.925\n",
      "Epoch 20: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=85.10, val_loss_step=12.10, val_loss_epoch=15.30, loss_epoch=97.20]Epoch 20, Train_loss: 97.165\n",
      "Epoch 21: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=91.10, val_loss_step=9.430, val_loss_epoch=12.90, loss_epoch=96.10]Epoch 21, Train_loss: 96.141\n",
      "Epoch 22: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=71.60, val_loss_step=9.470, val_loss_epoch=13.60, loss_epoch=97.20]Epoch 22, Train_loss: 97.245\n",
      "Epoch 23: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=65.10, val_loss_step=9.690, val_loss_epoch=12.80, loss_epoch=94.70]Epoch 23, Train_loss: 94.741\n",
      "Epoch 24: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=72.20, val_loss_step=10.30, val_loss_epoch=13.20, loss_epoch=98.50]Epoch 24, Train_loss: 98.48\n",
      "Epoch 25: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=96.70, val_loss_step=9.950, val_loss_epoch=12.50, loss_epoch=92.10]Epoch 25, Train_loss: 92.055\n",
      "Epoch 26: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=104.0, val_loss_step=8.730, val_loss_epoch=11.70, loss_epoch=96.90]Epoch 26, Train_loss: 96.884\n",
      "Epoch 27: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=110.0, val_loss_step=10.40, val_loss_epoch=12.70, loss_epoch=92.10]Epoch 27, Train_loss: 92.065\n",
      "Epoch 28: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=74, loss_step=109.0, val_loss_step=10.40, val_loss_epoch=13.60, loss_epoch=94.50]Epoch 28, Train_loss: 94.453\n",
      "Epoch 29: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=84.50, val_loss_step=10.50, val_loss_epoch=12.60, loss_epoch=99.60]Epoch 29, Train_loss: 99.636\n",
      "Epoch 00030: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 30: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=82.00, val_loss_step=8.880, val_loss_epoch=10.60, loss_epoch=89.80]Epoch 30, Train_loss: 89.827\n",
      "Epoch 31: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=86.70, val_loss_step=8.730, val_loss_epoch=10.90, loss_epoch=90.20]Epoch 31, Train_loss: 90.207\n",
      "Epoch 32: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=93.90, val_loss_step=9.460, val_loss_epoch=10.60, loss_epoch=93.80]Epoch 32, Train_loss: 93.831\n",
      "Epoch 33: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=90.40, val_loss_step=9.230, val_loss_epoch=10.40, loss_epoch=93.60]Epoch 33, Train_loss: 93.588\n",
      "Epoch 34: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=149.0, val_loss_step=8.420, val_loss_epoch=9.990, loss_epoch=91.80]Epoch 34, Train_loss: 91.79\n",
      "Epoch 00035: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 35: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=108.0, val_loss_step=9.140, val_loss_epoch=9.880, loss_epoch=94.50]Epoch 35, Train_loss: 94.487\n",
      "Epoch 36: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=34.40, val_loss_step=8.840, val_loss_epoch=9.700, loss_epoch=86.90]Epoch 36, Train_loss: 86.932\n",
      "Epoch 37: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=102.0, val_loss_step=8.340, val_loss_epoch=9.770, loss_epoch=89.90]Epoch 37, Train_loss: 89.867\n",
      "Epoch 38: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=115.0, val_loss_step=8.960, val_loss_epoch=9.840, loss_epoch=89.90]Epoch 38, Train_loss: 89.867\n",
      "Epoch 39: 100%|██████████| 150/150 [01:11<00:00,  2.08it/s, v_num=74, loss_step=50.60, val_loss_step=8.840, val_loss_epoch=9.760, loss_epoch=90.00]Epoch 39, Train_loss: 89.972\n",
      "Epoch 40: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=104.0, val_loss_step=8.760, val_loss_epoch=9.620, loss_epoch=91.50]Epoch 40, Train_loss: 91.527\n",
      "Epoch 00041: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 41: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=73.50, val_loss_step=8.930, val_loss_epoch=9.970, loss_epoch=89.70]Epoch 41, Train_loss: 89.702\n",
      "Epoch 42: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=126.0, val_loss_step=9.340, val_loss_epoch=9.830, loss_epoch=93.60]Epoch 42, Train_loss: 93.608\n",
      "Epoch 43: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=92.30, val_loss_step=8.980, val_loss_epoch=9.810, loss_epoch=89.60]Epoch 43, Train_loss: 89.612\n",
      "Epoch 44: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=99.10, val_loss_step=9.160, val_loss_epoch=9.770, loss_epoch=92.30]Epoch 44, Train_loss: 92.326\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 45: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=48.70, val_loss_step=9.010, val_loss_epoch=9.630, loss_epoch=89.80]Epoch 45, Train_loss: 89.759\n",
      "Epoch 46: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=75.10, val_loss_step=8.790, val_loss_epoch=9.820, loss_epoch=93.20]Epoch 46, Train_loss: 93.159\n",
      "Epoch 47: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=102.0, val_loss_step=9.230, val_loss_epoch=9.750, loss_epoch=90.00]Epoch 47, Train_loss: 90.03\n",
      "Epoch 48: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=106.0, val_loss_step=9.200, val_loss_epoch=9.790, loss_epoch=91.10]Epoch 48, Train_loss: 91.091\n",
      "Epoch 00049: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 49: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=97.30, val_loss_step=9.150, val_loss_epoch=9.730, loss_epoch=89.80]Epoch 49, Train_loss: 89.772\n",
      "Epoch 50: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=174.0, val_loss_step=9.090, val_loss_epoch=9.700, loss_epoch=91.10]Epoch 50, Train_loss: 91.126\n",
      "Epoch 51: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=74, loss_step=131.0, val_loss_step=9.170, val_loss_epoch=9.680, loss_epoch=87.70]Epoch 51, Train_loss: 87.671\n",
      "Epoch 52: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=94.40, val_loss_step=8.760, val_loss_epoch=9.900, loss_epoch=90.60]Epoch 52, Train_loss: 90.608\n",
      "Epoch 00053: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 53: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=40.30, val_loss_step=8.900, val_loss_epoch=9.720, loss_epoch=90.20]Epoch 53, Train_loss: 90.249\n",
      "Epoch 54: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=75.90, val_loss_step=9.080, val_loss_epoch=9.710, loss_epoch=92.60]Epoch 54, Train_loss: 92.632\n",
      "Epoch 55: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=74, loss_step=97.60, val_loss_step=8.920, val_loss_epoch=9.810, loss_epoch=88.10]Epoch 55, Train_loss: 88.139\n",
      "Epoch 56: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=153.0, val_loss_step=9.030, val_loss_epoch=9.700, loss_epoch=92.40]Epoch 56, Train_loss: 92.42\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 57: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=70.70, val_loss_step=8.890, val_loss_epoch=9.620, loss_epoch=88.60]Epoch 57, Train_loss: 88.553\n",
      "Epoch 58: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=84.10, val_loss_step=9.000, val_loss_epoch=9.710, loss_epoch=95.60]Epoch 58, Train_loss: 95.639\n",
      "Epoch 59: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=83.40, val_loss_step=8.910, val_loss_epoch=10.00, loss_epoch=94.70]Epoch 59, Train_loss: 94.672\n",
      "Epoch 60: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=78.80, val_loss_step=8.770, val_loss_epoch=9.720, loss_epoch=88.30]Epoch 60, Train_loss: 88.251\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 61: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=101.0, val_loss_step=9.040, val_loss_epoch=9.570, loss_epoch=91.80]Epoch 61, Train_loss: 91.84\n",
      "Epoch 62: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=65.40, val_loss_step=9.240, val_loss_epoch=9.790, loss_epoch=90.60]Epoch 62, Train_loss: 90.644\n",
      "Epoch 63: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=64.40, val_loss_step=8.980, val_loss_epoch=9.620, loss_epoch=90.70]Epoch 63, Train_loss: 90.667\n",
      "Epoch 64: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=74, loss_step=104.0, val_loss_step=9.180, val_loss_epoch=9.740, loss_epoch=93.00]Epoch 64, Train_loss: 92.978\n",
      "Epoch 00065: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 65: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=119.0, val_loss_step=8.960, val_loss_epoch=9.620, loss_epoch=91.90]Epoch 65, Train_loss: 91.864\n",
      "Epoch 66: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=107.0, val_loss_step=9.180, val_loss_epoch=9.740, loss_epoch=91.10]Epoch 66, Train_loss: 91.11\n",
      "Epoch 67: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=103.0, val_loss_step=8.830, val_loss_epoch=9.670, loss_epoch=93.40]Epoch 67, Train_loss: 93.436\n",
      "Epoch 68: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=98.60, val_loss_step=8.970, val_loss_epoch=9.640, loss_epoch=93.70]Epoch 68, Train_loss: 93.735\n",
      "Epoch 69: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=92.30, val_loss_step=8.790, val_loss_epoch=9.660, loss_epoch=88.90]Epoch 69, Train_loss: 88.897\n",
      "Epoch 70: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=79.20, val_loss_step=9.040, val_loss_epoch=9.590, loss_epoch=91.70]Epoch 70, Train_loss: 91.741\n",
      "Epoch 71: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=76.10, val_loss_step=8.970, val_loss_epoch=9.640, loss_epoch=90.30]Epoch 71, Train_loss: 90.335\n",
      "Epoch 72: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=125.0, val_loss_step=8.900, val_loss_epoch=9.690, loss_epoch=94.50]Epoch 72, Train_loss: 94.531\n",
      "Epoch 73: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=43.90, val_loss_step=9.250, val_loss_epoch=9.740, loss_epoch=91.20]Epoch 73, Train_loss: 91.233\n",
      "Epoch 74: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=77.10, val_loss_step=8.900, val_loss_epoch=9.650, loss_epoch=94.90]Epoch 74, Train_loss: 94.86\n",
      "Epoch 75: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=76.80, val_loss_step=9.230, val_loss_epoch=9.760, loss_epoch=91.10]Epoch 75, Train_loss: 91.076\n",
      "Epoch 76: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=93.30, val_loss_step=9.010, val_loss_epoch=9.720, loss_epoch=92.80]Epoch 76, Train_loss: 92.808\n",
      "Epoch 77: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=61.20, val_loss_step=9.030, val_loss_epoch=9.780, loss_epoch=89.50]Epoch 77, Train_loss: 89.539\n",
      "Epoch 78: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=60.70, val_loss_step=9.110, val_loss_epoch=9.820, loss_epoch=89.40]Epoch 78, Train_loss: 89.397\n",
      "Epoch 79: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=76.10, val_loss_step=9.300, val_loss_epoch=9.870, loss_epoch=87.60]Epoch 79, Train_loss: 87.632\n",
      "Epoch 80: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=74, loss_step=93.10, val_loss_step=8.830, val_loss_epoch=9.810, loss_epoch=87.50]Epoch 80, Train_loss: 87.494\n",
      "Epoch 81: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=94.80, val_loss_step=9.040, val_loss_epoch=9.690, loss_epoch=94.90]Epoch 81, Train_loss: 94.873\n",
      "Epoch 82: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=74, loss_step=95.80, val_loss_step=8.730, val_loss_epoch=9.760, loss_epoch=91.50]Epoch 82, Train_loss: 91.538\n",
      "Epoch 83: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=70.30, val_loss_step=9.020, val_loss_epoch=9.740, loss_epoch=89.50]Epoch 83, Train_loss: 89.535\n",
      "Epoch 84: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=111.0, val_loss_step=9.010, val_loss_epoch=9.720, loss_epoch=91.90]Epoch 84, Train_loss: 91.949\n",
      "Epoch 85: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=46.50, val_loss_step=8.960, val_loss_epoch=9.640, loss_epoch=91.50]Epoch 85, Train_loss: 91.493\n",
      "Epoch 86: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=91.60, val_loss_step=9.080, val_loss_epoch=9.570, loss_epoch=97.20]Epoch 86, Train_loss: 97.212\n",
      "Epoch 87: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=91.60, val_loss_step=8.880, val_loss_epoch=9.530, loss_epoch=94.40]Epoch 87, Train_loss: 94.4\n",
      "Epoch 88: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=130.0, val_loss_step=9.430, val_loss_epoch=9.920, loss_epoch=91.60]Epoch 88, Train_loss: 91.633\n",
      "Epoch 89: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=97.50, val_loss_step=8.610, val_loss_epoch=9.620, loss_epoch=87.80]Epoch 89, Train_loss: 87.814\n",
      "Epoch 90: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=66.00, val_loss_step=9.050, val_loss_epoch=9.800, loss_epoch=88.60]Epoch 90, Train_loss: 88.626\n",
      "Epoch 91: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=53.50, val_loss_step=9.050, val_loss_epoch=9.680, loss_epoch=93.10]Epoch 91, Train_loss: 93.108\n",
      "Epoch 92: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=74, loss_step=65.00, val_loss_step=8.980, val_loss_epoch=9.850, loss_epoch=87.90]Epoch 92, Train_loss: 87.888\n",
      "Epoch 93: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=65.60, val_loss_step=9.050, val_loss_epoch=9.750, loss_epoch=91.80]Epoch 93, Train_loss: 91.835\n",
      "Epoch 94: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=77.00, val_loss_step=8.890, val_loss_epoch=9.630, loss_epoch=96.00]Epoch 94, Train_loss: 95.964\n",
      "Epoch 95: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=74, loss_step=61.80, val_loss_step=9.360, val_loss_epoch=9.780, loss_epoch=91.50]Epoch 95, Train_loss: 91.49\n",
      "Epoch 96: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s, v_num=74, loss_step=57.50, val_loss_step=8.940, val_loss_epoch=9.590, loss_epoch=88.30]Epoch 96, Train_loss: 88.334\n",
      "Epoch 97: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=130.0, val_loss_step=9.270, val_loss_epoch=9.800, loss_epoch=88.60]Epoch 97, Train_loss: 88.633\n",
      "Epoch 98: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=122.0, val_loss_step=9.350, val_loss_epoch=9.760, loss_epoch=90.70]Epoch 98, Train_loss: 90.692\n",
      "Epoch 99: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=74, loss_step=105.0, val_loss_step=9.000, val_loss_epoch=9.670, loss_epoch=90.70]Epoch 99, Train_loss: 90.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=74, loss_step=105.0, val_loss_step=9.000, val_loss_epoch=9.670, loss_epoch=90.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.283    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=192.0, val_loss_step=126.0, val_loss_epoch=123.0, loss_epoch=301.0]Epoch 0, Train_loss: 300.843\n",
      "Epoch 1: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=127.0, val_loss_step=23.50, val_loss_epoch=37.90, loss_epoch=150.0]Epoch 1, Train_loss: 149.774\n",
      "Epoch 2: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=114.0, val_loss_step=32.70, val_loss_epoch=40.40, loss_epoch=135.0]Epoch 2, Train_loss: 134.664\n",
      "Epoch 3: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=75.70, val_loss_step=22.20, val_loss_epoch=34.70, loss_epoch=125.0]Epoch 3, Train_loss: 124.508\n",
      "Epoch 4: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=158.0, val_loss_step=32.60, val_loss_epoch=40.80, loss_epoch=122.0]Epoch 4, Train_loss: 122.296\n",
      "Epoch 5: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=88.00, val_loss_step=24.50, val_loss_epoch=33.60, loss_epoch=120.0]Epoch 5, Train_loss: 119.814\n",
      "Epoch 6: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=95.40, val_loss_step=25.80, val_loss_epoch=30.00, loss_epoch=111.0]Epoch 6, Train_loss: 111.04\n",
      "Epoch 7: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=75, loss_step=108.0, val_loss_step=15.90, val_loss_epoch=22.40, loss_epoch=109.0]Epoch 7, Train_loss: 109.186\n",
      "Epoch 8: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=81.50, val_loss_step=16.90, val_loss_epoch=22.20, loss_epoch=106.0]Epoch 8, Train_loss: 105.684\n",
      "Epoch 9: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=78.30, val_loss_step=12.80, val_loss_epoch=18.80, loss_epoch=105.0]Epoch 9, Train_loss: 105.382\n",
      "Epoch 10: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=116.0, val_loss_step=19.10, val_loss_epoch=27.10, loss_epoch=103.0]Epoch 10, Train_loss: 102.694\n",
      "Epoch 11: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=38.30, val_loss_step=10.90, val_loss_epoch=16.10, loss_epoch=99.00]Epoch 11, Train_loss: 98.957\n",
      "Epoch 12: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=84.80, val_loss_step=10.70, val_loss_epoch=17.10, loss_epoch=102.0]Epoch 12, Train_loss: 101.87\n",
      "Epoch 13: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=75, loss_step=100.0, val_loss_step=11.00, val_loss_epoch=16.70, loss_epoch=96.70]Epoch 13, Train_loss: 96.711\n",
      "Epoch 14: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=80.40, val_loss_step=11.30, val_loss_epoch=14.10, loss_epoch=98.20]Epoch 14, Train_loss: 98.158\n",
      "Epoch 15: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=93.60, val_loss_step=8.710, val_loss_epoch=13.40, loss_epoch=94.50]Epoch 15, Train_loss: 94.467\n",
      "Epoch 16: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=152.0, val_loss_step=9.120, val_loss_epoch=13.10, loss_epoch=94.50]Epoch 16, Train_loss: 94.493\n",
      "Epoch 17: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=60.60, val_loss_step=10.40, val_loss_epoch=15.20, loss_epoch=95.90]Epoch 17, Train_loss: 95.917\n",
      "Epoch 18: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=68.50, val_loss_step=13.70, val_loss_epoch=16.60, loss_epoch=92.80]Epoch 18, Train_loss: 92.79\n",
      "Epoch 19: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=111.0, val_loss_step=10.70, val_loss_epoch=13.00, loss_epoch=95.70]Epoch 19, Train_loss: 95.723\n",
      "Epoch 20: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=84.10, val_loss_step=11.10, val_loss_epoch=13.50, loss_epoch=90.40]Epoch 20, Train_loss: 90.368\n",
      "Epoch 21: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=75, loss_step=97.70, val_loss_step=8.510, val_loss_epoch=11.50, loss_epoch=91.60]Epoch 21, Train_loss: 91.582\n",
      "Epoch 22: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=77.20, val_loss_step=8.440, val_loss_epoch=10.90, loss_epoch=88.20]Epoch 22, Train_loss: 88.233\n",
      "Epoch 23: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=46.80, val_loss_step=9.030, val_loss_epoch=12.00, loss_epoch=89.90]Epoch 23, Train_loss: 89.903\n",
      "Epoch 24: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=127.0, val_loss_step=10.60, val_loss_epoch=12.00, loss_epoch=96.70]Epoch 24, Train_loss: 96.688\n",
      "Epoch 25: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=96.60, val_loss_step=8.420, val_loss_epoch=11.10, loss_epoch=93.40]Epoch 25, Train_loss: 93.352\n",
      "Epoch 26: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=96.40, val_loss_step=11.30, val_loss_epoch=12.50, loss_epoch=91.10]Epoch 26, Train_loss: 91.143\n",
      "Epoch 00027: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 27: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=97.70, val_loss_step=8.390, val_loss_epoch=10.20, loss_epoch=88.00]Epoch 27, Train_loss: 87.968\n",
      "Epoch 28: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=75, loss_step=77.90, val_loss_step=8.060, val_loss_epoch=9.640, loss_epoch=86.00]Epoch 28, Train_loss: 86.013\n",
      "Epoch 29: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s, v_num=75, loss_step=72.20, val_loss_step=8.230, val_loss_epoch=9.880, loss_epoch=88.70]Epoch 29, Train_loss: 88.744\n",
      "Epoch 30: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=73.30, val_loss_step=8.640, val_loss_epoch=10.00, loss_epoch=87.00]Epoch 30, Train_loss: 86.976\n",
      "Epoch 31: 100%|██████████| 150/150 [01:02<00:00,  2.40it/s, v_num=75, loss_step=106.0, val_loss_step=8.620, val_loss_epoch=9.390, loss_epoch=82.80]Epoch 31, Train_loss: 82.825\n",
      "Epoch 32: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=86.10, val_loss_step=8.100, val_loss_epoch=9.960, loss_epoch=87.20]Epoch 32, Train_loss: 87.237\n",
      "Epoch 33: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=74.20, val_loss_step=8.270, val_loss_epoch=9.460, loss_epoch=91.40]Epoch 33, Train_loss: 91.415\n",
      "Epoch 34: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=103.0, val_loss_step=8.420, val_loss_epoch=9.570, loss_epoch=86.50]Epoch 34, Train_loss: 86.49\n",
      "Epoch 35: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=130.0, val_loss_step=8.380, val_loss_epoch=9.790, loss_epoch=86.60]Epoch 35, Train_loss: 86.594\n",
      "Epoch 00036: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 36: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=83.10, val_loss_step=8.680, val_loss_epoch=9.560, loss_epoch=85.70]Epoch 36, Train_loss: 85.666\n",
      "Epoch 37: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=80.70, val_loss_step=8.050, val_loss_epoch=9.080, loss_epoch=86.00]Epoch 37, Train_loss: 86.033\n",
      "Epoch 38: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=82.70, val_loss_step=7.780, val_loss_epoch=9.180, loss_epoch=85.20]Epoch 38, Train_loss: 85.237\n",
      "Epoch 39: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=90.40, val_loss_step=7.960, val_loss_epoch=8.880, loss_epoch=85.60]Epoch 39, Train_loss: 85.646\n",
      "Epoch 00040: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 40: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=81.60, val_loss_step=8.510, val_loss_epoch=9.220, loss_epoch=85.10]Epoch 40, Train_loss: 85.106\n",
      "Epoch 41: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=122.0, val_loss_step=8.410, val_loss_epoch=9.150, loss_epoch=85.60]Epoch 41, Train_loss: 85.576\n",
      "Epoch 42: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=91.00, val_loss_step=8.330, val_loss_epoch=9.060, loss_epoch=88.70]Epoch 42, Train_loss: 88.732\n",
      "Epoch 43: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=48.00, val_loss_step=8.710, val_loss_epoch=9.460, loss_epoch=81.90]Epoch 43, Train_loss: 81.86\n",
      "Epoch 44: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=57.50, val_loss_step=8.360, val_loss_epoch=9.150, loss_epoch=87.20]Epoch 44, Train_loss: 87.217\n",
      "Epoch 45: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=116.0, val_loss_step=8.220, val_loss_epoch=8.920, loss_epoch=87.10]Epoch 45, Train_loss: 87.069\n",
      "Epoch 46: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=95.50, val_loss_step=8.780, val_loss_epoch=9.190, loss_epoch=87.40]Epoch 46, Train_loss: 87.377\n",
      "Epoch 47: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=75, loss_step=93.60, val_loss_step=8.690, val_loss_epoch=9.090, loss_epoch=83.20]Epoch 47, Train_loss: 83.206\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 48: 100%|██████████| 150/150 [01:02<00:00,  2.38it/s, v_num=75, loss_step=87.30, val_loss_step=8.530, val_loss_epoch=9.080, loss_epoch=84.90]Epoch 48, Train_loss: 84.904\n",
      "Epoch 49: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=105.0, val_loss_step=9.030, val_loss_epoch=9.090, loss_epoch=89.70]Epoch 49, Train_loss: 89.678\n",
      "Epoch 50: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=112.0, val_loss_step=8.710, val_loss_epoch=9.030, loss_epoch=87.40]Epoch 50, Train_loss: 87.377\n",
      "Epoch 51: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=62.10, val_loss_step=8.780, val_loss_epoch=9.230, loss_epoch=80.90]Epoch 51, Train_loss: 80.869\n",
      "Epoch 52: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=110.0, val_loss_step=9.290, val_loss_epoch=9.480, loss_epoch=85.20]Epoch 52, Train_loss: 85.23\n",
      "Epoch 53: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=62.80, val_loss_step=8.780, val_loss_epoch=9.210, loss_epoch=88.80]Epoch 53, Train_loss: 88.758\n",
      "Epoch 54: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=104.0, val_loss_step=8.850, val_loss_epoch=9.150, loss_epoch=86.10]Epoch 54, Train_loss: 86.101\n",
      "Epoch 55: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=48.90, val_loss_step=8.840, val_loss_epoch=9.160, loss_epoch=88.80]Epoch 55, Train_loss: 88.761\n",
      "Epoch 00056: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 56: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=60.70, val_loss_step=9.210, val_loss_epoch=9.370, loss_epoch=86.60]Epoch 56, Train_loss: 86.602\n",
      "Epoch 57: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=69.80, val_loss_step=8.460, val_loss_epoch=8.960, loss_epoch=85.40]Epoch 57, Train_loss: 85.369\n",
      "Epoch 58: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=60.00, val_loss_step=8.880, val_loss_epoch=9.360, loss_epoch=87.20]Epoch 58, Train_loss: 87.2\n",
      "Epoch 59: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=108.0, val_loss_step=9.110, val_loss_epoch=9.210, loss_epoch=88.30]Epoch 59, Train_loss: 88.258\n",
      "Epoch 00060: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 60: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=117.0, val_loss_step=9.010, val_loss_epoch=9.310, loss_epoch=85.60]Epoch 60, Train_loss: 85.631\n",
      "Epoch 61: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=106.0, val_loss_step=8.820, val_loss_epoch=9.370, loss_epoch=84.00]Epoch 61, Train_loss: 83.955\n",
      "Epoch 62: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=77.70, val_loss_step=9.560, val_loss_epoch=9.730, loss_epoch=84.20]Epoch 62, Train_loss: 84.172\n",
      "Epoch 63: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=93.70, val_loss_step=9.700, val_loss_epoch=9.680, loss_epoch=86.60]Epoch 63, Train_loss: 86.61\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 64: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=125.0, val_loss_step=8.920, val_loss_epoch=9.110, loss_epoch=87.10]Epoch 64, Train_loss: 87.143\n",
      "Epoch 65: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=61.10, val_loss_step=8.670, val_loss_epoch=9.050, loss_epoch=87.40]Epoch 65, Train_loss: 87.354\n",
      "Epoch 66: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=73.50, val_loss_step=8.950, val_loss_epoch=9.230, loss_epoch=86.30]Epoch 66, Train_loss: 86.314\n",
      "Epoch 67: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=74.90, val_loss_step=8.850, val_loss_epoch=9.130, loss_epoch=88.80]Epoch 67, Train_loss: 88.752\n",
      "Epoch 00068: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 68: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=75, loss_step=108.0, val_loss_step=8.630, val_loss_epoch=9.090, loss_epoch=88.90]Epoch 68, Train_loss: 88.871\n",
      "Epoch 69: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=59.30, val_loss_step=8.520, val_loss_epoch=9.120, loss_epoch=87.90]Epoch 69, Train_loss: 87.939\n",
      "Epoch 70: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=75, loss_step=87.90, val_loss_step=9.060, val_loss_epoch=9.200, loss_epoch=87.30]Epoch 70, Train_loss: 87.25\n",
      "Epoch 71: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=75, loss_step=37.20, val_loss_step=8.990, val_loss_epoch=9.290, loss_epoch=85.90]Epoch 71, Train_loss: 85.925\n",
      "Epoch 00072: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 72: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=40.10, val_loss_step=9.020, val_loss_epoch=9.180, loss_epoch=86.60]Epoch 72, Train_loss: 86.599\n",
      "Epoch 73: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=75, loss_step=57.80, val_loss_step=8.660, val_loss_epoch=9.090, loss_epoch=86.90]Epoch 73, Train_loss: 86.907\n",
      "Epoch 74: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=75, loss_step=119.0, val_loss_step=9.070, val_loss_epoch=9.240, loss_epoch=84.20]Epoch 74, Train_loss: 84.246\n",
      "Epoch 75: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=75, loss_step=87.60, val_loss_step=9.460, val_loss_epoch=9.660, loss_epoch=87.30]Epoch 75, Train_loss: 87.324\n",
      "Epoch 76: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, v_num=75, loss_step=86.80, val_loss_step=8.750, val_loss_epoch=9.130, loss_epoch=85.00]Epoch 76, Train_loss: 85.028\n",
      "Epoch 77: 100%|██████████| 150/150 [01:03<00:00,  2.38it/s, v_num=75, loss_step=94.60, val_loss_step=8.890, val_loss_epoch=9.130, loss_epoch=84.80]Epoch 77, Train_loss: 84.841\n",
      "Epoch 78: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=75, loss_step=103.0, val_loss_step=8.760, val_loss_epoch=9.170, loss_epoch=85.40]Epoch 78, Train_loss: 85.446\n",
      "Epoch 79: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=75, loss_step=105.0, val_loss_step=8.790, val_loss_epoch=9.260, loss_epoch=86.90]Epoch 79, Train_loss: 86.892\n",
      "Epoch 80: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=75, loss_step=74.60, val_loss_step=8.910, val_loss_epoch=9.220, loss_epoch=87.40]Epoch 80, Train_loss: 87.364\n",
      "Epoch 81: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=75, loss_step=77.70, val_loss_step=8.420, val_loss_epoch=8.940, loss_epoch=87.50]Epoch 81, Train_loss: 87.452\n",
      "Epoch 82: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s, v_num=75, loss_step=58.70, val_loss_step=8.800, val_loss_epoch=9.130, loss_epoch=85.50]Epoch 82, Train_loss: 85.514\n",
      "Epoch 83: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=75, loss_step=94.90, val_loss_step=8.960, val_loss_epoch=9.160, loss_epoch=89.00]Epoch 83, Train_loss: 89.013\n",
      "Epoch 84: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s, v_num=75, loss_step=78.70, val_loss_step=8.570, val_loss_epoch=9.330, loss_epoch=85.70]Epoch 84, Train_loss: 85.652\n",
      "Epoch 85: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=75, loss_step=96.40, val_loss_step=8.890, val_loss_epoch=9.120, loss_epoch=87.70]Epoch 85, Train_loss: 87.678\n",
      "Epoch 86: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=75, loss_step=55.70, val_loss_step=9.050, val_loss_epoch=9.370, loss_epoch=87.40]Epoch 86, Train_loss: 87.44\n",
      "Epoch 87: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=75, loss_step=100.0, val_loss_step=8.640, val_loss_epoch=9.050, loss_epoch=86.10]Epoch 87, Train_loss: 86.071\n",
      "Epoch 88: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=75, loss_step=137.0, val_loss_step=9.410, val_loss_epoch=9.360, loss_epoch=87.30]Epoch 88, Train_loss: 87.25\n",
      "Epoch 89: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=75, loss_step=103.0, val_loss_step=9.910, val_loss_epoch=9.970, loss_epoch=84.60]Epoch 89, Train_loss: 84.586\n",
      "Epoch 90: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=75, loss_step=50.20, val_loss_step=9.260, val_loss_epoch=9.500, loss_epoch=87.40]Epoch 90, Train_loss: 87.419\n",
      "Epoch 91: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=75, loss_step=110.0, val_loss_step=8.730, val_loss_epoch=9.250, loss_epoch=88.60]Epoch 91, Train_loss: 88.577\n",
      "Epoch 92: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=75, loss_step=73.20, val_loss_step=9.140, val_loss_epoch=9.340, loss_epoch=86.20]Epoch 92, Train_loss: 86.167\n",
      "Epoch 93: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=75, loss_step=74.30, val_loss_step=8.990, val_loss_epoch=9.370, loss_epoch=86.30]Epoch 93, Train_loss: 86.33\n",
      "Epoch 94: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=75, loss_step=95.40, val_loss_step=9.060, val_loss_epoch=9.450, loss_epoch=89.50]Epoch 94, Train_loss: 89.465\n",
      "Epoch 95: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s, v_num=75, loss_step=74.90, val_loss_step=8.860, val_loss_epoch=9.100, loss_epoch=85.30]Epoch 95, Train_loss: 85.346\n",
      "Epoch 96: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s, v_num=75, loss_step=85.60, val_loss_step=9.050, val_loss_epoch=9.310, loss_epoch=84.20]Epoch 96, Train_loss: 84.203\n",
      "Epoch 97: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=75, loss_step=74.70, val_loss_step=9.140, val_loss_epoch=9.120, loss_epoch=85.70]Epoch 97, Train_loss: 85.736\n",
      "Epoch 98: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=75, loss_step=105.0, val_loss_step=8.860, val_loss_epoch=9.270, loss_epoch=86.10]Epoch 98, Train_loss: 86.129\n",
      "Epoch 99: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=75, loss_step=109.0, val_loss_step=8.580, val_loss_epoch=9.160, loss_epoch=86.30]Epoch 99, Train_loss: 86.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=75, loss_step=109.0, val_loss_step=8.580, val_loss_epoch=9.160, loss_epoch=86.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.283    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s, v_num=76, loss_step=148.0, val_loss_step=33.20, val_loss_epoch=49.70, loss_epoch=300.0]Epoch 0, Train_loss: 299.864\n",
      "Epoch 1: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=76, loss_step=150.0, val_loss_step=151.0, val_loss_epoch=172.0, loss_epoch=170.0]Epoch 1, Train_loss: 170.221\n",
      "Epoch 2: 100%|██████████| 150/150 [01:05<00:00,  2.31it/s, v_num=76, loss_step=164.0, val_loss_step=161.0, val_loss_epoch=163.0, loss_epoch=157.0]Epoch 2, Train_loss: 156.519\n",
      "Epoch 3: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s, v_num=76, loss_step=75.20, val_loss_step=27.00, val_loss_epoch=42.00, loss_epoch=146.0]Epoch 3, Train_loss: 146.101\n",
      "Epoch 4: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=76, loss_step=113.0, val_loss_step=122.0, val_loss_epoch=132.0, loss_epoch=133.0]Epoch 4, Train_loss: 133.337\n",
      "Epoch 5: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=76, loss_step=106.0, val_loss_step=13.70, val_loss_epoch=24.40, loss_epoch=123.0]Epoch 5, Train_loss: 123.109\n",
      "Epoch 6: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=76, loss_step=88.60, val_loss_step=36.60, val_loss_epoch=38.10, loss_epoch=119.0]Epoch 6, Train_loss: 119.031\n",
      "Epoch 7: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=129.0, val_loss_step=24.90, val_loss_epoch=27.40, loss_epoch=113.0]Epoch 7, Train_loss: 113.282\n",
      "Epoch 8: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s, v_num=76, loss_step=101.0, val_loss_step=19.40, val_loss_epoch=24.00, loss_epoch=114.0]Epoch 8, Train_loss: 114.331\n",
      "Epoch 9: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=76, loss_step=102.0, val_loss_step=14.60, val_loss_epoch=18.90, loss_epoch=104.0]Epoch 9, Train_loss: 103.964\n",
      "Epoch 10: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=76, loss_step=64.60, val_loss_step=37.90, val_loss_epoch=39.10, loss_epoch=102.0]Epoch 10, Train_loss: 102.413\n",
      "Epoch 11: 100%|██████████| 150/150 [01:20<00:00,  1.86it/s, v_num=76, loss_step=99.60, val_loss_step=19.70, val_loss_epoch=22.60, loss_epoch=95.00]Epoch 11, Train_loss: 95.005\n",
      "Epoch 12: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=76, loss_step=117.0, val_loss_step=16.30, val_loss_epoch=18.60, loss_epoch=94.10]Epoch 12, Train_loss: 94.091\n",
      "Epoch 13: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=76, loss_step=72.30, val_loss_step=19.70, val_loss_epoch=21.70, loss_epoch=94.50]Epoch 13, Train_loss: 94.497\n",
      "Epoch 14: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=138.0, val_loss_step=13.70, val_loss_epoch=16.40, loss_epoch=93.30]Epoch 14, Train_loss: 93.305\n",
      "Epoch 15: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=76, loss_step=76.50, val_loss_step=10.80, val_loss_epoch=13.60, loss_epoch=94.80]Epoch 15, Train_loss: 94.782\n",
      "Epoch 16: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=153.0, val_loss_step=8.620, val_loss_epoch=11.70, loss_epoch=91.50]Epoch 16, Train_loss: 91.458\n",
      "Epoch 17: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=82.30, val_loss_step=11.20, val_loss_epoch=12.40, loss_epoch=90.90]Epoch 17, Train_loss: 90.945\n",
      "Epoch 18: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=76, loss_step=130.0, val_loss_step=11.20, val_loss_epoch=12.30, loss_epoch=90.90]Epoch 18, Train_loss: 90.868\n",
      "Epoch 19: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=93.40, val_loss_step=8.350, val_loss_epoch=10.20, loss_epoch=91.90]Epoch 19, Train_loss: 91.888\n",
      "Epoch 20: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=76, loss_step=43.50, val_loss_step=12.30, val_loss_epoch=13.00, loss_epoch=92.20]Epoch 20, Train_loss: 92.231\n",
      "Epoch 00021: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 21: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=76, loss_step=64.70, val_loss_step=9.810, val_loss_epoch=10.20, loss_epoch=88.60]Epoch 21, Train_loss: 88.568\n",
      "Epoch 22: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=76, loss_step=35.30, val_loss_step=9.040, val_loss_epoch=9.400, loss_epoch=85.80]Epoch 22, Train_loss: 85.782\n",
      "Epoch 23: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=76, loss_step=79.20, val_loss_step=9.890, val_loss_epoch=10.10, loss_epoch=87.30]Epoch 23, Train_loss: 87.326\n",
      "Epoch 24: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=81.00, val_loss_step=8.790, val_loss_epoch=9.030, loss_epoch=87.60]Epoch 24, Train_loss: 87.614\n",
      "Epoch 25: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=76, loss_step=57.10, val_loss_step=8.250, val_loss_epoch=8.980, loss_epoch=82.80]Epoch 25, Train_loss: 82.787\n",
      "Epoch 26: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=76, loss_step=45.10, val_loss_step=9.600, val_loss_epoch=10.00, loss_epoch=88.00]Epoch 26, Train_loss: 88.03\n",
      "Epoch 27: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=76, loss_step=83.90, val_loss_step=9.550, val_loss_epoch=9.550, loss_epoch=85.30]Epoch 27, Train_loss: 85.257\n",
      "Epoch 28: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=76, loss_step=44.40, val_loss_step=7.880, val_loss_epoch=8.780, loss_epoch=84.80]Epoch 28, Train_loss: 84.827\n",
      "Epoch 29: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=76, loss_step=95.40, val_loss_step=7.450, val_loss_epoch=8.090, loss_epoch=84.10]Epoch 29, Train_loss: 84.096\n",
      "Epoch 00030: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 30: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=76, loss_step=104.0, val_loss_step=8.080, val_loss_epoch=8.440, loss_epoch=87.20]Epoch 30, Train_loss: 87.241\n",
      "Epoch 31: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=76, loss_step=95.10, val_loss_step=8.000, val_loss_epoch=8.300, loss_epoch=85.10]Epoch 31, Train_loss: 85.119\n",
      "Epoch 32: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=56.00, val_loss_step=7.840, val_loss_epoch=8.300, loss_epoch=84.00]Epoch 32, Train_loss: 84.009\n",
      "Epoch 33: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s, v_num=76, loss_step=83.00, val_loss_step=7.560, val_loss_epoch=8.260, loss_epoch=88.50]Epoch 33, Train_loss: 88.473\n",
      "Epoch 00034: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 34: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=76, loss_step=82.50, val_loss_step=8.280, val_loss_epoch=8.320, loss_epoch=87.50]Epoch 34, Train_loss: 87.514\n",
      "Epoch 35: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=102.0, val_loss_step=8.940, val_loss_epoch=8.850, loss_epoch=84.70]Epoch 35, Train_loss: 84.653\n",
      "Epoch 36: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=76, loss_step=75.90, val_loss_step=9.160, val_loss_epoch=8.890, loss_epoch=83.20]Epoch 36, Train_loss: 83.201\n",
      "Epoch 37: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=66.70, val_loss_step=8.240, val_loss_epoch=8.230, loss_epoch=87.00]Epoch 37, Train_loss: 87.012\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 38: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s, v_num=76, loss_step=74.00, val_loss_step=9.980, val_loss_epoch=9.690, loss_epoch=88.30]Epoch 38, Train_loss: 88.329\n",
      "Epoch 39: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=76, loss_step=87.60, val_loss_step=9.200, val_loss_epoch=8.800, loss_epoch=88.70]Epoch 39, Train_loss: 88.724\n",
      "Epoch 40: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=76, loss_step=121.0, val_loss_step=9.020, val_loss_epoch=8.940, loss_epoch=85.10]Epoch 40, Train_loss: 85.129\n",
      "Epoch 41: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=76, loss_step=65.10, val_loss_step=8.560, val_loss_epoch=8.800, loss_epoch=87.40]Epoch 41, Train_loss: 87.414\n",
      "Epoch 00042: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 42: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=76, loss_step=74.80, val_loss_step=8.460, val_loss_epoch=8.570, loss_epoch=85.20]Epoch 42, Train_loss: 85.246\n",
      "Epoch 43: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=76, loss_step=86.60, val_loss_step=8.480, val_loss_epoch=8.430, loss_epoch=81.90]Epoch 43, Train_loss: 81.91\n",
      "Epoch 44: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=76, loss_step=87.30, val_loss_step=8.660, val_loss_epoch=8.710, loss_epoch=86.10]Epoch 44, Train_loss: 86.089\n",
      "Epoch 45: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=76, loss_step=82.70, val_loss_step=8.270, val_loss_epoch=8.260, loss_epoch=82.90]Epoch 45, Train_loss: 82.861\n",
      "Epoch 46: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=76, loss_step=85.20, val_loss_step=8.510, val_loss_epoch=8.370, loss_epoch=87.20]Epoch 46, Train_loss: 87.166\n",
      "Epoch 47: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=103.0, val_loss_step=8.750, val_loss_epoch=8.670, loss_epoch=84.30]Epoch 47, Train_loss: 84.346\n",
      "Epoch 00048: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 48: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=76, loss_step=122.0, val_loss_step=9.480, val_loss_epoch=8.980, loss_epoch=83.40]Epoch 48, Train_loss: 83.425\n",
      "Epoch 49: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=121.0, val_loss_step=8.460, val_loss_epoch=8.310, loss_epoch=90.60]Epoch 49, Train_loss: 90.644\n",
      "Epoch 50: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=76, loss_step=136.0, val_loss_step=10.00, val_loss_epoch=9.390, loss_epoch=85.70]Epoch 50, Train_loss: 85.666\n",
      "Epoch 51: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=76, loss_step=118.0, val_loss_step=9.620, val_loss_epoch=9.280, loss_epoch=83.30]Epoch 51, Train_loss: 83.258\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 52: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=76, loss_step=51.90, val_loss_step=9.280, val_loss_epoch=8.980, loss_epoch=86.50]Epoch 52, Train_loss: 86.457\n",
      "Epoch 53: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=76, loss_step=58.10, val_loss_step=8.510, val_loss_epoch=8.520, loss_epoch=84.30]Epoch 53, Train_loss: 84.337\n",
      "Epoch 54: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=76, loss_step=69.00, val_loss_step=9.050, val_loss_epoch=8.590, loss_epoch=88.10]Epoch 54, Train_loss: 88.053\n",
      "Epoch 55: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=76, loss_step=58.80, val_loss_step=8.270, val_loss_epoch=8.460, loss_epoch=85.50]Epoch 55, Train_loss: 85.527\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 56: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=102.0, val_loss_step=9.220, val_loss_epoch=9.070, loss_epoch=83.40]Epoch 56, Train_loss: 83.401\n",
      "Epoch 57: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=106.0, val_loss_step=8.080, val_loss_epoch=8.170, loss_epoch=85.90]Epoch 57, Train_loss: 85.934\n",
      "Epoch 58: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=69.80, val_loss_step=10.00, val_loss_epoch=9.430, loss_epoch=84.30]Epoch 58, Train_loss: 84.297\n",
      "Epoch 59: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=83.60, val_loss_step=9.240, val_loss_epoch=8.930, loss_epoch=83.70]Epoch 59, Train_loss: 83.664\n",
      "Epoch 00060: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 60: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=76, loss_step=57.80, val_loss_step=8.980, val_loss_epoch=8.750, loss_epoch=85.70]Epoch 60, Train_loss: 85.74\n",
      "Epoch 61: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=39.20, val_loss_step=9.170, val_loss_epoch=8.830, loss_epoch=87.90]Epoch 61, Train_loss: 87.876\n",
      "Epoch 62: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=76, loss_step=57.80, val_loss_step=8.720, val_loss_epoch=8.620, loss_epoch=85.40]Epoch 62, Train_loss: 85.448\n",
      "Epoch 63: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=50.70, val_loss_step=9.860, val_loss_epoch=9.480, loss_epoch=84.40]Epoch 63, Train_loss: 84.402\n",
      "Epoch 64: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=90.50, val_loss_step=8.680, val_loss_epoch=8.480, loss_epoch=82.30]Epoch 64, Train_loss: 82.319\n",
      "Epoch 65: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=76.30, val_loss_step=8.490, val_loss_epoch=8.530, loss_epoch=84.40]Epoch 65, Train_loss: 84.356\n",
      "Epoch 66: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=76, loss_step=103.0, val_loss_step=8.630, val_loss_epoch=8.460, loss_epoch=84.90]Epoch 66, Train_loss: 84.889\n",
      "Epoch 67: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=72.30, val_loss_step=8.340, val_loss_epoch=8.270, loss_epoch=86.00]Epoch 67, Train_loss: 86.011\n",
      "Epoch 68: 100%|██████████| 150/150 [01:11<00:00,  2.08it/s, v_num=76, loss_step=82.70, val_loss_step=8.720, val_loss_epoch=8.510, loss_epoch=86.80]Epoch 68, Train_loss: 86.78\n",
      "Epoch 69: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=88.20, val_loss_step=8.520, val_loss_epoch=8.350, loss_epoch=88.30]Epoch 69, Train_loss: 88.291\n",
      "Epoch 70: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=76, loss_step=113.0, val_loss_step=9.590, val_loss_epoch=9.130, loss_epoch=85.00]Epoch 70, Train_loss: 85.011\n",
      "Epoch 71: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=76, loss_step=125.0, val_loss_step=8.460, val_loss_epoch=8.460, loss_epoch=89.90]Epoch 71, Train_loss: 89.915\n",
      "Epoch 72: 100%|██████████| 150/150 [01:04<00:00,  2.32it/s, v_num=76, loss_step=83.00, val_loss_step=8.410, val_loss_epoch=8.540, loss_epoch=85.40]Epoch 72, Train_loss: 85.408\n",
      "Epoch 73: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=76, loss_step=89.20, val_loss_step=9.490, val_loss_epoch=9.010, loss_epoch=87.00]Epoch 73, Train_loss: 87.033\n",
      "Epoch 74: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=76, loss_step=94.60, val_loss_step=9.080, val_loss_epoch=8.900, loss_epoch=87.00]Epoch 74, Train_loss: 87.008\n",
      "Epoch 75: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s, v_num=76, loss_step=86.40, val_loss_step=9.060, val_loss_epoch=8.860, loss_epoch=88.70]Epoch 75, Train_loss: 88.733\n",
      "Epoch 76: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=76, loss_step=101.0, val_loss_step=8.820, val_loss_epoch=8.730, loss_epoch=83.00]Epoch 76, Train_loss: 83.012\n",
      "Epoch 77: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=76, loss_step=83.50, val_loss_step=8.390, val_loss_epoch=8.370, loss_epoch=89.50]Epoch 77, Train_loss: 89.534\n",
      "Epoch 78: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=76, loss_step=95.50, val_loss_step=8.680, val_loss_epoch=8.930, loss_epoch=88.70]Epoch 78, Train_loss: 88.657\n",
      "Epoch 79: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=76, loss_step=110.0, val_loss_step=9.090, val_loss_epoch=8.820, loss_epoch=89.20]Epoch 79, Train_loss: 89.17\n",
      "Epoch 80: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=100.0, val_loss_step=9.060, val_loss_epoch=8.740, loss_epoch=87.20]Epoch 80, Train_loss: 87.226\n",
      "Epoch 81: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=76, loss_step=102.0, val_loss_step=8.150, val_loss_epoch=8.310, loss_epoch=83.40]Epoch 81, Train_loss: 83.421\n",
      "Epoch 82: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=110.0, val_loss_step=9.430, val_loss_epoch=9.230, loss_epoch=89.00]Epoch 82, Train_loss: 89.042\n",
      "Epoch 83: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=76, loss_step=59.60, val_loss_step=8.420, val_loss_epoch=8.340, loss_epoch=83.40]Epoch 83, Train_loss: 83.431\n",
      "Epoch 84: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=76, loss_step=91.40, val_loss_step=8.970, val_loss_epoch=8.670, loss_epoch=87.50]Epoch 84, Train_loss: 87.464\n",
      "Epoch 85: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=76, loss_step=108.0, val_loss_step=8.580, val_loss_epoch=8.360, loss_epoch=86.90]Epoch 85, Train_loss: 86.871\n",
      "Epoch 86: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=76, loss_step=50.80, val_loss_step=9.060, val_loss_epoch=8.810, loss_epoch=86.90]Epoch 86, Train_loss: 86.925\n",
      "Epoch 87: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s, v_num=76, loss_step=71.30, val_loss_step=8.410, val_loss_epoch=8.350, loss_epoch=87.00]Epoch 87, Train_loss: 87.018\n",
      "Epoch 88: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=84.30, val_loss_step=8.760, val_loss_epoch=8.570, loss_epoch=87.00]Epoch 88, Train_loss: 87.0\n",
      "Epoch 89: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=113.0, val_loss_step=8.670, val_loss_epoch=8.470, loss_epoch=88.00]Epoch 89, Train_loss: 88.029\n",
      "Epoch 90: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=76, loss_step=52.10, val_loss_step=8.450, val_loss_epoch=8.490, loss_epoch=87.10]Epoch 90, Train_loss: 87.14\n",
      "Epoch 91: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=94.80, val_loss_step=8.600, val_loss_epoch=8.500, loss_epoch=85.70]Epoch 91, Train_loss: 85.74\n",
      "Epoch 92: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=76, loss_step=76.80, val_loss_step=8.440, val_loss_epoch=8.250, loss_epoch=83.90]Epoch 92, Train_loss: 83.878\n",
      "Epoch 93: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s, v_num=76, loss_step=135.0, val_loss_step=8.850, val_loss_epoch=8.670, loss_epoch=84.90]Epoch 93, Train_loss: 84.909\n",
      "Epoch 94: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=76, loss_step=48.40, val_loss_step=8.540, val_loss_epoch=8.530, loss_epoch=82.40]Epoch 94, Train_loss: 82.417\n",
      "Epoch 95: 100%|██████████| 150/150 [01:14<00:00,  2.01it/s, v_num=76, loss_step=85.50, val_loss_step=8.170, val_loss_epoch=8.130, loss_epoch=87.50]Epoch 95, Train_loss: 87.54\n",
      "Epoch 96: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=76, loss_step=108.0, val_loss_step=8.540, val_loss_epoch=8.650, loss_epoch=86.60]Epoch 96, Train_loss: 86.555\n",
      "Epoch 97: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=76, loss_step=98.20, val_loss_step=8.820, val_loss_epoch=8.490, loss_epoch=84.90]Epoch 97, Train_loss: 84.869\n",
      "Epoch 98: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=76, loss_step=91.00, val_loss_step=8.620, val_loss_epoch=8.580, loss_epoch=84.90]Epoch 98, Train_loss: 84.852\n",
      "Epoch 99: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=76, loss_step=57.00, val_loss_step=8.110, val_loss_epoch=8.220, loss_epoch=85.00]Epoch 99, Train_loss: 85.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:14<00:00,  2.03it/s, v_num=76, loss_step=57.00, val_loss_step=8.110, val_loss_epoch=8.220, loss_epoch=85.00]\n"
     ]
    }
   ],
   "source": [
    "model_15 = detection_16.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_17.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_18.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_19.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_20.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_21.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)\n",
    "model_15 = detection_22.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.283    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=158.0, val_loss_step=29.70, val_loss_epoch=45.90, loss_epoch=322.0]Epoch 0, Train_loss: 321.677\n",
      "Epoch 1: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=143.0, val_loss_step=62.10, val_loss_epoch=69.50, loss_epoch=170.0]Epoch 1, Train_loss: 170.292\n",
      "Epoch 2: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=126.0, val_loss_step=42.50, val_loss_epoch=54.90, loss_epoch=138.0]Epoch 2, Train_loss: 137.908\n",
      "Epoch 3: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=102.0, val_loss_step=19.70, val_loss_epoch=35.90, loss_epoch=130.0]Epoch 3, Train_loss: 130.488\n",
      "Epoch 4: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=77, loss_step=97.00, val_loss_step=28.20, val_loss_epoch=38.70, loss_epoch=121.0]Epoch 4, Train_loss: 120.722\n",
      "Epoch 5: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=95.40, val_loss_step=23.40, val_loss_epoch=33.60, loss_epoch=113.0]Epoch 5, Train_loss: 113.39\n",
      "Epoch 6: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=102.0, val_loss_step=18.60, val_loss_epoch=25.90, loss_epoch=104.0]Epoch 6, Train_loss: 104.236\n",
      "Epoch 7: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=47.70, val_loss_step=17.70, val_loss_epoch=23.80, loss_epoch=99.80]Epoch 7, Train_loss: 99.819\n",
      "Epoch 8: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=85.30, val_loss_step=13.60, val_loss_epoch=19.80, loss_epoch=93.50]Epoch 8, Train_loss: 93.466\n",
      "Epoch 9: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=80.20, val_loss_step=23.90, val_loss_epoch=31.30, loss_epoch=96.70]Epoch 9, Train_loss: 96.685\n",
      "Epoch 10: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=88.80, val_loss_step=13.40, val_loss_epoch=18.10, loss_epoch=96.70]Epoch 10, Train_loss: 96.682\n",
      "Epoch 11: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=104.0, val_loss_step=11.40, val_loss_epoch=16.10, loss_epoch=93.10]Epoch 11, Train_loss: 93.147\n",
      "Epoch 12: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=60.10, val_loss_step=21.50, val_loss_epoch=25.00, loss_epoch=89.70]Epoch 12, Train_loss: 89.737\n",
      "Epoch 13: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=65.20, val_loss_step=11.90, val_loss_epoch=17.30, loss_epoch=91.00]Epoch 13, Train_loss: 91.001\n",
      "Epoch 14: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=60.50, val_loss_step=12.20, val_loss_epoch=20.30, loss_epoch=92.80]Epoch 14, Train_loss: 92.835\n",
      "Epoch 15: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=65.40, val_loss_step=16.20, val_loss_epoch=22.40, loss_epoch=83.50]Epoch 15, Train_loss: 83.468\n",
      "Epoch 16: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=99.10, val_loss_step=12.60, val_loss_epoch=19.90, loss_epoch=82.80]Epoch 16, Train_loss: 82.773\n",
      "Epoch 17: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=77, loss_step=70.80, val_loss_step=12.10, val_loss_epoch=14.30, loss_epoch=84.30]Epoch 17, Train_loss: 84.272\n",
      "Epoch 18: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=86.20, val_loss_step=11.00, val_loss_epoch=13.60, loss_epoch=80.50]Epoch 18, Train_loss: 80.513\n",
      "Epoch 19: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=111.0, val_loss_step=10.30, val_loss_epoch=12.90, loss_epoch=78.40]Epoch 19, Train_loss: 78.394\n",
      "Epoch 20: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=82.10, val_loss_step=10.20, val_loss_epoch=13.10, loss_epoch=75.20]Epoch 20, Train_loss: 75.216\n",
      "Epoch 21: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=104.0, val_loss_step=9.560, val_loss_epoch=13.70, loss_epoch=74.40]Epoch 21, Train_loss: 74.423\n",
      "Epoch 22: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=62.30, val_loss_step=8.710, val_loss_epoch=11.30, loss_epoch=78.30]Epoch 22, Train_loss: 78.323\n",
      "Epoch 23: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=96.00, val_loss_step=9.910, val_loss_epoch=12.70, loss_epoch=77.70]Epoch 23, Train_loss: 77.658\n",
      "Epoch 24: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=64.30, val_loss_step=8.800, val_loss_epoch=12.00, loss_epoch=73.10]Epoch 24, Train_loss: 73.127\n",
      "Epoch 25: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=45.10, val_loss_step=10.60, val_loss_epoch=14.20, loss_epoch=72.50]Epoch 25, Train_loss: 72.539\n",
      "Epoch 26: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=49.40, val_loss_step=10.90, val_loss_epoch=12.70, loss_epoch=74.20]Epoch 26, Train_loss: 74.233\n",
      "Epoch 27: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=64.50, val_loss_step=8.980, val_loss_epoch=10.80, loss_epoch=69.10]Epoch 27, Train_loss: 69.125\n",
      "Epoch 28: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=57.70, val_loss_step=8.730, val_loss_epoch=10.40, loss_epoch=69.30]Epoch 28, Train_loss: 69.269\n",
      "Epoch 29: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=50.30, val_loss_step=11.80, val_loss_epoch=14.70, loss_epoch=69.80]Epoch 29, Train_loss: 69.752\n",
      "Epoch 30: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=50.00, val_loss_step=11.30, val_loss_epoch=11.40, loss_epoch=68.20]Epoch 30, Train_loss: 68.201\n",
      "Epoch 31: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=81.60, val_loss_step=11.10, val_loss_epoch=12.80, loss_epoch=65.90]Epoch 31, Train_loss: 65.929\n",
      "Epoch 32: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=85.10, val_loss_step=9.970, val_loss_epoch=10.00, loss_epoch=68.20]Epoch 32, Train_loss: 68.248\n",
      "Epoch 33: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=64.80, val_loss_step=7.990, val_loss_epoch=8.660, loss_epoch=66.20]Epoch 33, Train_loss: 66.164\n",
      "Epoch 34: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=67.30, val_loss_step=8.470, val_loss_epoch=9.170, loss_epoch=63.70]Epoch 34, Train_loss: 63.709\n",
      "Epoch 35: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=99.60, val_loss_step=11.10, val_loss_epoch=11.60, loss_epoch=60.70]Epoch 35, Train_loss: 60.691\n",
      "Epoch 36: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=53.90, val_loss_step=8.690, val_loss_epoch=9.470, loss_epoch=61.50]Epoch 36, Train_loss: 61.546\n",
      "Epoch 37: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=56.50, val_loss_step=8.270, val_loss_epoch=9.230, loss_epoch=64.30]Epoch 37, Train_loss: 64.27\n",
      "Epoch 38: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=61.30, val_loss_step=10.20, val_loss_epoch=10.50, loss_epoch=61.40]Epoch 38, Train_loss: 61.362\n",
      "Epoch 39: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=77, loss_step=103.0, val_loss_step=9.560, val_loss_epoch=10.30, loss_epoch=67.50]Epoch 39, Train_loss: 67.484\n",
      "Epoch 40: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=49.00, val_loss_step=8.750, val_loss_epoch=9.530, loss_epoch=58.80]Epoch 40, Train_loss: 58.775\n",
      "Epoch 41: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=68.10, val_loss_step=9.220, val_loss_epoch=9.090, loss_epoch=61.40]Epoch 41, Train_loss: 61.354\n",
      "Epoch 42: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=44.60, val_loss_step=7.780, val_loss_epoch=10.10, loss_epoch=61.30]Epoch 42, Train_loss: 61.253\n",
      "Epoch 43: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=71.80, val_loss_step=11.80, val_loss_epoch=13.40, loss_epoch=61.30]Epoch 43, Train_loss: 61.273\n",
      "Epoch 44: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=116.0, val_loss_step=9.260, val_loss_epoch=9.980, loss_epoch=57.80]Epoch 44, Train_loss: 57.765\n",
      "Epoch 45: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=69.80, val_loss_step=10.60, val_loss_epoch=9.790, loss_epoch=61.70]Epoch 45, Train_loss: 61.717\n",
      "Epoch 46: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=34.40, val_loss_step=7.660, val_loss_epoch=7.870, loss_epoch=56.40]Epoch 46, Train_loss: 56.366\n",
      "Epoch 47: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=77, loss_step=114.0, val_loss_step=9.120, val_loss_epoch=8.410, loss_epoch=61.60]Epoch 47, Train_loss: 61.582\n",
      "Epoch 48: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=50.20, val_loss_step=7.730, val_loss_epoch=8.400, loss_epoch=56.60]Epoch 48, Train_loss: 56.601\n",
      "Epoch 49: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=37.70, val_loss_step=7.740, val_loss_epoch=7.310, loss_epoch=56.50]Epoch 49, Train_loss: 56.454\n",
      "Epoch 50: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=84.30, val_loss_step=7.990, val_loss_epoch=9.040, loss_epoch=57.30]Epoch 50, Train_loss: 57.256\n",
      "Epoch 51: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=57.00, val_loss_step=7.200, val_loss_epoch=7.370, loss_epoch=57.10]Epoch 51, Train_loss: 57.112\n",
      "Epoch 52: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=90.80, val_loss_step=8.680, val_loss_epoch=8.700, loss_epoch=53.50]Epoch 52, Train_loss: 53.485\n",
      "Epoch 53: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=48.30, val_loss_step=8.070, val_loss_epoch=7.200, loss_epoch=58.60]Epoch 53, Train_loss: 58.625\n",
      "Epoch 54: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=54.80, val_loss_step=8.300, val_loss_epoch=8.900, loss_epoch=55.60]Epoch 54, Train_loss: 55.625\n",
      "Epoch 55: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=51.40, val_loss_step=6.950, val_loss_epoch=7.330, loss_epoch=54.30]Epoch 55, Train_loss: 54.305\n",
      "Epoch 56: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=48.60, val_loss_step=8.700, val_loss_epoch=7.920, loss_epoch=54.80]Epoch 56, Train_loss: 54.773\n",
      "Epoch 57: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=74.40, val_loss_step=7.510, val_loss_epoch=6.880, loss_epoch=56.10]Epoch 57, Train_loss: 56.058\n",
      "Epoch 58: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=47.70, val_loss_step=8.100, val_loss_epoch=7.800, loss_epoch=57.40]Epoch 58, Train_loss: 57.354\n",
      "Epoch 00059: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 59: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=49.60, val_loss_step=6.880, val_loss_epoch=6.200, loss_epoch=51.70]Epoch 59, Train_loss: 51.656\n",
      "Epoch 60: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=54.50, val_loss_step=6.800, val_loss_epoch=6.170, loss_epoch=51.10]Epoch 60, Train_loss: 51.141\n",
      "Epoch 61: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=88.00, val_loss_step=6.920, val_loss_epoch=6.190, loss_epoch=49.60]Epoch 61, Train_loss: 49.64\n",
      "Epoch 62: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=77, loss_step=75.80, val_loss_step=6.840, val_loss_epoch=5.950, loss_epoch=50.00]Epoch 62, Train_loss: 49.984\n",
      "Epoch 63: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=73.80, val_loss_step=6.750, val_loss_epoch=5.840, loss_epoch=50.70]Epoch 63, Train_loss: 50.721\n",
      "Epoch 64: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=38.70, val_loss_step=6.870, val_loss_epoch=5.950, loss_epoch=50.70]Epoch 64, Train_loss: 50.651\n",
      "Epoch 65: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=56.30, val_loss_step=6.910, val_loss_epoch=6.080, loss_epoch=49.60]Epoch 65, Train_loss: 49.551\n",
      "Epoch 66: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=33.00, val_loss_step=6.950, val_loss_epoch=5.880, loss_epoch=50.70]Epoch 66, Train_loss: 50.65\n",
      "Epoch 67: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=77, loss_step=67.30, val_loss_step=7.010, val_loss_epoch=5.960, loss_epoch=48.90]Epoch 67, Train_loss: 48.889\n",
      "Epoch 68: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=77, loss_step=40.20, val_loss_step=6.870, val_loss_epoch=5.780, loss_epoch=47.80]Epoch 68, Train_loss: 47.804\n",
      "Epoch 69: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=77, loss_step=70.60, val_loss_step=6.790, val_loss_epoch=5.770, loss_epoch=48.10]Epoch 69, Train_loss: 48.127\n",
      "Epoch 70: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=77, loss_step=61.10, val_loss_step=6.790, val_loss_epoch=5.530, loss_epoch=49.00]Epoch 70, Train_loss: 49.029\n",
      "Epoch 71: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=77, loss_step=74.50, val_loss_step=6.870, val_loss_epoch=5.770, loss_epoch=49.80]Epoch 71, Train_loss: 49.792\n",
      "Epoch 72: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=77, loss_step=57.60, val_loss_step=6.800, val_loss_epoch=5.630, loss_epoch=47.70]Epoch 72, Train_loss: 47.71\n",
      "Epoch 73: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=77, loss_step=60.10, val_loss_step=6.870, val_loss_epoch=5.690, loss_epoch=49.00]Epoch 73, Train_loss: 48.989\n",
      "Epoch 74: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=77, loss_step=57.50, val_loss_step=6.750, val_loss_epoch=5.670, loss_epoch=48.50]Epoch 74, Train_loss: 48.517\n",
      "Epoch 00075: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 75: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s, v_num=77, loss_step=56.40, val_loss_step=6.710, val_loss_epoch=5.630, loss_epoch=46.40]Epoch 75, Train_loss: 46.415\n",
      "Epoch 76: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=77, loss_step=41.00, val_loss_step=6.740, val_loss_epoch=5.600, loss_epoch=47.20]Epoch 76, Train_loss: 47.191\n",
      "Epoch 77: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s, v_num=77, loss_step=68.20, val_loss_step=6.650, val_loss_epoch=5.520, loss_epoch=47.30]Epoch 77, Train_loss: 47.279\n",
      "Epoch 78: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=77, loss_step=50.30, val_loss_step=6.760, val_loss_epoch=5.660, loss_epoch=47.00]Epoch 78, Train_loss: 47.05\n",
      "Epoch 79: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=77, loss_step=46.10, val_loss_step=6.610, val_loss_epoch=5.450, loss_epoch=46.60]Epoch 79, Train_loss: 46.636\n",
      "Epoch 80: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=77, loss_step=55.00, val_loss_step=6.710, val_loss_epoch=5.490, loss_epoch=45.50]Epoch 80, Train_loss: 45.472\n",
      "Epoch 81: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=77, loss_step=54.50, val_loss_step=6.600, val_loss_epoch=5.430, loss_epoch=49.10]Epoch 81, Train_loss: 49.12\n",
      "Epoch 82: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=93.10, val_loss_step=6.630, val_loss_epoch=5.430, loss_epoch=45.30]Epoch 82, Train_loss: 45.287\n",
      "Epoch 83: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=74.30, val_loss_step=6.590, val_loss_epoch=5.330, loss_epoch=47.00]Epoch 83, Train_loss: 47.02\n",
      "Epoch 84: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=77, loss_step=47.00, val_loss_step=6.700, val_loss_epoch=5.630, loss_epoch=45.90]Epoch 84, Train_loss: 45.934\n",
      "Epoch 85: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=77, loss_step=68.70, val_loss_step=6.670, val_loss_epoch=5.570, loss_epoch=44.90]Epoch 85, Train_loss: 44.939\n",
      "Epoch 86: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=77, loss_step=57.50, val_loss_step=6.670, val_loss_epoch=5.410, loss_epoch=46.70]Epoch 86, Train_loss: 46.733\n",
      "Epoch 87: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=38.10, val_loss_step=6.620, val_loss_epoch=5.480, loss_epoch=44.40]Epoch 87, Train_loss: 44.422\n",
      "Epoch 88: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=72.40, val_loss_step=6.770, val_loss_epoch=5.490, loss_epoch=45.90]Epoch 88, Train_loss: 45.86\n",
      "Epoch 89: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=77, loss_step=31.00, val_loss_step=6.690, val_loss_epoch=5.410, loss_epoch=47.10]Epoch 89, Train_loss: 47.12\n",
      "Epoch 90: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=77, loss_step=54.10, val_loss_step=6.600, val_loss_epoch=5.390, loss_epoch=44.80]Epoch 90, Train_loss: 44.827\n",
      "Epoch 91: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=88.10, val_loss_step=6.690, val_loss_epoch=5.460, loss_epoch=46.80]Epoch 91, Train_loss: 46.752\n",
      "Epoch 92: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=77, loss_step=33.90, val_loss_step=6.690, val_loss_epoch=5.380, loss_epoch=45.90]Epoch 92, Train_loss: 45.926\n",
      "Epoch 93: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=29.40, val_loss_step=6.770, val_loss_epoch=5.430, loss_epoch=44.60]Epoch 93, Train_loss: 44.627\n",
      "Epoch 00094: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 94: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=77, loss_step=42.70, val_loss_step=6.770, val_loss_epoch=5.400, loss_epoch=46.20]Epoch 94, Train_loss: 46.194\n",
      "Epoch 95: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=77, loss_step=53.00, val_loss_step=6.730, val_loss_epoch=5.330, loss_epoch=45.00]Epoch 95, Train_loss: 45.005\n",
      "Epoch 96: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=77, loss_step=59.30, val_loss_step=6.850, val_loss_epoch=5.450, loss_epoch=44.60]Epoch 96, Train_loss: 44.553\n",
      "Epoch 97: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=50.40, val_loss_step=6.800, val_loss_epoch=5.430, loss_epoch=45.90]Epoch 97, Train_loss: 45.92\n",
      "Epoch 98: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=77, loss_step=54.00, val_loss_step=6.760, val_loss_epoch=5.370, loss_epoch=46.30]Epoch 98, Train_loss: 46.279\n",
      "Epoch 99: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=77, loss_step=55.00, val_loss_step=6.780, val_loss_epoch=5.340, loss_epoch=46.10]Epoch 99, Train_loss: 46.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 99: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=77, loss_step=55.00, val_loss_step=6.780, val_loss_epoch=5.340, loss_epoch=46.10]\n"
     ]
    }
   ],
   "source": [
    "model_15 = detection_23.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.283    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=203.0, val_loss_step=31.90, val_loss_epoch=44.50, loss_epoch=348.0]Epoch 0, Train_loss: 348.3\n",
      "Epoch 1: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=88.60, val_loss_step=38.40, val_loss_epoch=49.80, loss_epoch=154.0]Epoch 1, Train_loss: 154.122\n",
      "Epoch 2: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=119.0, val_loss_step=26.20, val_loss_epoch=41.00, loss_epoch=144.0]Epoch 2, Train_loss: 143.909\n",
      "Epoch 3: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=110.0, val_loss_step=26.10, val_loss_epoch=33.80, loss_epoch=125.0]Epoch 3, Train_loss: 124.862\n",
      "Epoch 4: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=85.00, val_loss_step=17.00, val_loss_epoch=27.80, loss_epoch=118.0]Epoch 4, Train_loss: 117.853\n",
      "Epoch 5: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=59.00, val_loss_step=15.20, val_loss_epoch=27.40, loss_epoch=111.0]Epoch 5, Train_loss: 110.94\n",
      "Epoch 6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=109.0, val_loss_step=13.60, val_loss_epoch=22.30, loss_epoch=108.0]Epoch 6, Train_loss: 108.469\n",
      "Epoch 7: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=79, loss_step=105.0, val_loss_step=14.00, val_loss_epoch=27.70, loss_epoch=103.0]Epoch 7, Train_loss: 103.362\n",
      "Epoch 8: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=79, loss_step=98.30, val_loss_step=11.00, val_loss_epoch=19.80, loss_epoch=99.30]Epoch 8, Train_loss: 99.293\n",
      "Epoch 9: 100%|██████████| 150/150 [01:04<00:00,  2.34it/s, v_num=79, loss_step=97.60, val_loss_step=13.80, val_loss_epoch=20.70, loss_epoch=95.10]Epoch 9, Train_loss: 95.141\n",
      "Epoch 10: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=79, loss_step=66.30, val_loss_step=11.00, val_loss_epoch=19.80, loss_epoch=92.20]Epoch 10, Train_loss: 92.212\n",
      "Epoch 11: 100%|██████████| 150/150 [01:03<00:00,  2.35it/s, v_num=79, loss_step=138.0, val_loss_step=16.60, val_loss_epoch=20.70, loss_epoch=90.50]Epoch 11, Train_loss: 90.537\n",
      "Epoch 12: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=79, loss_step=130.0, val_loss_step=11.20, val_loss_epoch=16.20, loss_epoch=88.50]Epoch 12, Train_loss: 88.546\n",
      "Epoch 13: 100%|██████████| 150/150 [01:03<00:00,  2.36it/s, v_num=79, loss_step=82.40, val_loss_step=16.60, val_loss_epoch=18.90, loss_epoch=82.50]Epoch 13, Train_loss: 82.55\n",
      "Epoch 14: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s, v_num=79, loss_step=75.10, val_loss_step=10.90, val_loss_epoch=14.60, loss_epoch=82.70]Epoch 14, Train_loss: 82.684\n",
      "Epoch 15: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=79, loss_step=54.90, val_loss_step=13.40, val_loss_epoch=16.70, loss_epoch=78.00]Epoch 15, Train_loss: 78.05\n",
      "Epoch 16: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=142.0, val_loss_step=11.30, val_loss_epoch=14.70, loss_epoch=80.10]Epoch 16, Train_loss: 80.087\n",
      "Epoch 17: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=53.90, val_loss_step=9.830, val_loss_epoch=13.00, loss_epoch=76.60]Epoch 17, Train_loss: 76.599\n",
      "Epoch 18: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=38.20, val_loss_step=9.770, val_loss_epoch=12.30, loss_epoch=75.00]Epoch 18, Train_loss: 75.024\n",
      "Epoch 19: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=79, loss_step=71.90, val_loss_step=9.090, val_loss_epoch=13.20, loss_epoch=74.00]Epoch 19, Train_loss: 74.037\n",
      "Epoch 20: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=72.40, val_loss_step=11.30, val_loss_epoch=14.50, loss_epoch=76.00]Epoch 20, Train_loss: 76.023\n",
      "Epoch 21: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=79, loss_step=74.50, val_loss_step=9.620, val_loss_epoch=12.90, loss_epoch=74.10]Epoch 21, Train_loss: 74.086\n",
      "Epoch 22: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=51.50, val_loss_step=9.720, val_loss_epoch=13.10, loss_epoch=71.20]Epoch 22, Train_loss: 71.25\n",
      "Epoch 23: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=72.50, val_loss_step=10.40, val_loss_epoch=15.80, loss_epoch=71.50]Epoch 23, Train_loss: 71.508\n",
      "Epoch 24: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=66.30, val_loss_step=8.480, val_loss_epoch=10.40, loss_epoch=68.60]Epoch 24, Train_loss: 68.564\n",
      "Epoch 25: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=79, loss_step=55.40, val_loss_step=8.220, val_loss_epoch=10.30, loss_epoch=69.20]Epoch 25, Train_loss: 69.194\n",
      "Epoch 26: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=79, loss_step=66.80, val_loss_step=8.080, val_loss_epoch=10.10, loss_epoch=69.80]Epoch 26, Train_loss: 69.792\n",
      "Epoch 27: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=82.00, val_loss_step=10.00, val_loss_epoch=12.40, loss_epoch=69.90]Epoch 27, Train_loss: 69.905\n",
      "Epoch 28: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=66.50, val_loss_step=10.70, val_loss_epoch=12.00, loss_epoch=65.80]Epoch 28, Train_loss: 65.827\n",
      "Epoch 29: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=69.70, val_loss_step=10.20, val_loss_epoch=11.90, loss_epoch=67.60]Epoch 29, Train_loss: 67.599\n",
      "Epoch 30: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=57.90, val_loss_step=8.580, val_loss_epoch=10.50, loss_epoch=61.90]Epoch 30, Train_loss: 61.896\n",
      "Epoch 31: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=53.50, val_loss_step=9.110, val_loss_epoch=10.20, loss_epoch=62.50]Epoch 31, Train_loss: 62.454\n",
      "Epoch 32: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=68.30, val_loss_step=8.760, val_loss_epoch=11.80, loss_epoch=61.80]Epoch 32, Train_loss: 61.836\n",
      "Epoch 33: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=79, loss_step=59.60, val_loss_step=8.460, val_loss_epoch=9.950, loss_epoch=64.70]Epoch 33, Train_loss: 64.727\n",
      "Epoch 34: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=67.50, val_loss_step=7.840, val_loss_epoch=9.860, loss_epoch=62.10]Epoch 34, Train_loss: 62.05\n",
      "Epoch 35: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=87.50, val_loss_step=8.140, val_loss_epoch=12.90, loss_epoch=64.30]Epoch 35, Train_loss: 64.251\n",
      "Epoch 36: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=79, loss_step=55.60, val_loss_step=8.650, val_loss_epoch=10.30, loss_epoch=64.20]Epoch 36, Train_loss: 64.21\n",
      "Epoch 00037: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 37: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=79, loss_step=38.00, val_loss_step=7.800, val_loss_epoch=8.760, loss_epoch=58.40]Epoch 37, Train_loss: 58.389\n",
      "Epoch 38: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=83.70, val_loss_step=7.780, val_loss_epoch=8.030, loss_epoch=57.20]Epoch 38, Train_loss: 57.239\n",
      "Epoch 39: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=79, loss_step=63.70, val_loss_step=7.550, val_loss_epoch=7.990, loss_epoch=56.00]Epoch 39, Train_loss: 56.045\n",
      "Epoch 40: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=51.90, val_loss_step=7.780, val_loss_epoch=8.230, loss_epoch=54.20]Epoch 40, Train_loss: 54.227\n",
      "Epoch 41: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=79, loss_step=44.50, val_loss_step=8.300, val_loss_epoch=9.240, loss_epoch=55.20]Epoch 41, Train_loss: 55.184\n",
      "Epoch 42: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=54.60, val_loss_step=7.580, val_loss_epoch=7.850, loss_epoch=58.00]Epoch 42, Train_loss: 58.003\n",
      "Epoch 43: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=79, loss_step=56.90, val_loss_step=7.730, val_loss_epoch=7.860, loss_epoch=54.40]Epoch 43, Train_loss: 54.38\n",
      "Epoch 44: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=56.70, val_loss_step=7.650, val_loss_epoch=7.350, loss_epoch=53.30]Epoch 44, Train_loss: 53.291\n",
      "Epoch 45: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=79, loss_step=69.00, val_loss_step=7.460, val_loss_epoch=8.030, loss_epoch=52.90]Epoch 45, Train_loss: 52.876\n",
      "Epoch 46: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=79, loss_step=53.70, val_loss_step=7.550, val_loss_epoch=7.860, loss_epoch=54.30]Epoch 46, Train_loss: 54.253\n",
      "Epoch 47: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=79, loss_step=49.40, val_loss_step=7.560, val_loss_epoch=7.490, loss_epoch=52.60]Epoch 47, Train_loss: 52.63\n",
      "Epoch 48: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=53.80, val_loss_step=7.310, val_loss_epoch=7.320, loss_epoch=50.60]Epoch 48, Train_loss: 50.632\n",
      "Epoch 49: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=79, loss_step=32.00, val_loss_step=7.500, val_loss_epoch=7.410, loss_epoch=50.00]Epoch 49, Train_loss: 50.035\n",
      "Epoch 50: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s, v_num=79, loss_step=42.50, val_loss_step=7.520, val_loss_epoch=7.260, loss_epoch=53.60]Epoch 50, Train_loss: 53.597\n",
      "Epoch 51: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s, v_num=79, loss_step=73.90, val_loss_step=7.390, val_loss_epoch=7.030, loss_epoch=49.60]Epoch 51, Train_loss: 49.647\n",
      "Epoch 52: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s, v_num=79, loss_step=72.90, val_loss_step=7.400, val_loss_epoch=7.460, loss_epoch=49.60]Epoch 52, Train_loss: 49.552\n",
      "Epoch 53: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=61.60, val_loss_step=7.400, val_loss_epoch=7.390, loss_epoch=52.80]Epoch 53, Train_loss: 52.782\n",
      "Epoch 54: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=79, loss_step=58.50, val_loss_step=7.340, val_loss_epoch=7.200, loss_epoch=50.50]Epoch 54, Train_loss: 50.538\n",
      "Epoch 55: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=79, loss_step=68.80, val_loss_step=7.590, val_loss_epoch=7.460, loss_epoch=49.30]Epoch 55, Train_loss: 49.312\n",
      "Epoch 56: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=79, loss_step=76.80, val_loss_step=7.260, val_loss_epoch=7.220, loss_epoch=48.90]Epoch 56, Train_loss: 48.859\n",
      "Epoch 57: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=79, loss_step=45.60, val_loss_step=7.250, val_loss_epoch=6.830, loss_epoch=49.70]Epoch 57, Train_loss: 49.69\n",
      "Epoch 58: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=79, loss_step=104.0, val_loss_step=7.570, val_loss_epoch=7.880, loss_epoch=53.20]Epoch 58, Train_loss: 53.171\n",
      "Epoch 59: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=79, loss_step=56.50, val_loss_step=7.200, val_loss_epoch=6.990, loss_epoch=45.30]Epoch 59, Train_loss: 45.303\n",
      "Epoch 60: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=79, loss_step=55.40, val_loss_step=7.370, val_loss_epoch=7.360, loss_epoch=50.20]Epoch 60, Train_loss: 50.198\n",
      "Epoch 61: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=79, loss_step=35.80, val_loss_step=7.210, val_loss_epoch=6.760, loss_epoch=47.60]Epoch 61, Train_loss: 47.639\n",
      "Epoch 62: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=79, loss_step=62.00, val_loss_step=7.070, val_loss_epoch=7.010, loss_epoch=48.30]Epoch 62, Train_loss: 48.294\n",
      "Epoch 63: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=79, loss_step=94.70, val_loss_step=7.240, val_loss_epoch=7.250, loss_epoch=51.30]Epoch 63, Train_loss: 51.33\n",
      "Epoch 64: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=79, loss_step=47.30, val_loss_step=7.290, val_loss_epoch=6.690, loss_epoch=49.90]Epoch 64, Train_loss: 49.863\n",
      "Epoch 65: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=79, loss_step=77.80, val_loss_step=7.180, val_loss_epoch=6.540, loss_epoch=49.10]Epoch 65, Train_loss: 49.083\n",
      "Epoch 00066: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 66: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=57.90, val_loss_step=7.200, val_loss_epoch=6.420, loss_epoch=47.90]Epoch 66, Train_loss: 47.895\n",
      "Epoch 67: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=69.60, val_loss_step=7.190, val_loss_epoch=6.420, loss_epoch=48.80]Epoch 67, Train_loss: 48.806\n",
      "Epoch 68: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=79, loss_step=121.0, val_loss_step=7.320, val_loss_epoch=6.850, loss_epoch=48.90]Epoch 68, Train_loss: 48.948\n",
      "Epoch 69: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=79, loss_step=62.10, val_loss_step=7.300, val_loss_epoch=6.520, loss_epoch=47.70]Epoch 69, Train_loss: 47.658\n",
      "Epoch 70: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=66.80, val_loss_step=7.600, val_loss_epoch=7.200, loss_epoch=46.90]Epoch 70, Train_loss: 46.875\n",
      "Epoch 71: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=79, loss_step=76.80, val_loss_step=7.250, val_loss_epoch=6.730, loss_epoch=44.40]Epoch 71, Train_loss: 44.403\n",
      "Epoch 72: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=79, loss_step=45.50, val_loss_step=7.340, val_loss_epoch=6.660, loss_epoch=46.50]Epoch 72, Train_loss: 46.543\n",
      "Epoch 73: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=66.90, val_loss_step=7.380, val_loss_epoch=6.510, loss_epoch=50.20]Epoch 73, Train_loss: 50.237\n",
      "Epoch 74: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=84.20, val_loss_step=7.240, val_loss_epoch=6.390, loss_epoch=47.50]Epoch 74, Train_loss: 47.514\n",
      "Epoch 75: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=42.40, val_loss_step=7.380, val_loss_epoch=6.560, loss_epoch=48.20]Epoch 75, Train_loss: 48.2\n",
      "Epoch 76: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=32.40, val_loss_step=7.260, val_loss_epoch=6.470, loss_epoch=48.00]Epoch 76, Train_loss: 48.028\n",
      "Epoch 77: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=79, loss_step=44.70, val_loss_step=7.270, val_loss_epoch=6.600, loss_epoch=47.20]Epoch 77, Train_loss: 47.16\n",
      "Epoch 00078: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 78: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=65.00, val_loss_step=7.320, val_loss_epoch=6.430, loss_epoch=45.10]Epoch 78, Train_loss: 45.084\n",
      "Epoch 79: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=80.00, val_loss_step=7.770, val_loss_epoch=7.070, loss_epoch=47.80]Epoch 79, Train_loss: 47.787\n",
      "Epoch 80: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=35.80, val_loss_step=7.620, val_loss_epoch=6.670, loss_epoch=46.60]Epoch 80, Train_loss: 46.558\n",
      "Epoch 81: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=39.20, val_loss_step=7.220, val_loss_epoch=6.490, loss_epoch=47.30]Epoch 81, Train_loss: 47.266\n",
      "Epoch 82: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=79, loss_step=36.30, val_loss_step=7.570, val_loss_epoch=6.590, loss_epoch=45.50]Epoch 82, Train_loss: 45.512\n",
      "Epoch 83: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=52.60, val_loss_step=7.490, val_loss_epoch=6.520, loss_epoch=47.80]Epoch 83, Train_loss: 47.844\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 84: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=55.90, val_loss_step=7.380, val_loss_epoch=6.530, loss_epoch=45.10]Epoch 84, Train_loss: 45.081\n",
      "Epoch 85: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=87.50, val_loss_step=7.550, val_loss_epoch=6.750, loss_epoch=49.60]Epoch 85, Train_loss: 49.594\n",
      "Epoch 86: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=87.80, val_loss_step=7.820, val_loss_epoch=6.800, loss_epoch=46.90]Epoch 86, Train_loss: 46.862\n",
      "Epoch 87: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=71.70, val_loss_step=7.690, val_loss_epoch=6.640, loss_epoch=49.10]Epoch 87, Train_loss: 49.052\n",
      "Epoch 88: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=72.20, val_loss_step=7.360, val_loss_epoch=6.490, loss_epoch=44.10]Epoch 88, Train_loss: 44.056\n",
      "Epoch 89: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=84.00, val_loss_step=7.490, val_loss_epoch=6.600, loss_epoch=46.90]Epoch 89, Train_loss: 46.927\n",
      "Epoch 00090: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 90: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=25.20, val_loss_step=7.340, val_loss_epoch=6.540, loss_epoch=46.40]Epoch 90, Train_loss: 46.444\n",
      "Epoch 91: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=79.50, val_loss_step=7.520, val_loss_epoch=6.420, loss_epoch=46.40]Epoch 91, Train_loss: 46.443\n",
      "Epoch 92: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=47.80, val_loss_step=7.700, val_loss_epoch=6.790, loss_epoch=46.80]Epoch 92, Train_loss: 46.787\n",
      "Epoch 93: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=79, loss_step=55.10, val_loss_step=7.700, val_loss_epoch=6.670, loss_epoch=46.40]Epoch 93, Train_loss: 46.426\n",
      "Epoch 94: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=79, loss_step=25.80, val_loss_step=7.670, val_loss_epoch=6.660, loss_epoch=46.90]Epoch 94, Train_loss: 46.94\n",
      "Epoch 95: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=79, loss_step=59.50, val_loss_step=7.640, val_loss_epoch=6.600, loss_epoch=44.50]Epoch 95, Train_loss: 44.544\n",
      "Epoch 00096: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 96: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=41.60, val_loss_step=7.560, val_loss_epoch=6.410, loss_epoch=48.20]Epoch 96, Train_loss: 48.2\n",
      "Epoch 97: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=79, loss_step=68.80, val_loss_step=7.450, val_loss_epoch=6.350, loss_epoch=48.30]Epoch 97, Train_loss: 48.328\n",
      "Epoch 98: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=79, loss_step=94.90, val_loss_step=7.550, val_loss_epoch=6.370, loss_epoch=43.50]Epoch 98, Train_loss: 43.545\n",
      "Epoch 99: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=79, loss_step=33.60, val_loss_step=7.290, val_loss_epoch=6.450, loss_epoch=47.70]Epoch 99, Train_loss: 47.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=79, loss_step=33.60, val_loss_step=7.290, val_loss_epoch=6.450, loss_epoch=47.70]\n"
     ]
    }
   ],
   "source": [
    "model_15 = detection_25.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.285    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=80, loss_step=150.0, val_loss_step=53.70, val_loss_epoch=59.00, loss_epoch=277.0]Epoch 0, Train_loss: 277.392\n",
      "Epoch 1: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=163.0, val_loss_step=31.10, val_loss_epoch=45.70, loss_epoch=157.0]Epoch 1, Train_loss: 157.489\n",
      "Epoch 2: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=130.0, val_loss_step=58.30, val_loss_epoch=76.80, loss_epoch=138.0]Epoch 2, Train_loss: 137.924\n",
      "Epoch 3: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=177.0, val_loss_step=32.40, val_loss_epoch=47.20, loss_epoch=127.0]Epoch 3, Train_loss: 127.151\n",
      "Epoch 4: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=65.00, val_loss_step=15.30, val_loss_epoch=25.10, loss_epoch=117.0]Epoch 4, Train_loss: 116.888\n",
      "Epoch 5: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=82.80, val_loss_step=13.30, val_loss_epoch=21.00, loss_epoch=107.0]Epoch 5, Train_loss: 107.488\n",
      "Epoch 6: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=128.0, val_loss_step=14.20, val_loss_epoch=21.20, loss_epoch=99.30]Epoch 6, Train_loss: 99.339\n",
      "Epoch 7: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=141.0, val_loss_step=11.90, val_loss_epoch=22.00, loss_epoch=98.60]Epoch 7, Train_loss: 98.628\n",
      "Epoch 8: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=67.00, val_loss_step=17.50, val_loss_epoch=20.20, loss_epoch=96.80]Epoch 8, Train_loss: 96.837\n",
      "Epoch 9: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=107.0, val_loss_step=10.80, val_loss_epoch=21.60, loss_epoch=93.10]Epoch 9, Train_loss: 93.099\n",
      "Epoch 10: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=85.20, val_loss_step=9.210, val_loss_epoch=13.70, loss_epoch=92.20]Epoch 10, Train_loss: 92.232\n",
      "Epoch 11: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=89.70, val_loss_step=9.660, val_loss_epoch=14.30, loss_epoch=90.60]Epoch 11, Train_loss: 90.619\n",
      "Epoch 12: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=80, loss_step=61.30, val_loss_step=11.90, val_loss_epoch=17.10, loss_epoch=90.10]Epoch 12, Train_loss: 90.074\n",
      "Epoch 13: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=73.30, val_loss_step=15.40, val_loss_epoch=15.50, loss_epoch=84.40]Epoch 13, Train_loss: 84.365\n",
      "Epoch 14: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=78.30, val_loss_step=13.30, val_loss_epoch=14.60, loss_epoch=80.60]Epoch 14, Train_loss: 80.613\n",
      "Epoch 15: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=95.40, val_loss_step=15.10, val_loss_epoch=18.20, loss_epoch=83.90]Epoch 15, Train_loss: 83.938\n",
      "Epoch 16: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=49.20, val_loss_step=11.80, val_loss_epoch=14.10, loss_epoch=84.10]Epoch 16, Train_loss: 84.11\n",
      "Epoch 17: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=80, loss_step=66.20, val_loss_step=11.00, val_loss_epoch=13.40, loss_epoch=81.40]Epoch 17, Train_loss: 81.371\n",
      "Epoch 18: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=53.50, val_loss_step=9.930, val_loss_epoch=12.60, loss_epoch=81.60]Epoch 18, Train_loss: 81.574\n",
      "Epoch 19: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=74.70, val_loss_step=9.000, val_loss_epoch=10.40, loss_epoch=74.80]Epoch 19, Train_loss: 74.77\n",
      "Epoch 20: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=98.40, val_loss_step=9.580, val_loss_epoch=12.60, loss_epoch=75.80]Epoch 20, Train_loss: 75.767\n",
      "Epoch 21: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=77.90, val_loss_step=8.840, val_loss_epoch=9.350, loss_epoch=71.20]Epoch 21, Train_loss: 71.151\n",
      "Epoch 22: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=86.50, val_loss_step=11.50, val_loss_epoch=11.90, loss_epoch=72.70]Epoch 22, Train_loss: 72.667\n",
      "Epoch 23: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=53.50, val_loss_step=10.00, val_loss_epoch=10.30, loss_epoch=70.30]Epoch 23, Train_loss: 70.286\n",
      "Epoch 24: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=66.50, val_loss_step=12.80, val_loss_epoch=13.50, loss_epoch=69.50]Epoch 24, Train_loss: 69.496\n",
      "Epoch 25: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=105.0, val_loss_step=9.330, val_loss_epoch=9.330, loss_epoch=68.80]Epoch 25, Train_loss: 68.755\n",
      "Epoch 26: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=86.90, val_loss_step=8.540, val_loss_epoch=10.00, loss_epoch=70.00]Epoch 26, Train_loss: 70.047\n",
      "Epoch 27: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=79.60, val_loss_step=9.580, val_loss_epoch=8.790, loss_epoch=64.60]Epoch 27, Train_loss: 64.639\n",
      "Epoch 28: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=66.20, val_loss_step=10.20, val_loss_epoch=11.80, loss_epoch=65.00]Epoch 28, Train_loss: 64.96\n",
      "Epoch 29: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=40.60, val_loss_step=7.870, val_loss_epoch=8.710, loss_epoch=63.90]Epoch 29, Train_loss: 63.907\n",
      "Epoch 30: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=55.70, val_loss_step=8.370, val_loss_epoch=8.650, loss_epoch=60.80]Epoch 30, Train_loss: 60.787\n",
      "Epoch 31: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=80.20, val_loss_step=9.360, val_loss_epoch=9.270, loss_epoch=62.60]Epoch 31, Train_loss: 62.58\n",
      "Epoch 32: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=48.10, val_loss_step=9.000, val_loss_epoch=9.510, loss_epoch=60.00]Epoch 32, Train_loss: 60.013\n",
      "Epoch 33: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=63.60, val_loss_step=8.110, val_loss_epoch=9.290, loss_epoch=65.30]Epoch 33, Train_loss: 65.324\n",
      "Epoch 34: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=57.10, val_loss_step=8.550, val_loss_epoch=10.30, loss_epoch=58.80]Epoch 34, Train_loss: 58.804\n",
      "Epoch 35: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=79.70, val_loss_step=7.710, val_loss_epoch=8.300, loss_epoch=60.30]Epoch 35, Train_loss: 60.29\n",
      "Epoch 36: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=80, loss_step=81.20, val_loss_step=8.520, val_loss_epoch=9.420, loss_epoch=63.70]Epoch 36, Train_loss: 63.716\n",
      "Epoch 37: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=50.10, val_loss_step=7.810, val_loss_epoch=8.600, loss_epoch=61.90]Epoch 37, Train_loss: 61.878\n",
      "Epoch 38: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=80, loss_step=37.90, val_loss_step=8.330, val_loss_epoch=8.180, loss_epoch=59.80]Epoch 38, Train_loss: 59.777\n",
      "Epoch 39: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=38.00, val_loss_step=8.160, val_loss_epoch=8.010, loss_epoch=59.80]Epoch 39, Train_loss: 59.774\n",
      "Epoch 40: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=62.50, val_loss_step=8.350, val_loss_epoch=8.620, loss_epoch=62.50]Epoch 40, Train_loss: 62.529\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 41: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=30.30, val_loss_step=7.490, val_loss_epoch=6.880, loss_epoch=51.90]Epoch 41, Train_loss: 51.853\n",
      "Epoch 42: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=57.80, val_loss_step=7.420, val_loss_epoch=6.880, loss_epoch=54.80]Epoch 42, Train_loss: 54.84\n",
      "Epoch 43: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=76.30, val_loss_step=7.410, val_loss_epoch=6.710, loss_epoch=56.30]Epoch 43, Train_loss: 56.349\n",
      "Epoch 44: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=54.70, val_loss_step=7.540, val_loss_epoch=6.770, loss_epoch=53.80]Epoch 44, Train_loss: 53.831\n",
      "Epoch 45: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=42.90, val_loss_step=7.360, val_loss_epoch=6.520, loss_epoch=50.90]Epoch 45, Train_loss: 50.924\n",
      "Epoch 46: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=80, loss_step=69.20, val_loss_step=7.380, val_loss_epoch=6.500, loss_epoch=49.10]Epoch 46, Train_loss: 49.052\n",
      "Epoch 47: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=47.70, val_loss_step=7.870, val_loss_epoch=7.310, loss_epoch=50.80]Epoch 47, Train_loss: 50.836\n",
      "Epoch 48: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=57.10, val_loss_step=7.350, val_loss_epoch=6.720, loss_epoch=52.00]Epoch 48, Train_loss: 51.991\n",
      "Epoch 49: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s, v_num=80, loss_step=29.30, val_loss_step=7.490, val_loss_epoch=6.510, loss_epoch=52.10]Epoch 49, Train_loss: 52.068\n",
      "Epoch 50: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=40.90, val_loss_step=7.460, val_loss_epoch=6.720, loss_epoch=52.40]Epoch 50, Train_loss: 52.442\n",
      "Epoch 51: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=72.80, val_loss_step=7.360, val_loss_epoch=6.590, loss_epoch=50.00]Epoch 51, Train_loss: 49.979\n",
      "Epoch 52: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=39.40, val_loss_step=7.330, val_loss_epoch=6.520, loss_epoch=51.40]Epoch 52, Train_loss: 51.351\n",
      "Epoch 00053: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 53: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=100.0, val_loss_step=7.180, val_loss_epoch=5.960, loss_epoch=48.80]Epoch 53, Train_loss: 48.824\n",
      "Epoch 54: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=89.00, val_loss_step=7.890, val_loss_epoch=6.810, loss_epoch=49.40]Epoch 54, Train_loss: 49.367\n",
      "Epoch 55: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=58.20, val_loss_step=7.140, val_loss_epoch=6.030, loss_epoch=49.20]Epoch 55, Train_loss: 49.22\n",
      "Epoch 56: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=42.00, val_loss_step=7.170, val_loss_epoch=5.870, loss_epoch=46.30]Epoch 56, Train_loss: 46.338\n",
      "Epoch 57: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=46.00, val_loss_step=7.230, val_loss_epoch=5.930, loss_epoch=52.80]Epoch 57, Train_loss: 52.758\n",
      "Epoch 58: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=45.80, val_loss_step=7.230, val_loss_epoch=5.940, loss_epoch=46.80]Epoch 58, Train_loss: 46.78\n",
      "Epoch 59: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=49.60, val_loss_step=7.300, val_loss_epoch=5.970, loss_epoch=49.00]Epoch 59, Train_loss: 48.993\n",
      "Epoch 60: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=44.40, val_loss_step=7.260, val_loss_epoch=5.990, loss_epoch=47.80]Epoch 60, Train_loss: 47.774\n",
      "Epoch 61: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=63.30, val_loss_step=7.360, val_loss_epoch=6.110, loss_epoch=50.10]Epoch 61, Train_loss: 50.056\n",
      "Epoch 62: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=22.60, val_loss_step=7.250, val_loss_epoch=5.940, loss_epoch=48.90]Epoch 62, Train_loss: 48.866\n",
      "Epoch 00063: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 63: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=53.30, val_loss_step=7.480, val_loss_epoch=6.000, loss_epoch=44.60]Epoch 63, Train_loss: 44.55\n",
      "Epoch 64: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=36.10, val_loss_step=7.390, val_loss_epoch=6.050, loss_epoch=48.70]Epoch 64, Train_loss: 48.733\n",
      "Epoch 65: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=32.60, val_loss_step=7.380, val_loss_epoch=5.990, loss_epoch=50.70]Epoch 65, Train_loss: 50.749\n",
      "Epoch 66: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=69.50, val_loss_step=7.370, val_loss_epoch=5.880, loss_epoch=48.50]Epoch 66, Train_loss: 48.454\n",
      "Epoch 67: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=52.10, val_loss_step=7.310, val_loss_epoch=5.850, loss_epoch=51.20]Epoch 67, Train_loss: 51.202\n",
      "Epoch 68: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=44.20, val_loss_step=7.490, val_loss_epoch=6.030, loss_epoch=46.90]Epoch 68, Train_loss: 46.867\n",
      "Epoch 69: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s, v_num=80, loss_step=37.30, val_loss_step=7.420, val_loss_epoch=5.880, loss_epoch=52.00]Epoch 69, Train_loss: 52.008\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 70: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=74.70, val_loss_step=7.480, val_loss_epoch=5.990, loss_epoch=53.50]Epoch 70, Train_loss: 53.514\n",
      "Epoch 71: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=48.00, val_loss_step=7.370, val_loss_epoch=5.850, loss_epoch=50.10]Epoch 71, Train_loss: 50.12\n",
      "Epoch 72: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=62.70, val_loss_step=7.450, val_loss_epoch=5.920, loss_epoch=49.30]Epoch 72, Train_loss: 49.282\n",
      "Epoch 73: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=80, loss_step=81.50, val_loss_step=7.630, val_loss_epoch=6.150, loss_epoch=48.00]Epoch 73, Train_loss: 47.977\n",
      "Epoch 74: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=80, loss_step=36.20, val_loss_step=7.460, val_loss_epoch=6.000, loss_epoch=47.40]Epoch 74, Train_loss: 47.417\n",
      "Epoch 75: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=36.60, val_loss_step=7.350, val_loss_epoch=5.890, loss_epoch=46.30]Epoch 75, Train_loss: 46.251\n",
      "Epoch 00076: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 76: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=46.90, val_loss_step=7.490, val_loss_epoch=5.910, loss_epoch=48.40]Epoch 76, Train_loss: 48.407\n",
      "Epoch 77: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=80, loss_step=47.50, val_loss_step=7.430, val_loss_epoch=5.940, loss_epoch=47.70]Epoch 77, Train_loss: 47.736\n",
      "Epoch 78: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s, v_num=80, loss_step=54.40, val_loss_step=7.490, val_loss_epoch=5.940, loss_epoch=46.30]Epoch 78, Train_loss: 46.273\n",
      "Epoch 79: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=80, loss_step=41.80, val_loss_step=7.570, val_loss_epoch=5.980, loss_epoch=48.30]Epoch 79, Train_loss: 48.289\n",
      "Epoch 80: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=80, loss_step=79.80, val_loss_step=7.520, val_loss_epoch=6.090, loss_epoch=50.20]Epoch 80, Train_loss: 50.168\n",
      "Epoch 81: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s, v_num=80, loss_step=45.20, val_loss_step=7.570, val_loss_epoch=5.920, loss_epoch=48.10]Epoch 81, Train_loss: 48.119\n",
      "Epoch 00082: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 82: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=80, loss_step=61.50, val_loss_step=7.350, val_loss_epoch=5.820, loss_epoch=46.80]Epoch 82, Train_loss: 46.808\n",
      "Epoch 83: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=74.20, val_loss_step=7.630, val_loss_epoch=5.990, loss_epoch=48.40]Epoch 83, Train_loss: 48.442\n",
      "Epoch 84: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=80, loss_step=61.00, val_loss_step=7.480, val_loss_epoch=5.960, loss_epoch=48.90]Epoch 84, Train_loss: 48.872\n",
      "Epoch 85: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=50.20, val_loss_step=7.360, val_loss_epoch=5.970, loss_epoch=47.20]Epoch 85, Train_loss: 47.218\n",
      "Epoch 86: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=80, loss_step=53.50, val_loss_step=7.530, val_loss_epoch=5.990, loss_epoch=47.70]Epoch 86, Train_loss: 47.706\n",
      "Epoch 87: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=80, loss_step=22.10, val_loss_step=7.410, val_loss_epoch=6.000, loss_epoch=49.60]Epoch 87, Train_loss: 49.637\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 88: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=80, loss_step=32.10, val_loss_step=7.610, val_loss_epoch=5.960, loss_epoch=46.90]Epoch 88, Train_loss: 46.884\n",
      "Epoch 89: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=80, loss_step=33.60, val_loss_step=7.540, val_loss_epoch=6.030, loss_epoch=49.00]Epoch 89, Train_loss: 49.004\n",
      "Epoch 90: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=80, loss_step=28.50, val_loss_step=7.420, val_loss_epoch=5.890, loss_epoch=48.20]Epoch 90, Train_loss: 48.169\n",
      "Epoch 91: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=80, loss_step=44.90, val_loss_step=7.550, val_loss_epoch=6.080, loss_epoch=48.40]Epoch 91, Train_loss: 48.431\n",
      "Epoch 92: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=33.30, val_loss_step=7.580, val_loss_epoch=6.170, loss_epoch=48.60]Epoch 92, Train_loss: 48.592\n",
      "Epoch 93: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=80, loss_step=38.30, val_loss_step=7.810, val_loss_epoch=6.190, loss_epoch=48.80]Epoch 93, Train_loss: 48.761\n",
      "Epoch 00094: reducing learning rate of group 0 to 2.5600e-08.\n",
      "Epoch 94: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=80, loss_step=46.70, val_loss_step=7.640, val_loss_epoch=6.020, loss_epoch=49.00]Epoch 94, Train_loss: 49.004\n",
      "Epoch 95: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=80, loss_step=59.90, val_loss_step=7.760, val_loss_epoch=6.070, loss_epoch=45.40]Epoch 95, Train_loss: 45.362\n",
      "Epoch 96: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=42.30, val_loss_step=7.420, val_loss_epoch=5.820, loss_epoch=48.30]Epoch 96, Train_loss: 48.295\n",
      "Epoch 97: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=80, loss_step=27.40, val_loss_step=7.730, val_loss_epoch=6.160, loss_epoch=49.20]Epoch 97, Train_loss: 49.211\n",
      "Epoch 98: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=80, loss_step=46.90, val_loss_step=7.490, val_loss_epoch=5.890, loss_epoch=46.70]Epoch 98, Train_loss: 46.687\n",
      "Epoch 99: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=80, loss_step=45.40, val_loss_step=7.420, val_loss_epoch=5.940, loss_epoch=47.80]Epoch 99, Train_loss: 47.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100: reducing learning rate of group 0 to 5.1200e-09.\n",
      "Epoch 99: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=80, loss_step=45.40, val_loss_step=7.420, val_loss_epoch=5.940, loss_epoch=47.80]\n"
     ]
    }
   ],
   "source": [
    "model_15 = detection_26.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | FacesPointsDetector | 4.3 M \n",
      "----------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.286    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\CV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=126.0, val_loss_step=32.00, val_loss_epoch=47.70, loss_epoch=285.0]Epoch 0, Train_loss: 285.485\n",
      "Epoch 1: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s, v_num=81, loss_step=137.0, val_loss_step=38.30, val_loss_epoch=49.50, loss_epoch=153.0]Epoch 1, Train_loss: 152.666\n",
      "Epoch 2: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=206.0, val_loss_step=37.80, val_loss_epoch=48.80, loss_epoch=143.0]Epoch 2, Train_loss: 142.776\n",
      "Epoch 3: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=110.0, val_loss_step=26.60, val_loss_epoch=34.70, loss_epoch=130.0]Epoch 3, Train_loss: 130.244\n",
      "Epoch 4: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=131.0, val_loss_step=17.50, val_loss_epoch=31.10, loss_epoch=121.0]Epoch 4, Train_loss: 121.398\n",
      "Epoch 5: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=78.50, val_loss_step=16.90, val_loss_epoch=25.20, loss_epoch=112.0]Epoch 5, Train_loss: 112.297\n",
      "Epoch 6: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=122.0, val_loss_step=12.20, val_loss_epoch=20.20, loss_epoch=103.0]Epoch 6, Train_loss: 102.772\n",
      "Epoch 7: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=76.60, val_loss_step=14.60, val_loss_epoch=21.60, loss_epoch=99.40]Epoch 7, Train_loss: 99.439\n",
      "Epoch 8: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=113.0, val_loss_step=11.30, val_loss_epoch=17.70, loss_epoch=99.70]Epoch 8, Train_loss: 99.728\n",
      "Epoch 9: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=98.30, val_loss_step=11.10, val_loss_epoch=16.40, loss_epoch=94.10]Epoch 9, Train_loss: 94.138\n",
      "Epoch 10: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=84.20, val_loss_step=11.30, val_loss_epoch=16.10, loss_epoch=91.80]Epoch 10, Train_loss: 91.807\n",
      "Epoch 11: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=61.90, val_loss_step=11.10, val_loss_epoch=15.90, loss_epoch=91.80]Epoch 11, Train_loss: 91.795\n",
      "Epoch 12: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=72.80, val_loss_step=9.730, val_loss_epoch=13.70, loss_epoch=87.20]Epoch 12, Train_loss: 87.209\n",
      "Epoch 13: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=47.60, val_loss_step=9.740, val_loss_epoch=16.00, loss_epoch=87.40]Epoch 13, Train_loss: 87.357\n",
      "Epoch 14: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=83.50, val_loss_step=10.90, val_loss_epoch=18.50, loss_epoch=83.90]Epoch 14, Train_loss: 83.905\n",
      "Epoch 15: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=89.10, val_loss_step=10.90, val_loss_epoch=13.20, loss_epoch=84.30]Epoch 15, Train_loss: 84.344\n",
      "Epoch 16: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=102.0, val_loss_step=13.80, val_loss_epoch=16.10, loss_epoch=80.50]Epoch 16, Train_loss: 80.521\n",
      "Epoch 17: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=90.40, val_loss_step=13.50, val_loss_epoch=14.80, loss_epoch=80.20]Epoch 17, Train_loss: 80.194\n",
      "Epoch 18: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=58.10, val_loss_step=9.290, val_loss_epoch=13.00, loss_epoch=78.30]Epoch 18, Train_loss: 78.299\n",
      "Epoch 19: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s, v_num=81, loss_step=72.80, val_loss_step=9.190, val_loss_epoch=13.10, loss_epoch=72.70]Epoch 19, Train_loss: 72.651\n",
      "Epoch 20: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=94.40, val_loss_step=14.10, val_loss_epoch=13.40, loss_epoch=70.20]Epoch 20, Train_loss: 70.204\n",
      "Epoch 21: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=85.90, val_loss_step=11.80, val_loss_epoch=12.50, loss_epoch=72.20]Epoch 21, Train_loss: 72.229\n",
      "Epoch 22: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=122.0, val_loss_step=9.050, val_loss_epoch=11.50, loss_epoch=71.70]Epoch 22, Train_loss: 71.725\n",
      "Epoch 23: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=85.40, val_loss_step=13.50, val_loss_epoch=15.40, loss_epoch=79.00]Epoch 23, Train_loss: 79.049\n",
      "Epoch 24: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=65.90, val_loss_step=8.440, val_loss_epoch=9.660, loss_epoch=73.20]Epoch 24, Train_loss: 73.167\n",
      "Epoch 25: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=61.80, val_loss_step=10.80, val_loss_epoch=12.40, loss_epoch=70.40]Epoch 25, Train_loss: 70.375\n",
      "Epoch 26: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=55.40, val_loss_step=15.10, val_loss_epoch=15.70, loss_epoch=69.70]Epoch 26, Train_loss: 69.67\n",
      "Epoch 27: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=45.00, val_loss_step=8.030, val_loss_epoch=9.530, loss_epoch=65.00]Epoch 27, Train_loss: 64.962\n",
      "Epoch 28: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=62.50, val_loss_step=8.180, val_loss_epoch=9.730, loss_epoch=69.30]Epoch 28, Train_loss: 69.323\n",
      "Epoch 29: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=34.50, val_loss_step=8.820, val_loss_epoch=9.260, loss_epoch=62.80]Epoch 29, Train_loss: 62.785\n",
      "Epoch 30: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=58.40, val_loss_step=9.310, val_loss_epoch=10.40, loss_epoch=65.30]Epoch 30, Train_loss: 65.313\n",
      "Epoch 31: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=65.50, val_loss_step=7.730, val_loss_epoch=9.380, loss_epoch=65.90]Epoch 31, Train_loss: 65.945\n",
      "Epoch 32: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=69.60, val_loss_step=7.890, val_loss_epoch=9.230, loss_epoch=65.00]Epoch 32, Train_loss: 64.973\n",
      "Epoch 33: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=60.70, val_loss_step=10.10, val_loss_epoch=9.640, loss_epoch=63.30]Epoch 33, Train_loss: 63.308\n",
      "Epoch 34: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=68.30, val_loss_step=8.490, val_loss_epoch=8.100, loss_epoch=61.00]Epoch 34, Train_loss: 61.027\n",
      "Epoch 35: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=38.60, val_loss_step=9.080, val_loss_epoch=9.550, loss_epoch=62.50]Epoch 35, Train_loss: 62.491\n",
      "Epoch 36: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=63.90, val_loss_step=7.920, val_loss_epoch=8.390, loss_epoch=58.50]Epoch 36, Train_loss: 58.55\n",
      "Epoch 37: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=36.50, val_loss_step=7.290, val_loss_epoch=8.560, loss_epoch=61.10]Epoch 37, Train_loss: 61.108\n",
      "Epoch 38: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=47.60, val_loss_step=9.460, val_loss_epoch=9.070, loss_epoch=61.50]Epoch 38, Train_loss: 61.515\n",
      "Epoch 39: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=105.0, val_loss_step=7.850, val_loss_epoch=7.660, loss_epoch=59.50]Epoch 39, Train_loss: 59.495\n",
      "Epoch 40: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=60.00, val_loss_step=7.570, val_loss_epoch=7.320, loss_epoch=55.40]Epoch 40, Train_loss: 55.426\n",
      "Epoch 41: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=46.80, val_loss_step=7.590, val_loss_epoch=7.870, loss_epoch=55.50]Epoch 41, Train_loss: 55.523\n",
      "Epoch 42: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=86.30, val_loss_step=7.960, val_loss_epoch=8.610, loss_epoch=57.80]Epoch 42, Train_loss: 57.797\n",
      "Epoch 43: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=56.10, val_loss_step=11.60, val_loss_epoch=11.60, loss_epoch=57.40]Epoch 43, Train_loss: 57.393\n",
      "Epoch 44: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s, v_num=81, loss_step=55.40, val_loss_step=8.780, val_loss_epoch=8.550, loss_epoch=54.30]Epoch 44, Train_loss: 54.33\n",
      "Epoch 45: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=81, loss_step=80.30, val_loss_step=7.590, val_loss_epoch=7.370, loss_epoch=59.30]Epoch 45, Train_loss: 59.287\n",
      "Epoch 46: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=81, loss_step=56.30, val_loss_step=9.510, val_loss_epoch=7.940, loss_epoch=56.20]Epoch 46, Train_loss: 56.158\n",
      "Epoch 47: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s, v_num=81, loss_step=32.90, val_loss_step=8.150, val_loss_epoch=7.650, loss_epoch=56.40]Epoch 47, Train_loss: 56.439\n",
      "Epoch 48: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=52.10, val_loss_step=7.750, val_loss_epoch=7.610, loss_epoch=58.60]Epoch 48, Train_loss: 58.616\n",
      "Epoch 49: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=84.50, val_loss_step=8.030, val_loss_epoch=7.990, loss_epoch=56.50]Epoch 49, Train_loss: 56.534\n",
      "Epoch 50: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=45.10, val_loss_step=7.680, val_loss_epoch=7.200, loss_epoch=62.50]Epoch 50, Train_loss: 62.531\n",
      "Epoch 51: 100%|██████████| 150/150 [01:08<00:00,  2.17it/s, v_num=81, loss_step=38.70, val_loss_step=7.820, val_loss_epoch=7.500, loss_epoch=56.60]Epoch 51, Train_loss: 56.644\n",
      "Epoch 52: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s, v_num=81, loss_step=62.00, val_loss_step=8.240, val_loss_epoch=8.730, loss_epoch=58.80]Epoch 52, Train_loss: 58.778\n",
      "Epoch 53: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s, v_num=81, loss_step=35.20, val_loss_step=8.100, val_loss_epoch=8.360, loss_epoch=53.40]Epoch 53, Train_loss: 53.413\n",
      "Epoch 54: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=81, loss_step=51.70, val_loss_step=7.540, val_loss_epoch=7.720, loss_epoch=56.10]Epoch 54, Train_loss: 56.113\n",
      "Epoch 55: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s, v_num=81, loss_step=47.80, val_loss_step=8.130, val_loss_epoch=8.480, loss_epoch=55.20]Epoch 55, Train_loss: 55.227\n",
      "Epoch 56: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s, v_num=81, loss_step=39.70, val_loss_step=7.450, val_loss_epoch=7.700, loss_epoch=54.40]Epoch 56, Train_loss: 54.41\n",
      "Epoch 57: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=61.10, val_loss_step=7.860, val_loss_epoch=8.410, loss_epoch=57.40]Epoch 57, Train_loss: 57.38\n",
      "Epoch 58: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=64.70, val_loss_step=7.480, val_loss_epoch=6.940, loss_epoch=55.10]Epoch 58, Train_loss: 55.053\n",
      "Epoch 59: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=80.10, val_loss_step=8.730, val_loss_epoch=8.820, loss_epoch=60.80]Epoch 59, Train_loss: 60.774\n",
      "Epoch 60: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=52.60, val_loss_step=7.560, val_loss_epoch=6.830, loss_epoch=53.20]Epoch 60, Train_loss: 53.225\n",
      "Epoch 61: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=41.60, val_loss_step=7.340, val_loss_epoch=8.280, loss_epoch=53.20]Epoch 61, Train_loss: 53.19\n",
      "Epoch 62: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=44.70, val_loss_step=7.950, val_loss_epoch=7.530, loss_epoch=54.70]Epoch 62, Train_loss: 54.657\n",
      "Epoch 63: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s, v_num=81, loss_step=47.40, val_loss_step=8.010, val_loss_epoch=7.090, loss_epoch=53.00]Epoch 63, Train_loss: 52.998\n",
      "Epoch 64: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=32.50, val_loss_step=7.290, val_loss_epoch=7.120, loss_epoch=50.50]Epoch 64, Train_loss: 50.525\n",
      "Epoch 65: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s, v_num=81, loss_step=52.80, val_loss_step=8.860, val_loss_epoch=8.340, loss_epoch=51.00]Epoch 65, Train_loss: 50.955\n",
      "Epoch 66: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s, v_num=81, loss_step=36.20, val_loss_step=7.250, val_loss_epoch=6.790, loss_epoch=54.60]Epoch 66, Train_loss: 54.598\n",
      "Epoch 67: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s, v_num=81, loss_step=92.40, val_loss_step=7.040, val_loss_epoch=6.940, loss_epoch=52.50]Epoch 67, Train_loss: 52.511\n",
      "Epoch 68: 100%|██████████| 150/150 [01:11<00:00,  2.08it/s, v_num=81, loss_step=78.70, val_loss_step=7.930, val_loss_epoch=7.720, loss_epoch=53.00]Epoch 68, Train_loss: 52.991\n",
      "Epoch 69: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=71.70, val_loss_step=6.910, val_loss_epoch=6.900, loss_epoch=52.50]Epoch 69, Train_loss: 52.481\n",
      "Epoch 70: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=52.20, val_loss_step=7.730, val_loss_epoch=7.670, loss_epoch=52.60]Epoch 70, Train_loss: 52.553\n",
      "Epoch 71: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=65.10, val_loss_step=7.590, val_loss_epoch=7.540, loss_epoch=51.10]Epoch 71, Train_loss: 51.057\n",
      "Epoch 72: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=67.60, val_loss_step=7.410, val_loss_epoch=7.300, loss_epoch=61.90]Epoch 72, Train_loss: 61.866\n",
      "Epoch 73: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=30.50, val_loss_step=7.520, val_loss_epoch=7.210, loss_epoch=59.00]Epoch 73, Train_loss: 58.983\n",
      "Epoch 74: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s, v_num=81, loss_step=56.30, val_loss_step=6.850, val_loss_epoch=6.750, loss_epoch=58.20]Epoch 74, Train_loss: 58.23\n",
      "Epoch 75: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=48.50, val_loss_step=7.320, val_loss_epoch=6.750, loss_epoch=54.50]Epoch 75, Train_loss: 54.482\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 76: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=34.60, val_loss_step=6.810, val_loss_epoch=6.370, loss_epoch=49.10]Epoch 76, Train_loss: 49.127\n",
      "Epoch 77: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=24.40, val_loss_step=6.770, val_loss_epoch=6.020, loss_epoch=47.60]Epoch 77, Train_loss: 47.626\n",
      "Epoch 78: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=43.60, val_loss_step=6.820, val_loss_epoch=5.940, loss_epoch=48.80]Epoch 78, Train_loss: 48.786\n",
      "Epoch 79: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=47.20, val_loss_step=6.880, val_loss_epoch=6.040, loss_epoch=47.80]Epoch 79, Train_loss: 47.812\n",
      "Epoch 80: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=64.90, val_loss_step=6.800, val_loss_epoch=5.850, loss_epoch=48.40]Epoch 80, Train_loss: 48.412\n",
      "Epoch 81: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=45.20, val_loss_step=6.890, val_loss_epoch=5.930, loss_epoch=50.80]Epoch 81, Train_loss: 50.803\n",
      "Epoch 82: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=37.40, val_loss_step=6.780, val_loss_epoch=5.750, loss_epoch=44.50]Epoch 82, Train_loss: 44.453\n",
      "Epoch 83: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=54.90, val_loss_step=6.870, val_loss_epoch=6.110, loss_epoch=50.00]Epoch 83, Train_loss: 49.978\n",
      "Epoch 84: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=52.30, val_loss_step=6.730, val_loss_epoch=5.870, loss_epoch=46.90]Epoch 84, Train_loss: 46.932\n",
      "Epoch 85: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=58.20, val_loss_step=6.790, val_loss_epoch=5.790, loss_epoch=45.50]Epoch 85, Train_loss: 45.452\n",
      "Epoch 86: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=42.00, val_loss_step=6.700, val_loss_epoch=5.700, loss_epoch=46.00]Epoch 86, Train_loss: 46.028\n",
      "Epoch 87: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=46.60, val_loss_step=6.860, val_loss_epoch=5.830, loss_epoch=48.90]Epoch 87, Train_loss: 48.889\n",
      "Epoch 88: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=29.70, val_loss_step=6.960, val_loss_epoch=5.800, loss_epoch=46.00]Epoch 88, Train_loss: 45.996\n",
      "Epoch 89: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s, v_num=81, loss_step=32.10, val_loss_step=6.860, val_loss_epoch=5.710, loss_epoch=45.80]Epoch 89, Train_loss: 45.801\n",
      "Epoch 90: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=59.50, val_loss_step=6.900, val_loss_epoch=5.620, loss_epoch=49.10]Epoch 90, Train_loss: 49.137\n",
      "Epoch 91: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=59.00, val_loss_step=6.850, val_loss_epoch=5.790, loss_epoch=46.80]Epoch 91, Train_loss: 46.759\n",
      "Epoch 92: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s, v_num=81, loss_step=86.80, val_loss_step=6.970, val_loss_epoch=5.910, loss_epoch=42.80]Epoch 92, Train_loss: 42.785\n",
      "Epoch 93: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=50.70, val_loss_step=6.710, val_loss_epoch=5.620, loss_epoch=45.50]Epoch 93, Train_loss: 45.47\n",
      "Epoch 94: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=40.40, val_loss_step=6.900, val_loss_epoch=5.690, loss_epoch=45.00]Epoch 94, Train_loss: 45.004\n",
      "Epoch 95: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s, v_num=81, loss_step=43.50, val_loss_step=6.830, val_loss_epoch=5.600, loss_epoch=47.30]Epoch 95, Train_loss: 47.294\n",
      "Epoch 96: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=95.80, val_loss_step=6.860, val_loss_epoch=5.640, loss_epoch=45.20]Epoch 96, Train_loss: 45.234\n",
      "Epoch 97: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s, v_num=81, loss_step=36.20, val_loss_step=6.770, val_loss_epoch=5.640, loss_epoch=48.10]Epoch 97, Train_loss: 48.075\n",
      "Epoch 98: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=32.50, val_loss_step=6.830, val_loss_epoch=5.720, loss_epoch=47.40]Epoch 98, Train_loss: 47.409\n",
      "Epoch 99: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s, v_num=81, loss_step=72.80, val_loss_step=6.740, val_loss_epoch=5.600, loss_epoch=45.40]Epoch 99, Train_loss: 45.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s, v_num=81, loss_step=72.80, val_loss_step=6.740, val_loss_epoch=5.600, loss_epoch=45.40]\n"
     ]
    }
   ],
   "source": [
    "import detection_27\n",
    "model_15 = detection_27.train_detector(train_gt=train_gt_path, train_img_dir=train_imgs_path, fast_train=False, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
