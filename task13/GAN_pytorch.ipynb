{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQg-V9XQ2T_f"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "Generative Adversarial Networks (GANs) are an approach to generative modeling based on deep learning methods.\n",
        "\n",
        "The standard problem settings for GANs are generation of photorealistic images and image-to-image translation tasks (translating photos of summer to winter, day to night etc.).\n",
        "\n",
        "In this task you will familiarize yourself with both these problems while trying to create fake images of sneakers.\n",
        "\n",
        "As GANs still do have certain limitations about generating large images, the task is decomposed into two: first, use a simple GAN to generate a bunch of low resolution images from noise, then upscale them using another generative model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-1cXZG02T_g"
      },
      "source": [
        "This notebook is built around [PyTorch](https://pytorch.org/) and [Lightning](https://pytorchlightning.ai/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E4XlI2qi2T_g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "-bash: line 4: unzip: command not found\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b\"\\nif [ ! -d data/ ]; then\\n    curl -sO 'https://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip'\\n    unzip -qo data.zip\\nfi\\n\"' returned non-zero exit status 127.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif [ ! -d data/ ]; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    curl -sO \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    unzip -qo data.zip\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2338\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2337\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2338\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\IPython\\core\\magics\\script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[1;34m(line, cell)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\IPython\\core\\magics\\script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command 'b\"\\nif [ ! -d data/ ]; then\\n    curl -sO 'https://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip'\\n    unzip -qo data.zip\\nfi\\n\"' returned non-zero exit status 127."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -d data/ ]; then\n",
        "    curl -sO 'https://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip'\n",
        "    unzip -qo data.zip\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG4kmBo8R0Jn",
        "outputId": "7f30baad-45ec-4c47-b828-f4217e34fa40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.2.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LMNVXiEZ2T_g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IkYc8NIvRpmF"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = (28, 28)\n",
        "NOISE_DIM = 100\n",
        "LOW_RES_SIZE = IMAGE_SIZE\n",
        "HIGH_RES_SIZE = (112, 112)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8nVxCxC2T_g",
        "outputId": "f429c96e-c482-4504-928b-7b8d280f1762"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5729"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_kwargs = {}\n",
        "dataset_root = \"./data\"\n",
        "images_dir = \"images\"\n",
        "image_filenames = sorted(os.listdir(os.path.join(dataset_root, images_dir)))\n",
        "len(image_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxy294r2T_h"
      },
      "source": [
        "# I. Image Generation\n",
        "\n",
        "Your first task is to solve a problem of generating photorealistic images out of noise (okay, this might sound optimistic).\n",
        "\n",
        "Namely, you are required to **create fake images of sneakers of resolution 28x28 given a vector of noise sampled from standard normal distribution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEeSWf-t2T_h"
      },
      "source": [
        "Here is simple PyTorch `Dataset` class for loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bh6UclOg2T_h"
      },
      "outputs": [],
      "source": [
        "class SneakersDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        images_dir=\"images\",\n",
        "        input_size=None,\n",
        "        target_size=None,\n",
        "    ):\n",
        "        self.images_dir = os.path.join(root_dir, images_dir)\n",
        "        self.input_size = input_size\n",
        "        self.target_size = target_size\n",
        "        files = os.listdir(self.images_dir)\n",
        "        self.all_images = sorted([file for file in files if file.endswith(\".jpg\")])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.images_dir, self.all_images[idx])\n",
        "        image_bgr = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "        if self.input_size is not None:\n",
        "            input_rgb = cv2.resize(image_rgb, self.input_size)\n",
        "            input_rgb = (input_rgb / 255).astype(np.float32)\n",
        "        if self.target_size is not None:\n",
        "            image_rgb = cv2.resize(image_rgb, self.target_size)\n",
        "        image_rgb = (image_rgb / 255).astype(np.float32)\n",
        "\n",
        "        # tanh values in [-1, 1]\n",
        "        image_rgb = torch.from_numpy(image_rgb * 2 - 1)\n",
        "        if self.input_size is not None:\n",
        "            input_rgb = torch.from_numpy(input_rgb * 2 - 1)\n",
        "            return input_rgb, image_rgb\n",
        "        else:\n",
        "            return image_rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miRyiLHc2T_h"
      },
      "source": [
        "Here we create `LightningDataModule` which will handle dataset loading in our case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_tePuYxj2T_h"
      },
      "outputs": [],
      "source": [
        "class SneakersGANDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.dataset = SneakersDataset(self.data_dir, target_size=IMAGE_SIZE)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=self.shuffle,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B2MfJORX2T_h"
      },
      "outputs": [],
      "source": [
        "def data2img(d: np.ndarray):\n",
        "    return 0.5 * d + 0.5\n",
        "\n",
        "\n",
        "def visualize_images(data, n_rows, n_cols):\n",
        "    n_samples = n_rows * n_cols\n",
        "\n",
        "    if len(data) != n_samples:\n",
        "        sample_indices = np.random.choice(\n",
        "            len(data),\n",
        "            n_samples,\n",
        "            replace=len(data) < n_samples,\n",
        "        )\n",
        "    else:\n",
        "        sample_indices = np.arange(len(data)).astype(int)\n",
        "\n",
        "    plt.figure(figsize=(int(2.5 * n_cols), int(2.5 * n_rows)))\n",
        "    for i, sample_index in enumerate(sample_indices):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        plt.imshow(data2img(data[sample_index]))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "rFngx2hW2T_h",
        "outputId": "1244ad07-4083-4b8d-ff39-914d2177149d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x720 with 32 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "dm = SneakersGANDataModule(\"data\", batch_size=32)\n",
        "dm.setup()\n",
        "visualize_images(next(iter(dm.train_dataloader())), 4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ixooZ_e2T_h"
      },
      "source": [
        "## Fully connected GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9nD0Oy2T_i"
      },
      "source": [
        "### Generator (0.5 pts) [cross-check:0]\n",
        "\n",
        "Our first step is to build a generator. You should use the layers in `torch.nn` (imported in the beginning as `nn`) to construct the model. Since we are using PyTorch and Lightning, you should create `nn.Module`. Use the default initializers for parameters.\n",
        "\n",
        "Architecture:\n",
        " * Fully connected (`Linear` in PyTorch) with output size of 1024\n",
        " * ReLU\n",
        " * Fully connected with output size of 1024\n",
        " * ReLU\n",
        " * Fully connected with output size of 28 x 28 x 3\n",
        " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
        " * Reshape into (28, 28, 3)\n",
        "\n",
        "> You could perform reshaping inside of `forward` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ek4myHCp2T_i"
      },
      "outputs": [],
      "source": [
        "class FCGenerator(nn.Module):\n",
        "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
        "        super().__init__()\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # TODO: add other layers\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28*28*3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z).reshape(-1, 28,28,3)\n",
        "        # keep batch size\n",
        "        img = img.view(img.size(0), *self.img_shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J4Ln0kZM2T_i"
      },
      "outputs": [],
      "source": [
        "fc_gen = FCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
        "img_generated = fc_gen(torch.randn(32, NOISE_DIM))\n",
        "assert img_generated.shape[1:] == IMAGE_SIZE + (3,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "You1A1iK2T_i"
      },
      "source": [
        "### Discriminator (0.5 pts) [cross-check:1]\n",
        "\n",
        "Now you are to build a discriminator. You should use the default initializers for parameters here as well.\n",
        "\n",
        "Architecture:\n",
        " * Flatten\n",
        " * Fully connected with output size of 256\n",
        " * Leaky ReLU(0.01)\n",
        " * Fully connected with output size of 256\n",
        " * Leaky ReLU(0.01)\n",
        " * Fully connected with output size of 1\n",
        " * Sigmoid (to obtain logits as an output)\n",
        "\n",
        "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the probability that each of the `batch_size` inputs is a real image.\n",
        "\n",
        "> You could perform flattening inside of `forward` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DyiCu7cn2T_i"
      },
      "outputs": [],
      "source": [
        "class FCDiscriminator(nn.Module):\n",
        "    def __init__(self, img_shape: tuple):\n",
        "        super().__init__()\n",
        "\n",
        "        inp = img_shape[0]*img_shape[1]*3\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(inp, 256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        return self.model(img_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhCMNZ_k2T_i",
        "outputId": "543d099f-19a8-44b1-bf99-fd03317d76b0"
      },
      "outputs": [],
      "source": [
        "fc_dis = FCDiscriminator(IMAGE_SIZE + (3,))\n",
        "prob_dis = fc_dis(img_generated)\n",
        "assert prob_dis.shape[1:] == (1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjPcLoqF2T_i"
      },
      "source": [
        "## GAN Loss (1.0 pts) [cross-check:2]\n",
        "\n",
        "Compute the generator and discriminator loss. The generator loss is:\n",
        "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
        "and the discriminator loss is:\n",
        "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
        "\n",
        "Instead of computing the expectation, you may average over elements of the minibatch, so make sure to combine the loss by *averaging* instead of summing.\n",
        "\n",
        "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm73TxIqRpmJ"
      },
      "source": [
        "There is a `LightningModule` with all necessary methods. You should fill all `TODO` comments.\n",
        "\n",
        "> Don't forget to use `detach` method for discriminator training step to prevent backpropagation for generator compuational graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "30a0jfdl2T_i"
      },
      "outputs": [],
      "source": [
        "class FCGAN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        width,\n",
        "        height,\n",
        "        channels,\n",
        "        noise_dim=100,\n",
        "        lr=0.0002,\n",
        "        b1=0.5,\n",
        "        b2=0.999,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # make <arg> available as self.hparams.<arg>\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Important: This property activates manual optimization.\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        img_shape = (width, height, channels)\n",
        "        self.generator = FCGenerator(\n",
        "            noise_dim=self.hparams.noise_dim,\n",
        "            img_shape=img_shape,\n",
        "        )\n",
        "        self.discriminator = FCDiscriminator(\n",
        "            img_shape=img_shape,\n",
        "        )\n",
        "\n",
        "        self.validation_z = torch.randn(8, noise_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def gan_loss(self, y_hat, y):\n",
        "        # TODO: implement GAN loss (hint: binary cross entropy)\n",
        "        # return 1\n",
        "        # self.l1_f = nn.BCELoss()\n",
        "        # self.l2_f = nn.BCELoss()\n",
        "\n",
        "        l1 = -nn.functional.binary_cross_entropy(torch.ones_like(y_hat), y_hat)\n",
        "        l2 = nn.functional.binary_cross_entropy(torch.zeros_like(y), y)\n",
        "        return l1 + l2\n",
        "\n",
        "    def training_step(self, imgs, batch_idx):\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
        "        # move to same device as imgs\n",
        "        z = z.type_as(imgs)\n",
        "\n",
        "        # optimize generator\n",
        "        self.generated_imgs = self(z)\n",
        "\n",
        "        # all fake, but we want to be real\n",
        "        valid = torch.ones(imgs.size(0), 1)\n",
        "        valid = valid.type_as(imgs)\n",
        "\n",
        "        g_loss = self.gan_loss(self.discriminator(self(z)), valid)\n",
        "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
        "\n",
        "        opt_g.zero_grad()\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "\n",
        "        # optimize discriminator\n",
        "        valid = torch.ones(imgs.size(0), 1)\n",
        "        valid = valid.type_as(imgs)\n",
        "        # TODO: loss for `discriminator(imgs)` and `valid`\n",
        "        real_loss = -self.gan_loss(self.discriminator(imgs), valid)\n",
        "\n",
        "        # TODO: zero vector for fake_loss computation\n",
        "        fake = torch.zeros(imgs.size(0), 1)\n",
        "        fake = fake.type_as(imgs)\n",
        "\n",
        "        fake_loss = self.gan_loss(self.discriminator(self(z)), fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
        "\n",
        "        opt_d.zero_grad()\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # send results for validation_z to TensorBoard\n",
        "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
        "        # channels before pixels\n",
        "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95JM5baZmMKi"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DK1-PrBF2T_i"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_C5wl2GAbEJG"
      },
      "outputs": [],
      "source": [
        "train_kwargs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "85yC1F7RRpmJ"
      },
      "outputs": [],
      "source": [
        "train_kwargs[\"max_epochs\"] = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bS0cdhgla6Qn"
      },
      "outputs": [],
      "source": [
        "# train_kwargs['accelerator'] = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "AHcwyJFA2T_i",
        "outputId": "2faa62c1-2cec-4b67-f41a-1afd6a4f60dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    575\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    577\u001b[0m     ckpt_path,\n\u001b[0;32m    578\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:966\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\strategies\\single_device.py:77\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: pl\u001b[38;5;241m.\u001b[39mTrainer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msetup(trainer)\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\strategies\\single_device.py:74\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model must be set before self.model.to()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\lightning_fabric\\utilities\\device_dtype_mixin.py:54\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m FCGAN(\u001b[38;5;241m*\u001b[39mIMAGE_SIZE, \u001b[38;5;241m3\u001b[39m, NOISE_DIM)\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:68\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[0;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39mfinalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[0;32m     70\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1013\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;124;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m     loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_loop\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:531\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mteardown()\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\pytorch_lightning\\accelerators\\cuda.py:77\u001b[0m, in \u001b[0;36mCUDAAccelerator.teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43m_clear_cuda_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\lightning_fabric\\accelerators\\cuda.py:370\u001b[0m, in \u001b[0;36m_clear_cuda_memory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCH_GREATER_EQUAL_2_0 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_clearCublasWorkspaces\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/95668\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_clearCublasWorkspaces()\n\u001b[1;32m--> 370\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\Python3_64\\lib\\site-packages\\torch\\cuda\\memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
        "model = FCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
        "trainer = pl.Trainer(**train_kwargs)\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxsBIlLm2T_i"
      },
      "outputs": [],
      "source": [
        "visualize_images(model(torch.randn(32, NOISE_DIM)).detach(), 4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXhZuP5GRpmK"
      },
      "source": [
        "> Hint: this architecture isn't great, so don't spend all your time to train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r8wLBjp2T_i"
      },
      "source": [
        "## Deep Convolutional GANs\n",
        "In the first part of the notebook, you have implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, you are to implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where both discriminator and generator are convolutional networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlmSNMvu2T_i"
      },
      "source": [
        "### Generator (1.0 pts) [cross-check:3]\n",
        "\n",
        "Architecture:\n",
        " * Fully connected with output size of 128x7x7\n",
        " * Reshape into (128, 7, 7)\n",
        " * ReLU\n",
        " * UpSampling2D(2)\n",
        " * Conv2D: 3x3, filters=128, padding=\"same\"\n",
        " * Batch Normalization 2D with momentum(0.8)\n",
        " * ReLU\n",
        " * UpSampling2D(2)\n",
        " * Conv2D: 3x3, filters=64, padding=\"same\"\n",
        " * Batch Normalization 2D with momentum(0.8)\n",
        " * ReLU\n",
        " * Conv2D: 3x3, filters=3, padding=\"same\"\n",
        " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
        "\n",
        "> There is no `padding=\"same\"` in PyTorch, but you could use `padding=k//2` and `padding_mode=\"zeros\"` for kernel size `k`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px1zHOt02T_i"
      },
      "outputs": [],
      "source": [
        "class DCGenerator(nn.Module):\n",
        "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
        "        super().__init__()\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        # make channel axis last\n",
        "        img = img.permute(0, 2, 3, 1)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VlTQU7d2T_i"
      },
      "outputs": [],
      "source": [
        "dc_gen = DCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
        "fake_imgs = dc_gen(torch.randn(10, NOISE_DIM))\n",
        "assert fake_imgs.shape[1:] == IMAGE_SIZE + (3,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyqq0l5l2T_i"
      },
      "source": [
        "### Discriminator (1.0 pts) [cross-check:4]\n",
        "\n",
        "Architecture:\n",
        " * Conv2D: 3x3, filters=32, strides=2, padding=\"same\"\n",
        " * Leaky ReLU(0.2)\n",
        " * Dropout(0.25)\n",
        " * Conv2D: 3x3, filters=64, strides=2, padding=\"same\"\n",
        " * Zero Padding 2D: ((0, 1), (0, 1))\n",
        " * Batch Normalization 2D with momentum(0.8)\n",
        " * Leaky ReLU(0.2)\n",
        " * Dropout(0.25)\n",
        " * Conv2D: 3x3, filters=128, strides=2, padding=\"same\"\n",
        " * Batch Normalization 2D with momentum(0.8)\n",
        " * Leaky ReLU(0.2)\n",
        " * Dropout(0.25)\n",
        " * Conv2D: 3x3, filters=256, strides=2, padding=\"same\"\n",
        " * Batch Normalization 2D with momentum(0.8)\n",
        " * Leaky ReLU(0.2)\n",
        " * Dropout(0.25)\n",
        " * Flatten\n",
        " * Fully connected layer with output size 1\n",
        " * Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeqlNeKM2T_i"
      },
      "outputs": [],
      "source": [
        "class DCDiscriminator(nn.Module):\n",
        "    def __init__(self, img_shape: tuple):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # channels first\n",
        "        img_chan = img.permute(0, 3, 1, 2)\n",
        "        return self.model(img_chan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv8LAYu52T_i"
      },
      "outputs": [],
      "source": [
        "dc_dis = DCDiscriminator(img_shape=IMAGE_SIZE + (3,))\n",
        "fake_proba = dc_dis(fake_imgs)\n",
        "assert fake_proba.shape[1:] == (1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE-qWG0Y2T_i"
      },
      "source": [
        "## Least Squares GAN loss (1.0 pts) [cross-check:5]\n",
        "We'll now look at [Least Squares GAN loss](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all you have to do is change the loss function and retrain the model. You'll implement equation (9) in the paper, with the generator loss:\n",
        "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
        "and the discriminator loss:\n",
        "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pjHNyLm2T_i"
      },
      "outputs": [],
      "source": [
        "class DCGAN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        width,\n",
        "        height,\n",
        "        channels,\n",
        "        noise_dim=100,\n",
        "        lr=0.0002,\n",
        "        b1=0.5,\n",
        "        b2=0.999,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # make <arg> available as self.hparams.<arg>\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Important: This property activates manual optimization.\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        img_shape = (width, height, channels)\n",
        "        self.generator = DCGenerator(\n",
        "            noise_dim=self.hparams.noise_dim,\n",
        "            img_shape=img_shape,\n",
        "        )\n",
        "        self.discriminator = DCDiscriminator(\n",
        "            img_shape=img_shape,\n",
        "        )\n",
        "\n",
        "        self.validation_z = torch.randn(8, noise_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def ls_loss(self, y_hat, y):\n",
        "        # TODO: implement least squares GAN loss\n",
        "        return ...\n",
        "\n",
        "    def training_step(self, imgs, batch_idx):\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
        "        # move to same device as imgs\n",
        "        z = z.type_as(imgs)\n",
        "\n",
        "        # optimize generator\n",
        "        # TODO: compute loss as before (FCGAN)\n",
        "        ...\n",
        "\n",
        "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
        "\n",
        "        opt_g.zero_grad()\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "\n",
        "        # optimize discriminator\n",
        "        # TODO: compute loss as before (FCGAN)\n",
        "        ...\n",
        "\n",
        "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
        "\n",
        "        opt_d.zero_grad()\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
        "        # channels before pixels\n",
        "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmQLyGSv2T_i"
      },
      "source": [
        "### Training\n",
        "Train generator and discriminator in a loop and draw results once every N iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3-C68ZNRpmP"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1pWMIA4RpmQ"
      },
      "outputs": [],
      "source": [
        "train_kwargs[\"max_epochs\"] = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4R3uSOJ2T_i"
      },
      "outputs": [],
      "source": [
        "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
        "model = DCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
        "trainer = pl.Trainer(**train_kwargs)\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEAcxvr7nDqE"
      },
      "source": [
        "Prepare data for **III. GAN metrics. PRD score**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rirxBd_wQikH"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "\n",
        "def predict(\n",
        "    model: pl.LightningModule,\n",
        "    data: torch.Tensor,\n",
        "    batch_size: int,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    output = []\n",
        "    slicer = range(0, len(data), batch_size)\n",
        "    if verbose:\n",
        "        slicer = tqdm.tqdm(slicer)\n",
        "\n",
        "    model.eval()\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    if use_gpu:\n",
        "        model.cuda()\n",
        "    with torch.no_grad():\n",
        "        for i in slicer:\n",
        "            x = data[i : i + batch_size]\n",
        "            if use_gpu:\n",
        "                x = x.cuda()\n",
        "            y = model(x)\n",
        "            if use_gpu:\n",
        "                y = y.cpu()\n",
        "            output.append(y)\n",
        "    if use_gpu:\n",
        "        model.cpu()\n",
        "\n",
        "    output = torch.cat(output, dim=0)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAWj43ixmQnk"
      },
      "outputs": [],
      "source": [
        "dataset = SneakersDataset(\"data\", target_size=IMAGE_SIZE)\n",
        "lr_data = torch.stack([dataset[i] for i in range(len(dataset))], dim=0)\n",
        "dc_fake_data = predict(model, torch.randn(lr_data.size(0), NOISE_DIM), BATCH_SIZE)\n",
        "print(\"LR data:\", lr_data.size())\n",
        "print(\"Fake LR data:\", dc_fake_data.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFEC7ayo2T_i"
      },
      "outputs": [],
      "source": [
        "visualize_images(dc_fake_data, 4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NarPWs-M2T_j"
      },
      "source": [
        "# II. Super Resolution\n",
        "\n",
        "In this part of the notebook you will train a generative model that solves an image-to-image problem, with \"small images\" as a source domain and \"large images\" being a target domain.\n",
        "\n",
        "To specify the task, you are to **scale small images of 28x28 pixels up to size of 112x112 pixels**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdq-1LSb2T_j"
      },
      "outputs": [],
      "source": [
        "class SneakersSRDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.dataset = SneakersDataset(\n",
        "            self.data_dir,\n",
        "            input_size=LOW_RES_SIZE,\n",
        "            target_size=HIGH_RES_SIZE,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=self.shuffle,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tITnKMbz2T_j"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "dm = SneakersSRDataModule(\"data\", batch_size=batch_size)\n",
        "dm.setup()\n",
        "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
        "\n",
        "assert real_data_lr.shape[1:] == LOW_RES_SIZE + (3,)\n",
        "assert real_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)\n",
        "\n",
        "plt.figure(figsize=(8, 14))\n",
        "for i in range(batch_size):\n",
        "    plt.subplot(batch_size, 2, 2 * i + 1)\n",
        "    plt.title(\"Low resolution\")\n",
        "    plt.imshow(data2img(real_data_lr[i]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(batch_size, 2, 2 * i + 2)\n",
        "    plt.title(\"High resolution\")\n",
        "    plt.imshow(data2img(real_data_hr[i]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vu7Kd2vHkTW"
      },
      "source": [
        "In the second part of this task, you are to train an [SRGAN](https://arxiv.org/abs/1609.04802)-like model. For your convinience, some layers are already implemented. Now it's your turn -- fill the gaps so the model passes the asserts below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgweaOiH8Ue"
      },
      "source": [
        "### Generator (1.5 pts) [cross-check:6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTyXzOqIBzQ"
      },
      "source": [
        "To build a SRGAN Generator, you will need a basic Residual Block(filters):\n",
        "\n",
        "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
        "* ReLU\n",
        "* Batch Normalization 2D with momentum(0.8)\n",
        "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
        "* Batch Normalization 2D with momentum(0.8)\n",
        "* Sum up outputs with inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcHFj_MEKEAy"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters: int):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z) + z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADazxfW1KEMY"
      },
      "source": [
        "Upsampling Block(filters):\n",
        "\n",
        "* UpSampling2D(2)\n",
        "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
        "* ReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEqo5dKaKEcv"
      },
      "outputs": [],
      "source": [
        "class UpsamplingBlock(nn.Module):\n",
        "    def __init__(self, filters: int, input_filters: int = None):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNK9xG5NKEq_"
      },
      "source": [
        "Now, using these basic building blocks, one is able to define a SRGAN generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mPTVp1hG9aQ"
      },
      "outputs": [],
      "source": [
        "class SRGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.init_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.residual_chain = nn.Sequential(\n",
        "            *[ResidualBlock(64) for _ in range(16)],\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=3 // 2, padding_mode=\"zeros\"),\n",
        "            nn.BatchNorm2d(64, momentum=0.8)\n",
        "        )\n",
        "        self.upsample_conv = nn.Sequential(\n",
        "            UpsamplingBlock(256, input_filters=64),\n",
        "            UpsamplingBlock(256),\n",
        "            nn.Conv2d(256, 3, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = z.permute(0, 3, 1, 2)\n",
        "        conv = self.init_conv(x)\n",
        "        x = self.residual_chain(conv) + conv\n",
        "        img = self.upsample_conv(x)\n",
        "        # make channel axis last\n",
        "        img = img.permute(0, 2, 3, 1)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KWElBB1Sld-"
      },
      "outputs": [],
      "source": [
        "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
        "fake_hr = SRGenerator()(real_data_lr)\n",
        "assert fake_hr.shape == real_data_hr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyoU_b6bXvAN"
      },
      "source": [
        "### Discriminator (1.5 pts) [cross-check:7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjBXT93Xx75"
      },
      "source": [
        "First, define a Discriminator Block(filters, strides):\n",
        "\n",
        "* Conv2D: 3x3, filters=filters, strides=strides, padding=\"same\"\n",
        "* Leaky ReLU(0.2)\n",
        "* (optional) Batch Normalization 2D with momentum(0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzwB98UXWDYM"
      },
      "outputs": [],
      "source": [
        "class DBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_filters: int,\n",
        "        filters: int,\n",
        "        strides: int,\n",
        "        batch_norm: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            # TODO: add layers (except BatchNorm2d)\n",
        "            ...\n",
        "        ]\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm2d(filters, momentum=0.8))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XySSJen22I"
      },
      "source": [
        "Now, use this block to build up SRGAN discriminator:\n",
        "\n",
        "* DBlock(filters=64, strides=1) with Batch Normalization\n",
        "* DBlock(filters=64, strides=2)\n",
        "* DBlock(filters=128, strides=1)\n",
        "* DBlock(filters=128, strides=2)\n",
        "* DBlock(filters=256, strides=1)\n",
        "* DBlock(filters=256, strides=2)\n",
        "* DBlock(filters=512, strides=1)\n",
        "* DBlock(filters=512, strides=2)\n",
        "* Flatten\n",
        "* Fully connected with output size of 1024\n",
        "* Leaky ReLU(0.2)\n",
        "* Fully connected with output size of 1\n",
        "* Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE1_Ea-Zn3Po"
      },
      "outputs": [],
      "source": [
        "class SRDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # TODO: add layers\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = z.permute(0, 3, 1, 2)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaAKs9BMp47b"
      },
      "outputs": [],
      "source": [
        "fake_probas = SRDiscriminator()(fake_hr)\n",
        "assert fake_probas.shape[1:] == (1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS1yeB5cquyO"
      },
      "source": [
        "Typically, SRGAN is trained with additional loss on features from pretrained VGG-19. However, you task will be a bit simplier: use **mean squared error** between real and fake data instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaydO1FJqA0G"
      },
      "outputs": [],
      "source": [
        "class SRGAN(pl.LightningModule):\n",
        "    def __init__(self, validation_lr):\n",
        "        super().__init__()\n",
        "\n",
        "        # Important: This property activates manual optimization.\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.generator = SRGenerator()\n",
        "        self.discriminator = SRDiscriminator()\n",
        "\n",
        "        self.validation_lr = validation_lr\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def gan_loss(self, y_hat, y):\n",
        "        # TODO: implement loss\n",
        "        return ...\n",
        "\n",
        "    def ls_loss(self, y_hat, y):\n",
        "        # TODO: implement loss\n",
        "        return ...\n",
        "\n",
        "    def mse_loss(self, sr_imgs, hr_imgs):\n",
        "        # TODO: implement MSE loss\n",
        "        return ...\n",
        "\n",
        "    def training_step(self, imgs, batch_idx):\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "        lr_imgs, hr_imgs = imgs\n",
        "\n",
        "        # optimize generator\n",
        "        sr_imgs = self(lr_imgs)\n",
        "\n",
        "        # all fake, but we want to be real\n",
        "        valid = torch.ones(lr_imgs.size(0), 1)\n",
        "        valid = valid.type_as(lr_imgs)\n",
        "\n",
        "        gan_loss = self.gan_loss(self.discriminator(sr_imgs), valid)\n",
        "        mse_loss = self.mse_loss(sr_imgs, hr_imgs)\n",
        "\n",
        "        g_loss = 1 * mse_loss + 0.5 * gan_loss\n",
        "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
        "        self.log(\"gan_loss\", gan_loss, prog_bar=True)\n",
        "        self.log(\"mse_loss\", mse_loss, prog_bar=True)\n",
        "\n",
        "        opt_g.zero_grad()\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "\n",
        "        # optimize discriminator\n",
        "        valid = torch.ones(hr_imgs.size(0), 1)\n",
        "        valid = valid.type_as(hr_imgs)\n",
        "        real_loss = self.ls_loss(self.discriminator(hr_imgs), valid)\n",
        "\n",
        "        fake = torch.zeros(lr_imgs.size(0), 1)\n",
        "        fake = fake.type_as(lr_imgs)\n",
        "        fake_loss = self.ls_loss(self.discriminator(self(lr_imgs).detach()), fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
        "\n",
        "        opt_d.zero_grad()\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt_g = torch.optim.Adam(\n",
        "            self.generator.parameters(),\n",
        "            lr=2e-4,\n",
        "            betas=(0.5, 0.999),\n",
        "        )\n",
        "        opt_d = torch.optim.Adam(\n",
        "            self.discriminator.parameters(),\n",
        "            lr=1e-3,\n",
        "            betas=(0.5, 0.999),\n",
        "        )\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        z = self.validation_lr.type_as(self.generator.init_conv[0].weight)\n",
        "        # channels before pixels\n",
        "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71X-grsBcsAt"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KmvmfcDvXSy"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xWztpoFse_y"
      },
      "outputs": [],
      "source": [
        "train_kwargs[\"max_epochs\"] = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrGJI_gTuJcR"
      },
      "outputs": [],
      "source": [
        "dm = SneakersSRDataModule(\"data\", batch_size=16)\n",
        "dm.setup()\n",
        "model = SRGAN(validation_lr=next(iter(dm.train_dataloader()))[0][:4])\n",
        "trainer = pl.Trainer(**train_kwargs)\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuWpdw7_vbxu"
      },
      "source": [
        "Prepare data for **III. GAN metrics**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZC8EP_xvhVD"
      },
      "outputs": [],
      "source": [
        "dm = SneakersSRDataModule(\"data\", batch_size=BATCH_SIZE)\n",
        "dm.setup()\n",
        "hr_data = []\n",
        "for lr, hr in dm.train_dataloader():\n",
        "    hr_data.append(hr)\n",
        "hr_data = torch.cat(hr_data, dim=0)\n",
        "\n",
        "batch_size = 16\n",
        "sr_fake_data_from_real_lr = predict(model, lr_data, batch_size)\n",
        "sr_fake_data_from_fake_lr = predict(model, dc_fake_data, batch_size)\n",
        "print(\"HR data:\", hr_data.size())\n",
        "print(\"SR from real LR:\", sr_fake_data_from_real_lr.size())\n",
        "print(\"SR from fake LR:\", sr_fake_data_from_fake_lr.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmLKMN_rujEn"
      },
      "outputs": [],
      "source": [
        "visualize_images(sr_fake_data_from_real_lr, 4, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhlAvVbpM9a9"
      },
      "outputs": [],
      "source": [
        "visualize_images(sr_fake_data_from_fake_lr, 4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auJUUp_ArHkb"
      },
      "source": [
        "# III. GAN metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr1sSMURrPLf"
      },
      "source": [
        "There exists a few metrics used to measure GAN performance. Some of them are based on comparing real samples against generated ones, while the other rely on additional pretrained models that are applied to both real and generated data in order to accumulate high-level statistics. In this task, you are going to use two metrics representing these two approaches -- namely, [Precision-Recall Density](https://arxiv.org/pdf/1806.00035) and [Frchet Inception Distance](https://arxiv.org/pdf/1706.08500)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJDHoskhrMso"
      },
      "source": [
        "##  Precision-Recall Density (PRD score) (1.0 pts) [cross-check:8]\n",
        "\n",
        "Your first task is to implement [Precision-Recall Density](https://arxiv.org/pdf/1806.00035.pdf) score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_oUr0sjrOti"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "\n",
        "def bin_counts(real_data, fake_data, n_bins=25):\n",
        "    real_data = real_data.reshape(len(real_data), -1)\n",
        "    fake_data = fake_data.reshape(len(fake_data), -1)\n",
        "\n",
        "    data = np.vstack([real_data, fake_data])\n",
        "\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_bins, n_init=10).fit(data)\n",
        "\n",
        "    real_labels = kmeans.labels_[: len(real_data)]\n",
        "    fake_labels = kmeans.labels_[len(real_data) :]\n",
        "\n",
        "    real_density, _ = np.histogram(\n",
        "        real_labels,\n",
        "        bins=n_bins,\n",
        "        range=[0, n_bins],\n",
        "        density=True,\n",
        "    )\n",
        "    # TODO: same for fake_labels\n",
        "    fake_density, _ = ...\n",
        "\n",
        "    return real_density, fake_density\n",
        "\n",
        "\n",
        "def sample_bin_counts(real_data, fake_data, n_bins=25, repeat_number=10, verbose=True):\n",
        "    real_densities = []\n",
        "    fake_densities = []\n",
        "    counter = range(repeat_number)\n",
        "    if verbose:\n",
        "        counter = tqdm.tqdm(counter)\n",
        "    for _ in counter:\n",
        "        real, fake = bin_counts(real_data, fake_data, n_bins=n_bins)\n",
        "        real_densities.append(real)\n",
        "        fake_densities.append(fake)\n",
        "    return np.array(real_densities).mean(axis=0), np.array(fake_densities).mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2RUvLSmpeB3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "\n",
        "def calculate_alpha_beta(real_density, fake_density, n_thetas=1000):\n",
        "    assert real_density.shape == fake_density.shape\n",
        "\n",
        "    alpha = []\n",
        "    beta = []\n",
        "\n",
        "    thetas = np.linspace(1e-6, np.pi / 2 - 1e-6, num=n_thetas)\n",
        "    for theta in thetas:\n",
        "        tan = math.tan(theta)\n",
        "        # TODO: implement paper formula\n",
        "        alpha.append(...)\n",
        "        beta.append(...)\n",
        "\n",
        "    return alpha, beta\n",
        "\n",
        "\n",
        "def calculate_prd_score(real_data, fake_data):\n",
        "    # Calculate bin counts from real and generated data multiple times\n",
        "    # TODO\n",
        "    real_density, fake_density = ...\n",
        "\n",
        "    plt.bar(\n",
        "        range(len(real_density)),\n",
        "        real_density,\n",
        "        width=1,\n",
        "        color=\"g\",\n",
        "        alpha=0.5,\n",
        "        label=\"Real density\",\n",
        "    )\n",
        "    plt.bar(\n",
        "        range(len(fake_density)),\n",
        "        fake_density,\n",
        "        width=1,\n",
        "        color=\"r\",\n",
        "        alpha=0.5,\n",
        "        label=\"Fake density\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate alpha and beta\n",
        "    # TODO\n",
        "    alpha, beta = ...\n",
        "\n",
        "    # Calculate area under curve (AUC) for alpha and beta\n",
        "    # TODO\n",
        "    score = ...\n",
        "\n",
        "    return score, alpha, beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXk1fvzBpnAw"
      },
      "source": [
        "Calculate PRD score for DCGAN (task I). You should pass `lr_data` and `dc_fake_data` to scoring function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wya_v6L5pn_3"
      },
      "outputs": [],
      "source": [
        "score, _, _ = calculate_prd_score(\n",
        "    lr_data,\n",
        "    dc_fake_data,\n",
        ")\n",
        "print(\"Score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2zud1znprDY"
      },
      "source": [
        "Now use PRD score to compare high resolution data generated from real low resolution data (`sr_fake_data_from_real_lr`) and fake resolution data (`sr_fake_data_from_fake_lr`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RStA_RiapsEt"
      },
      "outputs": [],
      "source": [
        "print(\"Generated from real LR\")\n",
        "score_real, alpha_real, beta_real = calculate_prd_score(\n",
        "    hr_data,\n",
        "    sr_fake_data_from_real_lr,\n",
        ")\n",
        "print(\"Score:\", score_real, end=\"\\n\\n\")\n",
        "\n",
        "print(\"Generated from fake LR\")\n",
        "score_fake, alpha_fake, beta_fake = calculate_prd_score(\n",
        "    hr_data,\n",
        "    sr_fake_data_from_fake_lr,\n",
        ")\n",
        "print(\"Score:\", score_fake, end=\"\\n\\n\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.plot(alpha_real, beta_real, color=\"g\", label=\"Generated from real LR\")\n",
        "plt.plot(alpha_fake, beta_fake, color=\"r\", label=\"Generated from fake LR\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3qk_19kR_zr"
      },
      "source": [
        "## Frchet Inception Distance (FID score) (1.0 pts) [cross-check:9]\n",
        "\n",
        "[Frechet Inception Distance](https://arxiv.org/pdf/1706.08500) is an improved version of [Inception score](https://arxiv.org/abs/1606.03498), that additionally calculates the statistics of real data and compares it to the statistics of generated data. It is probably the most widely-used option for evaluating GANs, and relies on features extracted with [InceptionV3](https://arxiv.org/abs/1512.00567) pretrained on ImageNet. These features assumed to come from a multivariate Gaussian distribution, so Frchet distance between two multivariate Gaussians can be calculated:\n",
        "\n",
        "$$\\text{FID} = ||\\mu_r - \\mu_g||^2 + \\text{Tr} (\\Sigma_r + \\Sigma_g - 2 (\\Sigma_r \\Sigma_g)^{1/2}),$$\n",
        "\n",
        "where $X_r \\sim \\mathcal{N} (\\mu_r, \\Sigma_r)$ and $X_g \\sim \\mathcal{N} (\\mu_g, \\Sigma_g)$ are the 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcHc-r8oSGj7"
      },
      "source": [
        "First, create InceptionV3 ([from torch repository](https://pytorch.org/hub/pytorch_vision_inception_v3/)) model. As you will be using it for feature extraction only, we should remove last fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d01uHGsrVzWR"
      },
      "outputs": [],
      "source": [
        "class InceptionHeadless(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.model = torch.hub.load(\"pytorch/vision\", \"inception_v3\", pretrained=True)\n",
        "        # remove last fc layer\n",
        "        self.model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = z.permute(0, 3, 1, 2)\n",
        "        x = self.upsample(x)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U0MbG9KzIMn"
      },
      "outputs": [],
      "source": [
        "inception = InceptionHeadless()\n",
        "inception.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    assert inception(hr_data[:32]).shape[1:] == (2048,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILNxlRbP25Ec"
      },
      "outputs": [],
      "source": [
        "def calculate_activations(data, batch_size=32, verbose=False):\n",
        "    # Calculate activations of Pool3 layer of InceptionV3\n",
        "    if verbose:\n",
        "        print(\"Calculating activations...\")\n",
        "    activations = predict(inception, data, batch_size=32)\n",
        "    return activations\n",
        "\n",
        "\n",
        "def calculate_activation_statistics(activations):\n",
        "    # Calculate mean and covariance of activations. Mind the dimensions!\n",
        "    # TODO\n",
        "    mu = ...\n",
        "    t = activations - mu\n",
        "    # TODO\n",
        "    sigma = ...\n",
        "    return mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohaJKXnVzIS8"
      },
      "outputs": [],
      "source": [
        "real_activations = calculate_activations(hr_data, verbose=True)\n",
        "real_mu, real_sigma = calculate_activation_statistics(real_activations)\n",
        "\n",
        "assert real_mu.shape == (2048,)\n",
        "assert real_sigma.shape == (2048, 2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYG7z9qSWcyA"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    assert mu1.shape == mu2.shape\n",
        "    assert sigma1.shape == sigma2.shape\n",
        "\n",
        "    sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1, sigma2))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(sigma1_sigma2):\n",
        "        sigma1_sigma2 = sigma1_sigma2.real\n",
        "\n",
        "    # Product might be almost singular\n",
        "    if not np.isfinite(sigma1_sigma2).all():\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1 + offset, sigma2 + offset))\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # use diff, sigma1, sigma2 to calculate FID according to the formula above\n",
        "    # TODO: implement score from paper\n",
        "    return ...\n",
        "\n",
        "\n",
        "def calculate_fid_score(real_data, fake_data, verbose=False):\n",
        "    # Run inception on real and fake data to obtain activations\n",
        "    # TODO\n",
        "    real_activations = ...\n",
        "    fake_activations = ...\n",
        "\n",
        "    # Calculate mu and sigma for both real and fake activations\n",
        "    # TODO\n",
        "    real_mu, real_sigma = ...\n",
        "    fake_mu, fake_sigma = ...\n",
        "\n",
        "    # Calculate Frechet distance\n",
        "    return calculate_frechet_distance(\n",
        "        real_mu,\n",
        "        real_sigma,\n",
        "        fake_mu,\n",
        "        fake_sigma,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQjxyGZLewHo"
      },
      "source": [
        "Calculate FID score between `hr_data` and `sr_fake_data_from_real_hr`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvHBjRgOeuCx"
      },
      "outputs": [],
      "source": [
        "score = calculate_fid_score(\n",
        "    hr_data,\n",
        "    sr_fake_data_from_real_lr,\n",
        ")\n",
        "print(\"Score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYWDEwZeyT9"
      },
      "source": [
        "Putting it all together: calculate FID score between `hr_data` and `sr_fake_data_from_fake_hr`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgyhdAyoezVL"
      },
      "outputs": [],
      "source": [
        "score = calculate_fid_score(\n",
        "    hr_data,\n",
        "    sr_fake_data_from_fake_lr,\n",
        ")\n",
        "print(\"Score:\", score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
